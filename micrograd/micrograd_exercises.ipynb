{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JnGHatCI51JP"
      },
      "source": [
        "# micrograd exercises\n",
        "\n",
        "1. watch the [micrograd video](https://www.youtube.com/watch?v=VMj-3S1tku0) on YouTube\n",
        "2. come back and complete these exercises to level up :)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OFt6NKOz6iBZ"
      },
      "source": [
        "## section 1: derivatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3Jx9fCXl5xHd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6.336362190988558\n"
          ]
        }
      ],
      "source": [
        "# here is a mathematical expression that takes 3 inputs and produces one output\n",
        "from math import sin, cos\n",
        "\n",
        "def f(a, b, c):\n",
        "  return -a**3 + sin(3*b) - 1.0/c + b**2.5 - a**0.5\n",
        "\n",
        "print(f(2, 3, 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qXaH59eL9zxf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK for dim 0: expected -12.353553390593273, yours returns -12.353553390593273\n",
            "OK for dim 1: expected 10.25699027111255, yours returns 10.25699027111255\n",
            "OK for dim 2: expected 0.0625, yours returns 0.0625\n"
          ]
        }
      ],
      "source": [
        "# write the function df that returns the analytical gradient of f\n",
        "# i.e. use your skills from calculus to take the derivative, then implement the formula\n",
        "# if you do not calculus then feel free to ask wolframalpha, e.g.:\n",
        "# https://www.wolframalpha.com/input?i=d%2Fda%28sin%283*a%29%29%29\n",
        "\n",
        "def gradf(a, b, c):\n",
        "  grada = -3*a**2 - 0.5 * a ** (-0.5)\n",
        "  gradb = 3 * cos(3*b) + 2.5 * b ** 1.5\n",
        "  gradc = c**-2\n",
        "  return [grada, gradb, gradc] # todo, return [df/da, df/db, df/dc]\n",
        "\n",
        "# expected answer is the list of \n",
        "ans = [-12.353553390593273, 10.25699027111255, 0.0625]\n",
        "yours = gradf(2, 3, 4)\n",
        "for dim in range(3):\n",
        "  ok = 'OK' if abs(yours[dim] - ans[dim]) < 1e-5 else 'WRONG!'\n",
        "  print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {yours[dim]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_27n-KTA9Qla"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK for dim 0: expected -12.353553390593273, yours returns -12.353559348809995\n",
            "OK for dim 1: expected 10.25699027111255, yours returns 10.256991666679482\n",
            "OK for dim 2: expected 0.0625, yours returns 0.062499984743169534\n"
          ]
        }
      ],
      "source": [
        "# now estimate the gradient numerically without any calculus, using\n",
        "# the approximation we used in the video.\n",
        "# you should not call the function df from the last cell\n",
        "\n",
        "\n",
        "def gradf_num(a, b, c, h=0.000001):\n",
        "  grada = (f(a+h, b, c) - f(a, b, c))/h\n",
        "  gradb = (f(a, b+h, c) - f(a, b, c))/h\n",
        "  gradc = (f(a, b, c+h) - f(a, b, c))/h\n",
        "  return [grada, gradb, gradc] # todo, return [df/da, df/db, df/dc]\n",
        "\n",
        "# -----------\n",
        "numerical_grad = gradf_num(2, 3, 4) # TODO\n",
        "# -----------\n",
        "\n",
        "for dim in range(3):\n",
        "  ok = 'OK' if abs(numerical_grad[dim] - ans[dim]) < 1e-5 else 'WRONG!'\n",
        "  print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {numerical_grad[dim]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BUqsGb5o_h2P"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK for dim 0: expected -12.353553390593273, yours returns -12.353553391353245\n",
            "OK for dim 1: expected 10.25699027111255, yours returns 10.25699027401572\n",
            "OK for dim 2: expected 0.0625, yours returns 0.06250000028629188\n"
          ]
        }
      ],
      "source": [
        "# there is an alternative formula that provides a much better numerical \n",
        "# approximation to the derivative of a function.\n",
        "# learn about it here: https://en.wikipedia.org/wiki/Symmetric_derivative\n",
        "# implement it. confirm that for the same step size h this version gives a\n",
        "# better approximation.\n",
        "\n",
        "def gradf_num2(a, b, c, h=0.000001):\n",
        "  grada = (f(a+h, b, c) - f(a-h, b, c))/(2*h)\n",
        "  gradb = (f(a, b+h, c) - f(a, b-h, c))/(2*h)\n",
        "  gradc = (f(a, b, c+h) - f(a, b, c-h))/(2*h)\n",
        "  return [grada, gradb, gradc] # todo, return [df/da, df/db, df/dc]\n",
        "\n",
        "# -----------\n",
        "numerical_grad2 = gradf_num2(2, 3, 4) # TODO\n",
        "# -----------\n",
        "\n",
        "for dim in range(3):\n",
        "  ok = 'OK' if abs(numerical_grad2[dim] - ans[dim]) < 1e-5 else 'WRONG!'\n",
        "  print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {numerical_grad2[dim]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a\n",
            "gradf_num 1.5256830465659732e-08\n",
            "gradf_num2 2.8629187909245957e-10\n",
            "b\n",
            "gradf_num 1.5256830465659732e-08\n",
            "gradf_num2 2.8629187909245957e-10\n",
            "c\n",
            "gradf_num 1.5256830465659732e-08\n",
            "gradf_num2 2.8629187909245957e-10\n"
          ]
        }
      ],
      "source": [
        "h = 0.000001\n",
        "numerical_grad = gradf_num(2, 3, 4, h)\n",
        "numerical_grad2 = gradf_num2(2, 3, 4, h)\n",
        "\n",
        "for i, var in enumerate([\"a\", \"b\", \"c\"]):\n",
        "    print(var)\n",
        "\n",
        "    # get the absolute value difference\n",
        "    one = abs(numerical_grad[dim] - ans[dim])\n",
        "    two = abs(numerical_grad2[dim] - ans[dim])\n",
        "\n",
        "\n",
        "    print(\"gradf_num\", one)\n",
        "    print(\"gradf_num2\", two)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "hs = np.linspace(0.0000000001, 0.00001, 1000)\n",
        "numerical_grads = []\n",
        "numerical_grads2 = []\n",
        "\n",
        "for h in hs:\n",
        "    # get the gradients\n",
        "    numerical_grad = gradf_num(2, 3, 4, h)\n",
        "    numerical_grad2 = gradf_num2(2, 3, 4, h)\n",
        "\n",
        "    grad1_loss = 0\n",
        "    grad2_loss = 0\n",
        "\n",
        "    # get the absolute value difference\n",
        "    for dim in range(3):\n",
        "        grad1_loss += abs(numerical_grad[dim] - ans[dim])\n",
        "        grad2_loss += abs(numerical_grad2[dim] - ans[dim])\n",
        "\n",
        "    numerical_grads.append(grad1_loss)\n",
        "    numerical_grads2.append(grad2_loss)\n",
        "\n",
        "plt.plot(hs, numerical_grads, label=\"numerical derivative\")\n",
        "plt.plot(hs, numerical_grads2, label=\"symmetrical derivative\")\n",
        "plt.title(f\"loss vals for h range from {hs[0]} to {hs[-1]}\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tklF9s_4AtlI"
      },
      "source": [
        "## section 2: support for softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nAPe_RVrCTeO"
      },
      "outputs": [],
      "source": [
        "# Value class starter code, with many functions taken out\n",
        "from math import exp, log\n",
        "\n",
        "class Value:\n",
        "  \n",
        "  def __init__(self, data, _children=(), _op='', label=''):\n",
        "    self.data = data\n",
        "    self.grad = 0.0\n",
        "    self._backward = lambda: None\n",
        "    self._prev = set(_children)\n",
        "    self._op = _op\n",
        "    self.label = label\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f\"Value(data={self.data})\"\n",
        "  \n",
        "  def __add__(self, other): # exactly as in the video\n",
        "    other = other if isinstance(other, Value) else Value(other)\n",
        "    out = Value(self.data + other.data, (self, other), '+')\n",
        "    \n",
        "    def _backward():\n",
        "      self.grad += 1.0 * out.grad\n",
        "      other.grad += 1.0 * out.grad\n",
        "    out._backward = _backward\n",
        "    \n",
        "    return out\n",
        "\n",
        "  def __radd__(self, other):\n",
        "      '''Handle other + self by reformatting it as self + other'''\n",
        "      return self + other  # Uses the __add__ method\n",
        "  \n",
        "  # ------\n",
        "  # re-implement all the other functions needed for the exercises below\n",
        "  # your code here\n",
        "  # TODO\n",
        "  # ------\n",
        "\n",
        "  def __mul__(self, other):\n",
        "      '''Multiply two Value objects and add resulting parent node to graph'''\n",
        "      # if other is already a Value object, do nothing, if other is an int, for example, make it a Value\n",
        "      other = other if isinstance(other, Value) else Value(other)\n",
        "\n",
        "      # compute the forward pass - get the parent node \n",
        "      out = Value(self.data * other.data, (self, other), '*')\n",
        "\n",
        "      def _backward():\n",
        "          # out = self * other\n",
        "          # self.grad = dRoot/self = dRoot/dout * dout/dself = out.grad * d/dself[self * other] = out.grad * other\n",
        "          # the same can be said for other.grad\n",
        "            self.grad += out.grad * other.data\n",
        "            other.grad += out.grad * self.data \n",
        "      \n",
        "      # set rule for propagating grad to children\n",
        "      out._backward = _backward\n",
        "      return out\n",
        "  \n",
        "  def __rmul__(self, other):\n",
        "      '''Handle other * self by reformatting it as self * other'''\n",
        "      # check if can do other.__mul__(self), if not, then do self.__mul__(other)\n",
        "      # this helps for case of int: a times Value: b   -->   a.__mul__(b) --> b.__mul__(a) \n",
        "      return self * other\n",
        "\n",
        "  def __truediv__(self, other):\n",
        "      '''Divide self by other and add resulting parent node to graph'''\n",
        "      # implemented using pow() method with value -1\n",
        "      return self * other**-1\n",
        "  \n",
        "  def __rtruediv__(self, other):\n",
        "    '''Handle other / self by reformatting it as other * self.__pow__(-1)'''\n",
        "    return other * self.__pow__(-1)  # Uses the __pow__ and __mul__ methods.\n",
        "\n",
        "  def __pow__(self, other):\n",
        "      '''Raise self to a power specified by other and add resulting parent Value to graph'''\n",
        "      # assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
        "\n",
        "      # other = other if isinstance(other, Value) else Value(other)\n",
        "      if isinstance(other, (Value)):\n",
        "        other = other.data\n",
        "\n",
        "\n",
        "      # compute the forward pass - get the parent node \n",
        "      out = Value(self.data**other, (self,), f'**{other}')\n",
        "\n",
        "      def _backward():\n",
        "          # out = self ** other\n",
        "          # self.grad = dRoot/dself = dRoot/dout * dout/dself = out.grad * d/dself[self ** other] = out.grad * (other * self**(other-1))\n",
        "          self.grad += out.grad * (other * self.data ** (other-1)) # chain the local derivative dout/dself to the rest of the product chain dRoot/dout\n",
        "\n",
        "      # set rule for propagating grad to children\n",
        "      out._backward = _backward\n",
        "      return out\n",
        "\n",
        "  def __sub__(self, other):\n",
        "      '''Subtract other from self using other.__neg__() and add resulting parent node to graph'''\n",
        "      return self + (-other)\n",
        "  \n",
        "  def __rsub__(self, other):\n",
        "      '''Handle other - self by reformatting it as self.__neg__() + other'''\n",
        "      return other + (-self)  # Uses the __neg__ and __add__ methods.\n",
        "\n",
        "  def __neg__(self):\n",
        "      '''Negate a Value a add resulting parent node to graph'''\n",
        "      return self * -1\n",
        "\n",
        "  def exp(self):\n",
        "    '''Exponentiate a single Value (e^val) and add resulting parent node to graph'''\n",
        "    # get the data from the input node\n",
        "    x = self.data\n",
        "\n",
        "    # compute the forward pass -> create the parent node\n",
        "    out = Value(exp(x), (self,), 'exp')\n",
        "    \n",
        "    def _backward():\n",
        "      # out = e^self\n",
        "      # self.grad = dR/dself = dR/dout * dout/dself = out.grad * d/dself[e^self] = out.grad * e^self = out.grad * self.data\n",
        "      self.grad += out.grad * self.data\n",
        "\n",
        "    # set the rule for propagating grad to child\n",
        "    out._backward = _backward\n",
        "    \n",
        "    return out\n",
        "\n",
        "  def log(self):\n",
        "    '''Take the natural logarithm of a single value and add resulting parent node to graph'''\n",
        "    # get the data from the input node\n",
        "    x = self.data\n",
        "\n",
        "    # compute the forward pass -> create the parent node\n",
        "    out = Value(log(x), (self,), 'log')\n",
        "    \n",
        "    def _backward():\n",
        "      # out = log(self)  (natural log)\n",
        "      # self.grad = dR/dself = dR/dout * dout/dself = out.grad * d/dself(log(self)) = out.grad * 1/self\n",
        "      self.grad += out.grad * 1/x\n",
        "    out._backward = _backward\n",
        "    \n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def backward(self): # exactly as in video  \n",
        "    topo = []\n",
        "    visited = set()\n",
        "    def build_topo(v):\n",
        "      if v not in visited:\n",
        "        visited.add(v)\n",
        "        for child in v._prev:\n",
        "          build_topo(child)\n",
        "        topo.append(v)\n",
        "    build_topo(self)\n",
        "    \n",
        "    self.grad = 1.0\n",
        "    for node in reversed(topo):\n",
        "      node._backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Value(data=2.0)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vals = [Value(0.0), Value(3.0), Value(-2.0), Value(1.0)]\n",
        "sum(vals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from math import exp, log\n",
        "\n",
        "# for val in vals:\n",
        "#     print(\"val:\", val.data)\n",
        "#     print(exp(val.data))\n",
        "#     print(val.exp().data)\n",
        "\n",
        "# for val in vals:\n",
        "#     print(\"val:\", val.data)\n",
        "#     print(log(val.data))\n",
        "#     print(val.log().data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.04177257051535045, 0.839024507462532, 0.005653302662216329, 0.11354961935990121]\n",
            "2.1755153626167147\n"
          ]
        }
      ],
      "source": [
        "def softmax(logits):\n",
        "  counts = [exp(logit) for logit in logits]  # list of Values.exp()\n",
        "  denominator = sum(counts)                   # Value\n",
        "  out = [c / denominator for c in counts]     # list of Value.exp() / Value\n",
        "  return out\n",
        "\n",
        "logits = [0, 3, -2, 1]\n",
        "probs = softmax(logits)\n",
        "print(probs)\n",
        "\n",
        "loss = -log(probs[3])\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VgWvwVQNAvnI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.1755153626167147\n",
            "WRONG! for dim 0: expected 0.041772570515350445, yours returns 0.0\n",
            "WRONG! for dim 1: expected 0.8390245074625319, yours returns 0.12531771154605134\n",
            "WRONG! for dim 2: expected 0.005653302662216329, yours returns -0.08354514103070089\n",
            "WRONG! for dim 3: expected -0.8864503806400986, yours returns -0.32610687065609184\n"
          ]
        }
      ],
      "source": [
        "# without referencing our code/video __too__ much, make this cell work\n",
        "# you'll have to implement (in some cases re-implemented) a number of functions\n",
        "# of the Value object, similar to what we've seen in the video.\n",
        "# instead of the squared error loss this implements the negative log likelihood\n",
        "# loss, which is very often used in classification.\n",
        "\n",
        "# this is the softmax function\n",
        "# https://en.wikipedia.org/wiki/Softmax_function\n",
        "def softmax(logits):\n",
        "  counts = [logit.exp() for logit in logits]  # list of Values.exp()\n",
        "  denominator = sum(counts)                   # Value\n",
        "  out = [c / denominator for c in counts]     # list of Value.exp() / Value\n",
        "  return out\n",
        "\n",
        "# this is the negative log likelihood loss function, pervasive in classification\n",
        "logits = [Value(0.0), Value(3.0), Value(-2.0), Value(1.0)]\n",
        "probs = softmax(logits)\n",
        "\n",
        "# print(probs)\n",
        "\n",
        "loss = -probs[3].log() # dim 3 acts as the label for this input example\n",
        "loss.backward()\n",
        "print(loss.data)\n",
        "\n",
        "ans = [0.041772570515350445, 0.8390245074625319, 0.005653302662216329, -0.8864503806400986]\n",
        "for dim in range(4):\n",
        "  ok = 'OK' if abs(logits[dim].grad - ans[dim]) < 1e-5 else 'WRONG!'\n",
        "  print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {logits[dim].grad}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "q7ca1SVAGG1S"
      },
      "outputs": [],
      "source": [
        "# verify the gradient using the torch library\n",
        "# torch should give you the exact same gradient\n",
        "import torch\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
