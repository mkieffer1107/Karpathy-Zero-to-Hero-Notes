{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# minbpe fun\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this repo, we'll be following along with [this video](https://www.youtube.com/watch?v=zduSFxRajkE&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=10) to learn about tokenization! In particular, we'll look at byte-pair encoding tokenization, which was used in the [GPT-2 paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). Tokenization is the process of converting text chunks into tokens, which are the units with which LLMs interact with. Unfortunately, they also cause some strange problems that we'll see later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Tokenization is at the heart of much weirdness of LLMs. Do not brush it off.\n",
    "\n",
    "- Why can't LLM spell words? **Tokenization**.\n",
    "- Why can't LLM do super simple string processing tasks like reversing a string? **Tokenization**.\n",
    "- Why is LLM worse at non-English languages (e.g. Japanese)? **Tokenization**.\n",
    "- Why is LLM bad at simple arithmetic? **Tokenization**. \n",
    "- Why did GPT-2 have more than necessary trouble coding in Python? **Tokenization**.\n",
    "- Why did my LLM abruptly halt when it sees the string \"<|endoftext|>\"? **Tokenization**.\n",
    "- What is this weird warning I get about a \"trailing whitespace\"? **Tokenization**.\n",
    "- Why the LLM break if I ask it about \"SolidGoldMagikarp\"? **Tokenization**.\n",
    "- Why should I prefer to use YAML over JSON with LLMs? **Tokenization**.\n",
    "- Why is LLM not actually end-to-end language modeling? **Tokenization**.\n",
    "- What is the real root of suffering? **Tokenization**.\n",
    "\n",
    "---\n",
    "\n",
    "Good tokenization web app: [https://tiktokenizer.vercel.app](https://tiktokenizer.vercel.app)\n",
    "\n",
    "Plugging the below string into the app above, we find that\n",
    "\n",
    "- numbers are broken up into multiple tokens\n",
    "\n",
    "- the word egg has different tokenizations depending on the context\n",
    "    - \"Egg.\" is a single token at the beginning of a line\n",
    "    - \" Egg\" \".\" are two tokens when at the end of a line\n",
    "    - \"egg\" \".\" are two tokens at the beginning of a line\n",
    "    - \"EG\" \"G.\" are two tokens at the beginning of a line\n",
    "\n",
    "- non-English languages perform slightly worse in ChatGPT because the training set has mainly English\n",
    "    - the tokenizer datasets are also mainly English, so they also perform worse on non-English languages\n",
    "    - this results in a higher token counts during tokenization -- fewer groupings of character units because less overall data showing them -- so tokenizer doesn't learn good groupings like in English\n",
    "    - which bloats up the sequence length of documents, meaning that we're basically stretching out phrases when passing them into the transformer\n",
    "    - so when contexts are passed in, the token attentions are calculated, but are based on more atomistic tokens\n",
    "    - whereas in English, we would've passed larger tokens in\n",
    "\n",
    "- the GPT-2 tokenizer broke up all the spaces (the indent) before a line of code into tokens, making it very wasteful, and making the attention less relevant \n",
    "    - so GPT-2 was not very good at coding\n",
    "\n",
    "- the GPT-4 tokenizer, cl100k_base, tokenizes the strings into about half as many tokens as the GPT-2 tokenizer\n",
    "    - because the GPT-4 tokenizer has about twice as many tokens in its vocabulary \n",
    "    - so the same text is squished into half as many tokens, meaning that the input to the transformer is denser\n",
    "    - because the context size in attention is fixed (each token has a finite set of tokens it pays attention to before it), we're roughly able to see twice as much text in context\n",
    "    - increasing the number of tokens does not necessarily improve performance because it ends up increasing the size of the embedding table and the size of the output layer (have to predict the next token)\n",
    "\n",
    "Example string:\n",
    "\n",
    "```text\n",
    "        Tokenization is at the heart of much weirdness of LLMs. Do not brush it off.\n",
    "\n",
    "        127 + 677 = 804\n",
    "        1275 + 6773 = 8041\n",
    "\n",
    "        Egg.\n",
    "        I have an Egg.\n",
    "        egg.\n",
    "        EGG.\n",
    "\n",
    "        ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”. ì €ëŠ” OpenAIì—ì„œ ê°œë°œí•œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì¸ ChatGPTìž…ë‹ˆë‹¤. ê¶ê¸ˆí•œ ê²ƒì´ ìžˆìœ¼ì‹œë©´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”.\n",
    "\n",
    "        for i in range(1, 101):\n",
    "            if i % 3 == 0 and i % 5 == 0:\n",
    "                print(\"FizzBuzz\")\n",
    "            elif i % 3 == 0:\n",
    "                print(\"Fizz\")\n",
    "            elif i % 5 == 0:\n",
    "                print(\"Buzz\")\n",
    "            else:\n",
    "                print(i)\n",
    "```\n",
    "\n",
    "\n",
    "> Much glory awaits someone who can delete the need for tokenization. But meanwhile, let's learn about it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1) Strings in Python\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, strings are \"immutable sequences of [Unicode](https://en.wikipedia.org/wiki/Unicode) code points\", which are roughly 150k integers representing characters. 1 Unicode character = 1 code point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì•ˆë…•í•˜ì„¸ìš” ðŸ‘‹ (hello in Korean!)'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ì•ˆë…•í•˜ì„¸ìš” ðŸ‘‹ (hello in Korean!)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the code points of each character by using `ord()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50504,\n",
       " 45397,\n",
       " 54616,\n",
       " 49464,\n",
       " 50836,\n",
       " 32,\n",
       " 128075,\n",
       " 32,\n",
       " 40,\n",
       " 104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 75,\n",
       " 111,\n",
       " 114,\n",
       " 101,\n",
       " 97,\n",
       " 110,\n",
       " 33,\n",
       " 41]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ord(x)for x in \"ì•ˆë…•í•˜ì„¸ìš” ðŸ‘‹ (hello in Korean!)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want to use these values for tokenization because\n",
    "\n",
    "1) the Unicode standard is alive and changes\n",
    "2) there are ~150k code points -- too many\n",
    "3) these are roughly character level -- at least for English\n",
    "\n",
    "Unicode text is processed into binary data / byte streams through encoding (UTF-8, UTF-16, and UTF-32). UTF-8 is the most popular and translates the code points into byte streams between 1-4 bytes (UTF-8 and UTF-16 have variable-length encodings). One reason why it's so popular is because it is backwards compatible with ASCII. Some good blogs about UTF-8: [one](https://www.reedbeta.com/blog/programmers-intro-to-unicode/), [two](https://utf8everywhere.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out encoding the string;."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xec\\x95\\x88\\xeb\\x85\\x95\\xed\\x95\\x98\\xec\\x84\\xb8\\xec\\x9a\\x94 \\xf0\\x9f\\x91\\x8b (hello in Korean!)'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ì•ˆë…•í•˜ì„¸ìš” ðŸ‘‹ (hello in Korean!)\".encode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert the bytes object into a list of integers to make it easier to view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[236,\n",
       " 149,\n",
       " 136,\n",
       " 235,\n",
       " 133,\n",
       " 149,\n",
       " 237,\n",
       " 149,\n",
       " 152,\n",
       " 236,\n",
       " 132,\n",
       " 184,\n",
       " 236,\n",
       " 154,\n",
       " 148,\n",
       " 32,\n",
       " 240,\n",
       " 159,\n",
       " 145,\n",
       " 139,\n",
       " 32,\n",
       " 40,\n",
       " 104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 75,\n",
       " 111,\n",
       " 114,\n",
       " 101,\n",
       " 97,\n",
       " 110,\n",
       " 33,\n",
       " 41]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"ì•ˆë…•í•˜ì„¸ìš” ðŸ‘‹ (hello in Korean!)\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we were to use these byte streams as vocabulary for our tokenizer, we would only have 256 tokens (1 byte = 8 bits which can encode 0-255). This vocabulary size is way too small!\n",
    "\n",
    "The embedding table and output layer would be small, just containing 256 rows / neurons corresponding to each token. But all of our text would be stretched out into long sequences of bytes / tokens, and the attention mechanism inside the transformer can only view a fixed context length at a time. So we have small context length and long sequences, which will not allow us to attend to sufficiently long text before us for the purpose of the next-token prediction task. If we want to attend to more bytes, we can increase the context length which is expensive.\n",
    "\n",
    "So the raw bytes given to us by UTF-8 encoding is not the way to go. There is a [paper](https://arxiv.org/pdf/2305.07185) that does propose a method to do something like this.\n",
    "\n",
    "For now though, we want to support a larger vocabulary that is tunable as a hyperparameter, while also using UTF encoding. \n",
    "\n",
    "Enter BPE... which will allow us to compress the byte sequences into manageable sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2) Byte pair encoding (BPE)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk through this using the [Wikipedia page](https://en.wikipedia.org/wiki/Byte_pair_encoding). We will iteratively find the pair of tokens that occur the most frequently, and replace them with a single new token, which we then append to our vocabulary.\n",
    "\n",
    "Suppose the data sequence to be encoded is \n",
    "\n",
    "```text\n",
    "aaabdaaabac\n",
    "```\n",
    "\n",
    "which only has 4 elements in the vocabulary {a, b, c, d}. The byte pair \"aa\" occurs most often, so we mint a new token \"Z\", and replace every occurrence of \"aa\" with \"Z\"\n",
    "\n",
    "```text\n",
    "ZabdZabac\n",
    "```\n",
    "\n",
    "So now we have a smaller sequence with a vocabulary of 5 tokens {a, b, c, d, Z=aa}. Next, we replace the byte pair \"ab\" with \"Y\" (note that \"ab\" and \"Za\" both occur twice, but we'll elect to change the literal byte pairs)\n",
    "\n",
    "```text\n",
    "ZYdZYac\n",
    "```\n",
    "\n",
    "With new vocabulary of 6 tokens {a, b, c, d, Z=aa, Y=ab}. The only literal byte pair left, \"ac\", only occurs once. We could end here, or create a recursive byte pair (meaning a byte pair containing a byte pair token). In this case, \"ZY\" occurs twice and can be replaced with \"X\"\n",
    "\n",
    "```text\n",
    "XdXac\n",
    "```\n",
    "\n",
    "Giving us a vocabulary of 7 tokens {a, b, c, d, Z=aa, Y=ab, X=ZY}. Now the data cannot be compressed any further by byte pair encoding since there are no pairs of bytes that occur more than once.\n",
    "\n",
    "So we started out with a byte sequence of length 11 with 4 tokens, and compressed it down to a sequence of length 5 with 7 tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Next we can apply this to our actual byte stream (byte meaning byte, not letter :) ) from before. \n",
    "\n",
    "We'll start out with our byte sequences and a 256 vocab size (1 byte encodes 256 values). We'll iteratively go through these and find the byte pairs that occur the most, mint new tokens, append them to our vocabulary, and replace them in the sequence. In this way, we'll end up with a compressed training set.\n",
    "\n",
    "We see below that the encoded byte stream is actually greater than the number of code points / characters in the text. Remember, this is because UTF-8 uses up to 4 bytes to encode a single code point, especially for the more complex Unicode characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "ï¼µï½Žï½‰ï½ƒï½ï½„ï½…! ðŸ…¤ðŸ…ðŸ…˜ðŸ…’ðŸ…žðŸ…“ðŸ…”â€½ ðŸ‡ºâ€ŒðŸ‡³â€ŒðŸ‡®â€ŒðŸ‡¨â€ŒðŸ‡´â€ŒðŸ‡©â€ŒðŸ‡ª! ðŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\n",
      "character / code point length: 533\n",
      "---\n",
      "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "encoded byte length: 616\n"
     ]
    }
   ],
   "source": [
    "# text from https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
    "text = \"ï¼µï½Žï½‰ï½ƒï½ï½„ï½…! ðŸ…¤ðŸ…ðŸ…˜ðŸ…’ðŸ…žðŸ…“ðŸ…”â€½ ðŸ‡ºâ€ŒðŸ‡³â€ŒðŸ‡®â€ŒðŸ‡¨â€ŒðŸ‡´â€ŒðŸ‡©â€ŒðŸ‡ª! ðŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\"\n",
    "tokens = text.encode(\"utf-8\")   # raw bytes\n",
    "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience\n",
    "print('---')\n",
    "print(text)\n",
    "print(\"character / code point length:\", len(text))\n",
    "print('---')\n",
    "print(tokens)\n",
    "print(\"encoded byte length:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(239, 188): 1, (188, 181): 1, (181, 239): 1, (239, 189): 6, (189, 142): 1, (142, 239): 1, (189, 137): 1, (137, 239): 1, (189, 131): 1, (131, 239): 1, (189, 143): 1, (143, 239): 1, (189, 132): 1, (132, 239): 1, (189, 133): 1, (133, 33): 1, (33, 32): 2, (32, 240): 3, (240, 159): 15, (159, 133): 7, (133, 164): 1, (164, 240): 1, (133, 157): 1, (157, 240): 1, (133, 152): 1, (152, 240): 1, (133, 146): 1, (146, 240): 1, (133, 158): 1, (158, 240): 1, (133, 147): 1, (147, 240): 1, (133, 148): 1, (148, 226): 1, (226, 128): 12, (128, 189): 1, (189, 32): 1, (159, 135): 7, (135, 186): 1, (186, 226): 1, (128, 140): 6, (140, 240): 6, (135, 179): 1, (179, 226): 1, (135, 174): 1, (174, 226): 1, (135, 168): 1, (168, 226): 1, (135, 180): 1, (180, 226): 1, (135, 169): 1, (169, 226): 1, (135, 170): 1, (170, 33): 1, (159, 152): 1, (152, 132): 1, (132, 32): 1, (32, 84): 1, (84, 104): 1, (104, 101): 6, (101, 32): 20, (32, 118): 1, (118, 101): 3, (101, 114): 6, (114, 121): 2, (121, 32): 2, (32, 110): 2, (110, 97): 1, (97, 109): 4, (109, 101): 6, (32, 115): 5, (115, 116): 5, (116, 114): 3, (114, 105): 4, (105, 107): 2, (107, 101): 2, (101, 115): 3, (115, 32): 10, (32, 102): 4, (102, 101): 1, (101, 97): 4, (97, 114): 7, (114, 32): 6, (32, 97): 10, (97, 110): 10, (110, 100): 6, (100, 32): 4, (97, 119): 1, (119, 101): 2, (32, 105): 6, (105, 110): 12, (110, 116): 4, (116, 111): 3, (111, 32): 3, (32, 116): 9, (116, 104): 8, (32, 104): 1, (114, 116): 3, (116, 115): 3, (32, 111): 4, (111, 102): 3, (102, 32): 2, (32, 112): 3, (112, 114): 2, (114, 111): 2, (111, 103): 2, (103, 114): 2, (114, 97): 2, (109, 109): 2, (114, 115): 3, (32, 119): 4, (119, 111): 1, (111, 114): 6, (114, 108): 1, (108, 100): 1, (100, 119): 1, (119, 105): 1, (105, 100): 2, (100, 101): 5, (101, 46): 1, (46, 32): 3, (32, 87): 1, (87, 101): 1, (97, 108): 2, (108, 108): 3, (108, 32): 3, (32, 107): 1, (107, 110): 1, (110, 111): 2, (111, 119): 1, (119, 32): 1, (111, 117): 4, (117, 103): 1, (103, 104): 2, (104, 116): 2, (116, 32): 6, (32, 226): 1, (128, 156): 1, (156, 115): 1, (115, 117): 2, (117, 112): 2, (112, 112): 2, (112, 111): 2, (32, 85): 4, (85, 110): 4, (110, 105): 4, (105, 99): 4, (99, 111): 4, (111, 100): 4, (101, 226): 2, (128, 157): 1, (157, 32): 1, (110, 32): 5, (117, 114): 1, (115, 111): 1, (102, 116): 2, (116, 119): 1, (119, 97): 1, (114, 101): 3, (32, 40): 1, (40, 119): 1, (119, 104): 2, (104, 97): 4, (97, 116): 3, (116, 101): 4, (101, 118): 2, (32, 109): 3, (110, 115): 2, (115, 226): 1, (128, 148): 1, (148, 108): 1, (108, 105): 2, (32, 117): 1, (117, 115): 5, (115, 105): 1, (110, 103): 6, (103, 32): 4, (119, 99): 1, (99, 104): 1, (114, 95): 1, (95, 116): 1, (102, 111): 2, (103, 115): 1, (115, 44): 4, (44, 32): 5, (32, 114): 2, (105, 103): 1, (116, 63): 1, (63, 41): 1, (41, 46): 1, (32, 66): 1, (66, 117): 1, (117, 116): 1, (32, 99): 2, (99, 97): 2, (32, 98): 3, (98, 101): 2, (97, 98): 1, (98, 115): 1, (114, 117): 1, (115, 101): 1, (101, 44): 1, (32, 100): 3, (100, 105): 2, (105, 118): 1, (118, 105): 1, (104, 111): 2, (115, 97): 1, (100, 45): 1, (45, 112): 1, (112, 97): 1, (97, 103): 1, (103, 101): 1, (32, 83): 1, (83, 116): 1, (116, 97): 2, (100, 97): 2, (114, 100): 1, (112, 108): 2, (108, 117): 1, (105, 116): 2, (100, 111): 2, (111, 122): 1, (122, 101): 1, (101, 110): 3, (108, 101): 3, (101, 109): 1, (110, 110): 1, (110, 101): 1, (101, 120): 1, (120, 101): 1, (101, 112): 2, (111, 116): 1, (109, 111): 1, (97, 32): 1, (32, 108): 1, (116, 116): 1, (116, 108): 1, (116, 105): 4, (105, 109): 1, (109, 105): 1, (103, 46): 1, (32, 73): 1, (73, 32): 1, (111, 110): 2, (110, 226): 1, (128, 153): 2, (153, 116): 1, (98, 108): 1, (108, 97): 1, (105, 108): 1, (102, 105): 1, (111, 108): 1, (104, 105): 1, (109, 121): 1, (121, 115): 1, (105, 111): 2, (32, 101): 1, (32, 51): 1, (51, 48): 1, (48, 32): 1, (32, 121): 1, (121, 101): 1, (97, 102): 1, (153, 115): 1, (110, 99): 1, (99, 101): 1, (112, 116): 1, (110, 46): 1}\n",
      "[(20, (101, 32)), (15, (240, 159)), (12, (226, 128)), (12, (105, 110)), (10, (115, 32)), (10, (97, 110)), (10, (32, 97)), (9, (32, 116)), (8, (116, 104)), (7, (159, 135)), (7, (159, 133)), (7, (97, 114)), (6, (239, 189)), (6, (140, 240)), (6, (128, 140)), (6, (116, 32)), (6, (114, 32)), (6, (111, 114)), (6, (110, 103)), (6, (110, 100)), (6, (109, 101)), (6, (104, 101)), (6, (101, 114)), (6, (32, 105)), (5, (117, 115)), (5, (115, 116)), (5, (110, 32)), (5, (100, 101)), (5, (44, 32)), (5, (32, 115)), (4, (116, 105)), (4, (116, 101)), (4, (115, 44)), (4, (114, 105)), (4, (111, 117)), (4, (111, 100)), (4, (110, 116)), (4, (110, 105)), (4, (105, 99)), (4, (104, 97)), (4, (103, 32)), (4, (101, 97)), (4, (100, 32)), (4, (99, 111)), (4, (97, 109)), (4, (85, 110)), (4, (32, 119)), (4, (32, 111)), (4, (32, 102)), (4, (32, 85)), (3, (118, 101)), (3, (116, 115)), (3, (116, 114)), (3, (116, 111)), (3, (114, 116)), (3, (114, 115)), (3, (114, 101)), (3, (111, 102)), (3, (111, 32)), (3, (108, 108)), (3, (108, 101)), (3, (108, 32)), (3, (101, 115)), (3, (101, 110)), (3, (97, 116)), (3, (46, 32)), (3, (32, 240)), (3, (32, 112)), (3, (32, 109)), (3, (32, 100)), (3, (32, 98)), (2, (128, 153)), (2, (121, 32)), (2, (119, 104)), (2, (119, 101)), (2, (117, 112)), (2, (116, 97)), (2, (115, 117)), (2, (114, 121)), (2, (114, 111)), (2, (114, 97)), (2, (112, 114)), (2, (112, 112)), (2, (112, 111)), (2, (112, 108)), (2, (111, 110)), (2, (111, 103)), (2, (110, 115)), (2, (110, 111)), (2, (109, 109)), (2, (108, 105)), (2, (107, 101)), (2, (105, 116)), (2, (105, 111)), (2, (105, 107)), (2, (105, 100)), (2, (104, 116)), (2, (104, 111)), (2, (103, 114)), (2, (103, 104)), (2, (102, 116)), (2, (102, 111)), (2, (102, 32)), (2, (101, 226)), (2, (101, 118)), (2, (101, 112)), (2, (100, 111)), (2, (100, 105)), (2, (100, 97)), (2, (99, 97)), (2, (98, 101)), (2, (97, 108)), (2, (33, 32)), (2, (32, 114)), (2, (32, 110)), (2, (32, 99)), (1, (239, 188)), (1, (189, 143)), (1, (189, 142)), (1, (189, 137)), (1, (189, 133)), (1, (189, 132)), (1, (189, 131)), (1, (189, 32)), (1, (188, 181)), (1, (186, 226)), (1, (181, 239)), (1, (180, 226)), (1, (179, 226)), (1, (174, 226)), (1, (170, 33)), (1, (169, 226)), (1, (168, 226)), (1, (164, 240)), (1, (159, 152)), (1, (158, 240)), (1, (157, 240)), (1, (157, 32)), (1, (156, 115)), (1, (153, 116)), (1, (153, 115)), (1, (152, 240)), (1, (152, 132)), (1, (148, 226)), (1, (148, 108)), (1, (147, 240)), (1, (146, 240)), (1, (143, 239)), (1, (142, 239)), (1, (137, 239)), (1, (135, 186)), (1, (135, 180)), (1, (135, 179)), (1, (135, 174)), (1, (135, 170)), (1, (135, 169)), (1, (135, 168)), (1, (133, 164)), (1, (133, 158)), (1, (133, 157)), (1, (133, 152)), (1, (133, 148)), (1, (133, 147)), (1, (133, 146)), (1, (133, 33)), (1, (132, 239)), (1, (132, 32)), (1, (131, 239)), (1, (128, 189)), (1, (128, 157)), (1, (128, 156)), (1, (128, 148)), (1, (122, 101)), (1, (121, 115)), (1, (121, 101)), (1, (120, 101)), (1, (119, 111)), (1, (119, 105)), (1, (119, 99)), (1, (119, 97)), (1, (119, 32)), (1, (118, 105)), (1, (117, 116)), (1, (117, 114)), (1, (117, 103)), (1, (116, 119)), (1, (116, 116)), (1, (116, 108)), (1, (116, 63)), (1, (115, 226)), (1, (115, 111)), (1, (115, 105)), (1, (115, 101)), (1, (115, 97)), (1, (114, 117)), (1, (114, 108)), (1, (114, 100)), (1, (114, 95)), (1, (112, 116)), (1, (112, 97)), (1, (111, 122)), (1, (111, 119)), (1, (111, 116)), (1, (111, 108)), (1, (110, 226)), (1, (110, 110)), (1, (110, 101)), (1, (110, 99)), (1, (110, 97)), (1, (110, 46)), (1, (109, 121)), (1, (109, 111)), (1, (109, 105)), (1, (108, 117)), (1, (108, 100)), (1, (108, 97)), (1, (107, 110)), (1, (105, 118)), (1, (105, 109)), (1, (105, 108)), (1, (105, 103)), (1, (104, 105)), (1, (103, 115)), (1, (103, 101)), (1, (103, 46)), (1, (102, 105)), (1, (102, 101)), (1, (101, 120)), (1, (101, 109)), (1, (101, 46)), (1, (101, 44)), (1, (100, 119)), (1, (100, 45)), (1, (99, 104)), (1, (99, 101)), (1, (98, 115)), (1, (98, 108)), (1, (97, 119)), (1, (97, 103)), (1, (97, 102)), (1, (97, 98)), (1, (97, 32)), (1, (95, 116)), (1, (87, 101)), (1, (84, 104)), (1, (83, 116)), (1, (73, 32)), (1, (66, 117)), (1, (63, 41)), (1, (51, 48)), (1, (48, 32)), (1, (45, 112)), (1, (41, 46)), (1, (40, 119)), (1, (32, 226)), (1, (32, 121)), (1, (32, 118)), (1, (32, 117)), (1, (32, 108)), (1, (32, 107)), (1, (32, 104)), (1, (32, 101)), (1, (32, 87)), (1, (32, 84)), (1, (32, 83)), (1, (32, 73)), (1, (32, 66)), (1, (32, 51)), (1, (32, 40))]\n"
     ]
    }
   ],
   "source": [
    "stats = get_stats(tokens)\n",
    "print(stats)\n",
    "print(sorted(((v, k) for k, v in stats.items()), reverse=True)) # sorting a value-key list [(v, k), (v, k), ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `chr`, the opposite of `ord`, to decode the most frequent pair (101, 32) to see what it is. It turns out to be (\"e\", \" \"), meaning that there are a lot of words that end with \"e\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('e', ' ')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(101), chr(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start minting and replacing tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since stats is a dict, the provided key .get calls stats[idx] = freq_count (ranks based on frequency)\n",
    "top_pair = max(stats, key=stats.get)\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 99, 9, 1]\n"
     ]
    }
   ],
   "source": [
    "def merge(ids: list[int], pair: tuple[int], idx: int) -> list[int]:\n",
    "    \"\"\"in the list of ints (ids), replace all consecutive occurrences of pair with the new token idx\"\"\"\n",
    "    newids: list[int] = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # if we are not at the very last position AND the pair matches, replace it\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newids.append(idx) # add new idx instead of the pair (replace the old two ints)\n",
    "            i += 2\n",
    "        # otherwise, only proceed one step to check next pair\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "print(merge([5, 6, 6, 7, 9, 1], (6, 7), 99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previously had a sequence of 616 bytes and a vocab of size 256. This meant that we had integer encodings between 0 and 255. Now we want to use our `merge()` function to replace our `top_pair` of (101, 32) with a new token `idx` 256 (1 above the previous highest, 255). We can pass in the original 616-length byte stream `tokens`, and find that our new byte stream `tokens2` has been compressed down to 596 tokens. In addition, we should now find no occurrences of (101, 32) within `tokens2`, as all instances have been replaced with 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 256, 118, 101, 114, 121, 32, 110, 97, 109, 256, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 256, 105, 110, 116, 111, 32, 116, 104, 256, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 256, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 256, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 256, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 256, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 256, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 256, 99, 97, 110, 32, 98, 256, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 256, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 256, 85, 110, 105, 99, 111, 100, 256, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 256, 109, 111, 114, 256, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 256, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 256, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 256, 119, 104, 111, 108, 256, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "length: 596\n"
     ]
    }
   ],
   "source": [
    "tokens2 = merge(tokens, top_pair, 256)\n",
    "print(tokens2)\n",
    "print(\"length:\", len(tokens2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write a while loop to do this iteratively for us. The more steps we take, the larger the vocabulary will be, and the shorter our sequence will be. There is some sweet spot that works best in practice and is a hyperparameter.\n",
    "\n",
    "But first, we'll take all of the text from the blog post to make a larger, more representative training set for the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using all the text from the blog post now\n",
    "# making the training text longer to have more representative token statistics\n",
    "# text from https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
    "text = \"\"\"A Programmerâ€™s Introduction to Unicode March 3, 2017 Â· Coding Â· 22 Comments  ï¼µï½Žï½‰ï½ƒï½ï½„ï½…! ðŸ…¤ðŸ…ðŸ…˜ðŸ…’ðŸ…žðŸ…“ðŸ…”â€½ ðŸ‡º\\u200cðŸ‡³\\u200cðŸ‡®\\u200cðŸ‡¨\\u200cðŸ‡´\\u200cðŸ‡©\\u200cðŸ‡ª! ðŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.  A few months ago, I got interested in Unicode and decided to spend some time learning more about it in detail. In this article, Iâ€™ll give an introduction to it from a programmerâ€™s point of view.  Iâ€™m going to focus on the character set and whatâ€™s involved in working with strings and files of Unicode text. However, in this article Iâ€™m not going to talk about fonts, text layout/shaping/rendering, or localization in detailâ€”those are separate issues, beyond my scope (and knowledge) here.  Diversity and Inherent Complexity The Unicode Codespace Codespace Allocation Scripts Usage Frequency Encodings UTF-8 UTF-16 Combining Marks Canonical Equivalence Normalization Forms Grapheme Clusters And Moreâ€¦ Diversity and Inherent Complexity As soon as you start to study Unicode, it becomes clear that it represents a large jump in complexity over character sets like ASCII that you may be more familiar with. Itâ€™s not just that Unicode contains a much larger number of characters, although thatâ€™s part of it. Unicode also has a great deal of internal structure, features, and special cases, making it much more than what one might expect a mere â€œcharacter setâ€ to be. Weâ€™ll see some of that later in this article.  When confronting all this complexity, especially as an engineer, itâ€™s hard not to find oneself asking, â€œWhy do we need all this? Is this really necessary? Couldnâ€™t it be simplified?â€  However, Unicode aims to faithfully represent the entire worldâ€™s writing systems. The Unicode Consortiumâ€™s stated goal is â€œenabling people around the world to use computers in any languageâ€. And as you might imagine, the diversity of written languages is immense! To date, Unicode supports 135 different scripts, covering some 1100 languages, and thereâ€™s still a long tail of over 100 unsupported scripts, both modern and historical, which people are still working to add.  Given this enormous diversity, itâ€™s inevitable that representing it is a complicated project. Unicode embraces that diversity, and accepts the complexity inherent in its mission to include all human writing systems. It doesnâ€™t make a lot of trade-offs in the name of simplification, and it makes exceptions to its own rules where necessary to further its mission.  Moreover, Unicode is committed not just to supporting texts in any single language, but also to letting multiple languages coexist within one textâ€”which introduces even more complexity.  Most programming languages have libraries available to handle the gory low-level details of text manipulation, but as a programmer, youâ€™ll still need to know about certain Unicode features in order to know when and how to apply them. It may take some time to wrap your head around it all, but donâ€™t be discouragedâ€”think about the billions of people for whom your software will be more accessible through supporting text in their language. Embrace the complexity!  The Unicode Codespace Letâ€™s start with some general orientation. The basic elements of Unicodeâ€”its â€œcharactersâ€, although that term isnâ€™t quite rightâ€”are called code points. Code points are identified by number, customarily written in hexadecimal with the prefix â€œU+â€, such as U+0041 â€œAâ€ latin capital letter a or U+03B8 â€œÎ¸â€ greek small letter theta. Each code point also has a short name, and quite a few other properties, specified in the Unicode Character Database.  The set of all possible code points is called the codespace. The Unicode codespace consists of 1,114,112 code points. However, only 128,237 of themâ€”about 12% of the codespaceâ€”are actually assigned, to date. Thereâ€™s plenty of room for growth! Unicode also reserves an additional 137,468 code points as â€œprivate useâ€ areas, which have no standardized meaning and are available for individual applications to define for their own purposes.  Codespace Allocation To get a feel for how the codespace is laid out, itâ€™s helpful to visualize it. Below is a map of the entire codespace, with one pixel per code point. Itâ€™s arranged in tiles for visual coherence; each small square is 16Ã—16 = 256 code points, and each large square is a â€œplaneâ€ of 65,536 code points. There are 17 planes altogether.  Map of the Unicode codespace (click to zoom)  White represents unassigned space. Blue is assigned code points, green is private-use areas, and the small red area is surrogates (more about those later). As you can see, the assigned code points are distributed somewhat sparsely, but concentrated in the first three planes.  Plane 0 is also known as the â€œBasic Multilingual Planeâ€, or BMP. The BMP contains essentially all the characters needed for modern text in any script, including Latin, Cyrillic, Greek, Han (Chinese), Japanese, Korean, Arabic, Hebrew, Devanagari (Indian), and many more.  (In the past, the codespace was just the BMP and no moreâ€”Unicode was originally conceived as a straightforward 16-bit encoding, with only 65,536 code points. It was expanded to its current size in 1996. However, the vast majority of code points in modern text belong to the BMP.)  Plane 1 contains historical scripts, such as Sumerian cuneiform and Egyptian hieroglyphs, as well as emoji and various other symbols. Plane 2 contains a large block of less-common and historical Han characters. The remaining planes are empty, except for a small number of rarely-used formatting characters in Plane 14; planes 15â€“16 are reserved entirely for private use.  Scripts Letâ€™s zoom in on the first three planes, since thatâ€™s where the action is:  Map of scripts in Unicode planes 0â€“2 (click to zoom)  This map color-codes the 135 different scripts in Unicode. You can see how Han () and Korean () take up most of the range of the BMP (the left large square). By contrast, all of the European, Middle Eastern, and South Asian scripts fit into the first row of the BMP in this diagram.  Many areas of the codespace are adapted or copied from earlier encodings. For example, the first 128 code points of Unicode are just a copy of ASCII. This has clear benefits for compatibilityâ€”itâ€™s easy to losslessly convert texts from smaller encodings into Unicode (and the other direction too, as long as no characters outside the smaller encoding are used).  Usage Frequency One more interesting way to visualize the codespace is to look at the distribution of usageâ€”in other words, how often each code point is actually used in real-world texts. Below is a heat map of planes 0â€“2 based on a large sample of text from Wikipedia and Twitter (all languages). Frequency increases from black (never seen) through red and yellow to white.  Heat map of code point usage frequency in Unicode planes 0â€“2 (click to zoom)  You can see that the vast majority of this text sample lies in the BMP, with only scattered usage of code points from planes 1â€“2. The biggest exception is emoji, which show up here as the several bright squares in the bottom row of plane 1.  Encodings Weâ€™ve seen that Unicode code points are abstractly identified by their index in the codespace, ranging from U+0000 to U+10FFFF. But how do code points get represented as bytes, in memory or in a file?  The most convenient, computer-friendliest (and programmer-friendliest) thing to do would be to just store the code point index as a 32-bit integer. This works, but it consumes 4 bytes per code point, which is sort of a lot. Using 32-bit ints for Unicode will cost you a bunch of extra storage, memory, and performance in bandwidth-bound scenarios, if you work with a lot of text.  Consequently, there are several more-compact encodings for Unicode. The 32-bit integer encoding is officially called UTF-32 (UTF = â€œUnicode Transformation Formatâ€), but itâ€™s rarely used for storage. At most, it comes up sometimes as a temporary internal representation, for examining or operating on the code points in a string.  Much more commonly, youâ€™ll see Unicode text encoded as either UTF-8 or UTF-16. These are both variable-length encodings, made up of 8-bit or 16-bit units, respectively. In these schemes, code points with smaller index values take up fewer bytes, which saves a lot of memory for typical texts. The trade-off is that processing UTF-8/16 texts is more programmatically involved, and likely slower.  UTF-8 In UTF-8, each code point is stored using 1 to 4 bytes, based on its index value.  UTF-8 uses a system of binary prefixes, in which the high bits of each byte mark whether itâ€™s a single byte, the beginning of a multi-byte sequence, or a continuation byte; the remaining bits, concatenated, give the code point index. This table shows how it works:  UTF-8 (binary)\\tCode point (binary)\\tRange 0xxxxxxx\\txxxxxxx\\tU+0000â€“U+007F 110xxxxx 10yyyyyy\\txxxxxyyyyyy\\tU+0080â€“U+07FF 1110xxxx 10yyyyyy 10zzzzzz\\txxxxyyyyyyzzzzzz\\tU+0800â€“U+FFFF 11110xxx 10yyyyyy 10zzzzzz 10wwwwww\\txxxyyyyyyzzzzzzwwwwww\\tU+10000â€“U+10FFFF A handy property of UTF-8 is that code points below 128 (ASCII characters) are encoded as single bytes, and all non-ASCII code points are encoded using sequences of bytes 128â€“255. This has a couple of nice consequences. First, any strings or files out there that are already in ASCII can also be interpreted as UTF-8 without any conversion. Second, lots of widely-used string programming idiomsâ€”such as null termination, or delimiters (newlines, tabs, commas, slashes, etc.)â€”will just work on UTF-8 strings. ASCII bytes never occur inside the encoding of non-ASCII code points, so searching byte-wise for a null terminator or a delimiter will do the right thing.  Thanks to this convenience, itâ€™s relatively simple to extend legacy ASCII programs and APIs to handle UTF-8 strings. UTF-8 is very widely used in the Unix/Linux and Web worlds, and many programmers argue UTF-8 should be the default encoding everywhere.  However, UTF-8 isnâ€™t a drop-in replacement for ASCII strings in all respects. For instance, code that iterates over the â€œcharactersâ€ in a string will need to decode UTF-8 and iterate over code points (or maybe grapheme clustersâ€”more about those later), not bytes. When you measure the â€œlengthâ€ of a string, youâ€™ll need to think about whether you want the length in bytes, the length in code points, the width of the text when rendered, or something else.  UTF-16 The other encoding that youâ€™re likely to encounter is UTF-16. It uses 16-bit words, with each code point stored as either 1 or 2 words.  Like UTF-8, we can express the UTF-16 encoding rules in the form of binary prefixes:  UTF-16 (binary)\\tCode point (binary)\\tRange xxxxxxxxxxxxxxxx\\txxxxxxxxxxxxxxxx\\tU+0000â€“U+FFFF 110110xxxxxxxxxx 110111yyyyyyyyyy\\txxxxxxxxxxyyyyyyyyyy + 0x10000\\tU+10000â€“U+10FFFF A more common way that people talk about UTF-16 encoding, though, is in terms of code points called â€œsurrogatesâ€. All the code points in the range U+D800â€“U+DFFFâ€”or in other words, the code points that match the binary prefixes 110110 and 110111 in the table aboveâ€”are reserved specifically for UTF-16 encoding, and donâ€™t represent any valid characters on their own. Theyâ€™re only meant to occur in the 2-word encoding pattern above, which is called a â€œsurrogate pairâ€. Surrogate code points are illegal in any other context! Theyâ€™re not allowed in UTF-8 or UTF-32 at all.  Historically, UTF-16 is a descendant of the original, pre-1996 versions of Unicode, in which there were only 65,536 code points. The original intention was that there would be no different â€œencodingsâ€; Unicode was supposed to be a straightforward 16-bit character set. Later, the codespace was expanded to make room for a long tail of less-common (but still important) Han characters, which the Unicode designers didnâ€™t originally plan for. Surrogates were then introduced, asâ€”to put it bluntlyâ€”a kludge, allowing 16-bit encodings to access the new code points.  Today, Javascript uses UTF-16 as its standard string representation: if you ask for the length of a string, or iterate over it, etc., the result will be in UTF-16 words, with any code points outside the BMP expressed as surrogate pairs. UTF-16 is also used by the Microsoft Win32 APIs; though Win32 supports either 8-bit or 16-bit strings, the 8-bit version unaccountably still doesnâ€™t support UTF-8â€”only legacy code-page encodings, like ANSI. This leaves UTF-16 as the only way to get proper Unicode support in Windows. (Update: in Win10 version 1903, they finally added UTF-8 support to the 8-bit APIs! ðŸ˜Š)  By the way, UTF-16â€™s words can be stored either little-endian or big-endian. Unicode has no opinion on that issue, though it does encourage the convention of putting U+FEFF zero width no-break space at the top of a UTF-16 file as a byte-order mark, to disambiguate the endianness. (If the file doesnâ€™t match the systemâ€™s endianness, the BOM will be decoded as U+FFFE, which isnâ€™t a valid code point.)  Combining Marks In the story so far, weâ€™ve been focusing on code points. But in Unicode, a â€œcharacterâ€ can be more complicated than just an individual code point!  Unicode includes a system for dynamically composing characters, by combining multiple code points together. This is used in various ways to gain flexibility without causing a huge combinatorial explosion in the number of code points.  In European languages, for example, this shows up in the application of diacritics to letters. Unicode supports a wide range of diacritics, including acute and grave accents, umlauts, cedillas, and many more. All these diacritics can be applied to any letter of any alphabetâ€”and in fact, multiple diacritics can be used on a single letter.  If Unicode tried to assign a distinct code point to every possible combination of letter and diacritics, things would rapidly get out of hand. Instead, the dynamic composition system enables you to construct the character you want, by starting with a base code point (the letter) and appending additional code points, called â€œcombining marksâ€, to specify the diacritics. When a text renderer sees a sequence like this in a string, it automatically stacks the diacritics over or under the base letter to create a composed character.  For example, the accented character â€œÃâ€ can be expressed as a string of two code points: U+0041 â€œAâ€ latin capital letter a plus U+0301 â€œâ—ŒÌâ€ combining acute accent. This string automatically gets rendered as a single character: â€œÃâ€.  Now, Unicode does also include many â€œprecomposedâ€ code points, each representing a letter with some combination of diacritics already applied, such as U+00C1 â€œÃâ€ latin capital letter a with acute or U+1EC7 â€œá»‡â€ latin small letter e with circumflex and dot below. I suspect these are mostly inherited from older encodings that were assimilated into Unicode, and kept around for compatibility. In practice, there are precomposed code points for most of the common letter-with-diacritic combinations in European-script languages, so they donâ€™t use dynamic composition that much in typical text.  Still, the system of combining marks does allow for an arbitrary number of diacritics to be stacked on any base character. The reductio-ad-absurdum of this is Zalgo text, which works by Í–ÍŸÍ…rÍžaá¹‹Ì«Ì Ì–ÍˆÌ—dÍ–Ì»Ì¹Ã³mÌªÍ™Í•Ì—ÌÄ¼Í‡Ì°Í“Ì³Ì«Ã½Í“Ì¥ÌŸÍ Ì•sÌ«tÌ«Ì±Í•Ì—Ì°Ì¼Ì˜ÍœaÌ¼Ì©Í–Í‡Ì ÍˆÌ£ÍcÌ™ÍkÌ–Ì±Ì¹ÍÍ˜iÌ¢nÌ¨ÌºÌÍ‡Í‡ÌŸÍ™Ä£Ì«Ì®ÍŽÌ»ÌŸÍ… Ì•nÌ¼ÌºÍˆÍžuÌ®Í™mÌºÌ­ÌŸÌ—ÍžeÌžÍ“Ì°Ì¤Í“Ì«rÌµoÌ–á¹·sÒ‰ÌªÍÌ­Ì¬ÌÌ¤ Ì®Í‰ÌÌžÌ—ÌŸÍ dÌ´ÌŸÌœÌ±Í•ÍšiÍ‡Ì«Ì¼Ì¯Ì­ÌœÍ¡á¸Í™Ì»Ì¼cÌ²Ì²Ì¹rÌ¨Ì Ì¹Ì£Ì°Ì¦iÌ±tÌ¤Ì»Ì¤ÍÍ™Ì˜Ì•iÌµÌœÌ­Ì¤Ì±ÍŽcÌµs Í˜oÌ±Ì²ÍˆÌ™Í–Í‡Ì²Í¢nÍ˜ ÌœÍˆeÌ¬Ì²Ì Ì©acÍ•ÌºÌ Í‰hÌ·Ìª ÌºÌ£Í–Ì±á¸»Ì«Ì¬ÌÌ¹á¸™Ì™ÌºÍ™Ì­Í“Ì²tÌžÌžÍ‡Ì²Í‰ÍtÌ·Í”ÌªÍ‰Ì²Ì»Ì Í™eÌ¦Ì»ÍˆÍ‰Í‡rÍ‡Ì­Ì­Ì¬Í–,Ì–Ì ÌœÍ™Í“Ì£Ì­sÌ˜Ì˜ÍˆoÌ±Ì°Ì¤Ì²Í… Ì›Ì¬ÌœÌ™tÌ¼Ì¦Í•Ì±Ì¹Í•Ì¥hÌ³Ì²ÍˆÍÍ…aÌ¦tÌ»Ì² Ì»ÌŸÌ­Ì¦Ì–tÌ›Ì°Ì©hÌ Í•Ì³ÌÌ«Í•eÍˆÌ¤Ì˜Í–ÌžÍ˜yÒ‰ÌÍ™ Ì·Í‰Í”Ì°Ì oÌžÌ°vÍˆÍˆÌ³Ì˜ÍœerÌ¶fÌ°ÍˆÍ”á¸»Í•Ì˜Ì«ÌºÌ²oÌ²Ì­Í™Í Í…wÌ±Ì³Ìº ÍœtÌ¸hÍ‡Ì­Í•Ì³ÍeÌ–Ì¯ÌŸÌ  ÍÌžÌœÍ”Ì©ÌªÍœÄ¼ÍŽÌªÌ²ÍšiÌÌ²Ì¹Ì™Ì©Ì¹nÌ¨Ì¦Ì©Ì–á¸™Ì¼Ì²Ì¼Í¢Í… Ì¬ÍsÌ¼ÍšÌ˜ÌžÍpÍ™Ì˜Ì»aÌ™cÒ‰Í‰ÌœÌ¤ÍˆÌ¯Ì–iÌ¥Í¡nÌ¦Ì Ì±ÍŸgÌ¸Ì—Ì»Ì¦Ì­Ì®ÌŸÍ… Ì³ÌªÌ Í–Ì³Ì¯Ì•aÌ«ÍœnÍdÍ¡ Ì£Ì¦Ì™Í…cÌªÌ—rÌ´Í™Ì®Ì¦Ì¹Ì³eÍ‡ÍšÌžÍ”Ì¹Ì«ÍŸaÌ™ÌºÌ™È›Í”ÍŽÌ˜Ì¹Í…eÌ¥Ì©Í aÍ–ÌªÌœÌ®Í™Ì¹nÌ¢Í‰Ì Í‡Í‰Í“Ì¦Ì¼ÌaÌ³Í–ÌªÌ¤Ì±pÌ–Í”Í”ÌŸÍ‡ÍŽÍ pÌ±ÍÌºÄ™Ì²ÍŽÍˆÌ°Ì²Ì¤Ì«aÌ¯ÍœrÌ¨Ì®Ì«Ì£Ì˜aÌ©Ì¯Í–nÌ¹Ì¦Ì°ÍŽÌ£ÌžÌžcÌ¨Ì¦Ì±Í”ÍŽÍÍ–eÌ¬Í“Í˜ Ì¤Ì°Ì©Í™Ì¤Ì¬Í™oÌµÌ¼Ì»Ì¬Ì»Í‡Ì®ÌªfÌ´ Ì¡Ì™Ì­Í“Í–ÌªÌ¤â€œÌ¸Í™Ì Ì¼cÌ³Ì—ÍœoÍÌ¼Í™Í”Ì®rÌžÌ«ÌºÌžÌ¥Ì¬ruÌºÌ»Ì¯Í‰Ì­Ì»Ì¯pÌ°Ì¥Í“Ì£Ì«Ì™Ì¤Í¢tÌ³ÍÌ³Ì–Í…iÌ¶ÍˆÌÍ™Ì¼Ì™Ì¹oÌ¡Í”nÌ™ÌºÌ¹Ì–Ì©ÍÍ…â€Ì¨Ì—Í–ÍšÌ©.Ì¯Í“  A few other places where dynamic character composition shows up in Unicode:  Vowel-pointing notation in Arabic and Hebrew. In these languages, words are normally spelled with some of their vowels left out. They then have diacritic notation to indicate the vowels (used in dictionaries, language-teaching materials, childrenâ€™s books, and such). These diacritics are expressed with combining marks.  A Hebrew example, with niqqud:\\t×Ö¶×ª ×“Ö·×œÖ°×ªÖ´Ö¼×™ ×”Öµ×–Ö´×™×– ×”Öµ× Ö´×™×¢Ö·, ×§Ö¶×˜Ö¶×‘ ×œÖ´×©Ö°××›Ö·Ö¼×ªÖ´Ö¼×™ ×™Ö¸×©××•Ö¹×“ Normal writing (no niqqud):\\t××ª ×“×œ×ª×™ ×”×–×™×– ×”× ×™×¢, ×§×˜×‘ ×œ×©×›×ª×™ ×™×©×•×“ Devanagari, the script used to write Hindi, Sanskrit, and many other South Asian languages, expresses certain vowels as combining marks attached to consonant letters. For example, â€œà¤¹â€ + â€œ\\u200bà¤¿â€ = â€œà¤¹à¤¿â€ (â€œhâ€ + â€œiâ€ = â€œhiâ€). Korean characters stand for syllables, but they are composed of letters called jamo that stand for the vowels and consonants in the syllable. While there are code points for precomposed Korean syllables, itâ€™s also possible to dynamically compose them by concatenating their jamo. For example, â€œá„’â€ + â€œá…¡â€ + â€œá†«â€ = â€œí•œâ€ (â€œhâ€ + â€œaâ€ + â€œnâ€ = â€œhanâ€). Canonical Equivalence In Unicode, precomposed characters exist alongside the dynamic composition system. A consequence of this is that there are multiple ways to express â€œthe sameâ€ stringâ€”different sequences of code points that result in the same user-perceived characters. For example, as we saw earlier, we can express the character â€œÃâ€ either as the single code point U+00C1, or as the string of two code points U+0041 U+0301.  Another source of ambiguity is the ordering of multiple diacritics in a single character. Diacritic order matters visually when two diacritics apply to the same side of the base character, e.g. both above: â€œÇ¡â€ (dot, then macron) is different from â€œÄÌ‡â€ (macron, then dot). However, when diacritics apply to different sides of the character, e.g. one above and one below, then the order doesnâ€™t affect rendering. Moreover, a character with multiple diacritics might have one of the diacritics precomposed and others expressed as combining marks.  For example, the Vietnamese letter â€œá»‡â€ can be expressed in five different ways:  Fully precomposed: U+1EC7 â€œá»‡â€ Partially precomposed: U+1EB9 â€œáº¹â€ + U+0302 â€œâ—ŒÌ‚â€ Partially precomposed: U+00EA â€œÃªâ€ + U+0323 â€œâ—ŒÌ£â€ Fully decomposed: U+0065 â€œeâ€ + U+0323 â€œâ—ŒÌ£â€ + U+0302 â€œâ—ŒÌ‚â€ Fully decomposed: U+0065 â€œeâ€ + U+0302 â€œâ—ŒÌ‚â€ + U+0323 â€œâ—ŒÌ£â€ Unicode refers to set of strings like this as â€œcanonically equivalentâ€. Canonically equivalent strings are supposed to be treated as identical for purposes of searching, sorting, rendering, text selection, and so on. This has implications for how you implement operations on text. For example, if an app has a â€œfind in fileâ€ operation and the user searches for â€œá»‡â€, it should, by default, find occurrences of any of the five versions of â€œá»‡â€ above!  Normalization Forms To address the problem of â€œhow to handle canonically equivalent stringsâ€, Unicode defines several normalization forms: ways of converting strings into a canonical form so that they can be compared code-point-by-code-point (or byte-by-byte).  The â€œNFDâ€ normalization form fully decomposes every character down to its component base and combining marks, taking apart any precomposed code points in the string. It also sorts the combining marks in each character according to their rendered position, so e.g. diacritics that go below the character come before the ones that go above the character. (It doesnâ€™t reorder diacritics in the same rendered position, since their order matters visually, as previously mentioned.)  The â€œNFCâ€ form, conversely, puts things back together into precomposed code points as much as possible. If an unusual combination of diacritics is called for, there may not be any precomposed code point for it, in which case NFC still precomposes what it can and leaves any remaining combining marks in place (again ordered by rendered position, as in NFD).  There are also forms called NFKD and NFKC. The â€œKâ€ here refers to compatibility decompositions, which cover characters that are â€œsimilarâ€ in some sense but not visually identical. However, Iâ€™m not going to cover that here.  Grapheme Clusters As weâ€™ve seen, Unicode contains various cases where a thing that a user thinks of as a single â€œcharacterâ€ might actually be made up of multiple code points under the hood. Unicode formalizes this using the notion of a grapheme cluster: a string of one or more code points that constitute a single â€œuser-perceived characterâ€.  UAX #29 defines the rules for what, precisely, qualifies as a grapheme cluster. Itâ€™s approximately â€œa base code point followed by any number of combining marksâ€, but the actual definition is a bit more complicated; it accounts for things like Korean jamo, and emoji ZWJ sequences.  The main thing grapheme clusters are used for is text editing: theyâ€™re often the most sensible unit for cursor placement and text selection boundaries. Using grapheme clusters for these purposes ensures that you canâ€™t accidentally chop off some diacritics when you copy-and-paste text, that left/right arrow keys always move the cursor by one visible character, and so on.  Another place where grapheme clusters are useful is in enforcing a string length limitâ€”say, on a database field. While the true, underlying limit might be something like the byte length of the string in UTF-8, you wouldnâ€™t want to enforce that by just truncating bytes. At a minimum, youâ€™d want to â€œround downâ€ to the nearest code point boundary; but even better, round down to the nearest grapheme cluster boundary. Otherwise, you might be corrupting the last character by cutting off a diacritic, or interrupting a jamo sequence or ZWJ sequence.  And Moreâ€¦ Thereâ€™s much more that could be said about Unicode from a programmerâ€™s perspective! I havenâ€™t gotten into such fun topics as case mapping, collation, compatibility decompositions and confusables, Unicode-aware regexes, or bidirectional text. Nor have I said anything yet about implementation issuesâ€”how to efficiently store and look-up data about the sparsely-assigned code points, or how to optimize UTF-8 decoding, string comparison, or NFC normalization. Perhaps Iâ€™ll return to some of those things in future posts.  Unicode is a fascinating and complex system. It has a many-to-one mapping between bytes and code points, and on top of that a many-to-one (or, under some circumstances, many-to-many) mapping between code points and â€œcharactersâ€. It has oddball special cases in every corner. But no one ever claimed that representing all written languages was going to be easy, and itâ€™s clear that weâ€™re never going back to the bad old days of a patchwork of incompatible encodings.  Further reading:  The Unicode Standard UTF-8 Everywhere Manifesto Dark corners of Unicode by Eevee ICU (International Components for Unicode)â€”C/C++/Java libraries implementing many Unicode algorithms and related things Python 3 Unicode Howto Google Noto Fontsâ€”set of fonts intended to cover all assigned code points\"\"\"\n",
    "tokens = text.encode(\"utf-8\") # raw bytes\n",
    "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the `merges` dictionary stores the mappings from a byte pair to a token, (int, int) -> int. \n",
    "\n",
    "The `merges` dictionary can be thought of as a tree where (int, int) are child nodes mapping to a single parent node. In this way, we construct a tree starting at the leaves, moving upward towards multiple roots, constructing a \"binary forest\" instead of a proper binary tree.\n",
    "\n",
    "In the print statement below, we see that the merging begins with the pair (101, 32), which is still the most common pair within the full text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (101, 32) into a new token 256\n",
      "merging (105, 110) into a new token 257\n",
      "merging (115, 32) into a new token 258\n",
      "merging (116, 104) into a new token 259\n",
      "merging (101, 114) into a new token 260\n",
      "merging (99, 111) into a new token 261\n",
      "merging (116, 32) into a new token 262\n",
      "merging (226, 128) into a new token 263\n",
      "merging (44, 32) into a new token 264\n",
      "merging (97, 110) into a new token 265\n",
      "merging (111, 114) into a new token 266\n",
      "merging (100, 32) into a new token 267\n",
      "merging (97, 114) into a new token 268\n",
      "merging (101, 110) into a new token 269\n",
      "merging (257, 103) into a new token 270\n",
      "merging (261, 100) into a new token 271\n",
      "merging (121, 32) into a new token 272\n",
      "merging (46, 32) into a new token 273\n",
      "merging (97, 108) into a new token 274\n",
      "merging (259, 256) into a new token 275\n",
      "original vocab size 256\n",
      "new vocab size: 276\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "  newids = []\n",
    "  i = 0\n",
    "  while i < len(ids):\n",
    "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "      newids.append(idx)\n",
    "      i += 2\n",
    "    else:\n",
    "      newids.append(ids[i])\n",
    "      i += 1\n",
    "  return newids\n",
    "\n",
    "# ---\n",
    "num_tokens = 256                      # original number of tokens\n",
    "vocab_size = 276                      # the desired final vocabulary size\n",
    "num_merges = vocab_size - num_tokens  # 276 - 256 = 20 merges\n",
    "ids = list(tokens)                    # copy so we don't destroy the original list\n",
    "\n",
    "merges = {}                          # mapping from byte pair to token: (int, int) -> int\n",
    "for i in range(num_merges):\n",
    "  stats = get_stats(ids)             # get mapping of pairs to counts\n",
    "  pair = max(stats, key=stats.get)   # get the highest frequency pair\n",
    "  idx = num_tokens + i               # mint a new token with idx one above the highest\n",
    "  print(f\"merging {pair} into a new token {idx}\")\n",
    "  ids = merge(ids, pair, idx)        # replace all instances of pair with the new token idx\n",
    "  merges[pair] = idx                 #\n",
    "\n",
    "print(\"original vocab size\", num_tokens)\n",
    "print(\"new vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 24597\n",
      "ids length: 19438\n",
      "compression ratio: 1.27X\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens length:\", len(tokens))\n",
    "print(\"ids length:\", len(ids))\n",
    "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3) Encoding and decoding\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the Tokenizer is a completely separate, independent module from the LLM. It has its own training dataset of text (which could be different from that of the LLM), on which you train the vocabulary using the Byte Pair Encoding (BPE) algorithm. It then translates back and forth between raw text and sequences of tokens. The LLM later only ever sees the tokens and never directly deals with any text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot 2024-02-14 at 8.23.33â€¯AM.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAACEQAAAHMCAYAAADWRO/2AAAAAXNSR0IArs4c6QAAAJZlWElmTU0AKgAAAAgABAEaAAUAAAABAAAAPgEbAAUAAAABAAAARgEoAAMAAAABAAIAAIdpAAQAAAABAAAATgAAAAAAAACQAAAAAQAAAJAAAAABAASShgAHAAAAEgAAAISgAQADAAAAAQABAACgAgAEAAAAAQAACESgAwAEAAAAAQAAAcwAAAAAQVNDSUkAAABTY3JlZW5zaG90C9c7owAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAAddpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IlhNUCBDb3JlIDYuMC4wIj4KICAgPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4KICAgICAgPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIKICAgICAgICAgICAgeG1sbnM6ZXhpZj0iaHR0cDovL25zLmFkb2JlLmNvbS9leGlmLzEuMC8iPgogICAgICAgICA8ZXhpZjpQaXhlbFlEaW1lbnNpb24+NDYwPC9leGlmOlBpeGVsWURpbWVuc2lvbj4KICAgICAgICAgPGV4aWY6UGl4ZWxYRGltZW5zaW9uPjIxMTY8L2V4aWY6UGl4ZWxYRGltZW5zaW9uPgogICAgICAgICA8ZXhpZjpVc2VyQ29tbWVudD5TY3JlZW5zaG90PC9leGlmOlVzZXJDb21tZW50PgogICAgICA8L3JkZjpEZXNjcmlwdGlvbj4KICAgPC9yZGY6UkRGPgo8L3g6eG1wbWV0YT4Ka6EURAAAABxpRE9UAAAAAgAAAAAAAADmAAAAKAAAAOYAAADmAABL+4IK678AAEAASURBVHgB7N0HuF1VnTfgFUroECFDAiTUoRmUKlKFCAhImaGDQxNpoXeRTxgYQEVwqEYCCCLoiGCAQYYiSi9KkR6kSCc0h6aEGr5ZW872nHPPbeeefe8u736e8ey69lrvf+cZH/fvrj3sk/9bgoUAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgUCKBYQIRJaqmoRAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQKJgECEB4EAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAonYBAROlKakAECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQICAQIRngAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECidgEBE6UpqQAQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgIBAhGeAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQKJ2AQETpSmpABAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAgECEZ4AAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAonYBAROlKakAECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQICAQIRngAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECidgEBE6UpqQAQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgIBAhGeAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQKJ2AQETpSmpABAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAgECEZ4AAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAonYBAROlKakAECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQICAQIRngAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECidgEBE6UpqQAQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgIBAhGeAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQKJ2AQETpSmpABAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAgECEZ4AAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAonYBAROlKakAECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQICAQIRngAABAgQIEBhSgZNOOilMnTp1SPvg5gQIECBAgAABAgQIECBAgACBLARmmGGGMHr06HDEEUdk0bw2CRAgQIAAgV4EBCJ6AXKYAAECBAgQyE7grbfeChMmTMjuBlomQIAAAQIECBAgQIAAAQIECORAYNKkSWGuuebKQU90gQABAgQIVEtAIKJa9TZaAgQIECCQK4G//OUvYf/990/69LWvfS3MO++8ueqfzhAgQIAAAQIECBAgQIAAAQIE2hV4/fXXwy9+8Yvk8okTJ4YRI0a025TrCBAgQIAAgTYFBCLahHMZAQIECBAgMHCB+kDEWWedJRAxcFItECBAgAABAgQIECBAgAABAjkRiIGIAw44IOmNQEROiqIbBAgQIFA5AYGIypXcgAkQIECAQH4EBCLyUws9IUCAAAECBAgQIECAAAECBDorIBDRWU+tESBAgACBdgQEItpRcw0BAgQIECDQEQGBiI4waoQAAQIECBAgQIAAAQIECBDIoYBARA6LoksECBAgUDkBgYjKldyACRAgQIBAfgQEIvJTCz0hQIAAAQIECBAgQIAAAQIEOisgENFZT60RIECAAIF2BAQi2lFzDQECBAgQINARAYGIjjBqhAABAgQIECBAgAABAgQIEMihgEBEDouiSwQIECBQOQGBiMqV3IAJECBAgEB+BAQi8lMLPSFAgAABAgQIECBAgAABAgQ6KyAQ0VlPrREgQIAAgXYEBCLaUXMNAQIECBAg0BEBgYiOMGqEAAECBAgQIECAAAECBAgQyKGAQEQOi6JLBAgQIFA5AYGIypXcgAkQIECAQH4EBCLyUws9IUCAAAECBAgQIECAAAECBDorIBDRWU+tESBAgACBdgQEItpRcw0BAgQIECDQEQGBiI4waoQAAQIECBAgQIAAAQIECBDIoYBARA6LoksECBAgUDkBgYjKldyACRAgQIBAfgQEIvJTCz0hQIAAAQIECBAgQIAAAQIEOisgENFZT60RIECAAIF2BAQi2lFzDQECBAgQINARAYGIjjBqhAABAgQIECBAgAABAgQIEMihgEBEDouiSwQIECBQOQGBiMqV3IAJECBAgEB+BAQi8lMLPSFAgAABAgQIECBAgAABAgQ6KyAQ0VlPrREgQIAAgXYEBCLaUXMNAQIECBAg0BEBgYiOMGqEAAECBAgQIECAAAECBAgQyKGAQEQOi6JLBAgQIFA5AYGIypXcgAkQIECAQH4EBCLyUws9IUCAAAECBAgQIECAAAECBDorIBDRWU+tESBAgACBdgQEItpRcw0BAgQIECDQEQGBiI4waoQAAQIECBAgQIAAAQIECBDIoYBARA6LoksECBAgUDkBgYjKldyACRAgQIBAfgQEIvJTCz0hQIAAAQIECBAgQIAAAQIEOisgENFZT60RIECAAIF2BAQi2lFzDQECBAgQINARAYGIjjBqhAABAgQIECBAgAABAgQIEMihgEBEDouiSwQIECBQOQGBiMqV3IAJECBAgEB+BAQi8lMLPSFAgAABAgQIECBAgAABAgQ6KyAQ0VlPrREgQIAAgXYEBCLaUXMNAQIECBAg0BEBgYiOMGqEAAECBAgQIECAAAECBAgQyKGAQEQOi6JLBAgQIFA5AYGIypXcgAkQIECAQH4EBCLyUws9IUCAAAECBAgQIECAAAECBDorIBDRWU+tESBAgACBdgQEItpRcw0BAgQIECDQEQGBiI4waoQAAQIECBAgQIAAAQIECBDIoYBARA6LoksECBAgUDkBgYjKldyACRAgQIBAfgQEIvJTCz0hQIAAAQIECBAgQIAAAQIEOisgENFZT60RIECAAIF2BAQi2lFzDQECBAgQINARAYGIjjBqhAABAgQIECBAgAABAgQIEMihgEBEDouiSwQIECBQOQGBiMqV3IAJECBAgEB+BAQi8lMLPSFAgAABAp0W+OCDD8LUqVMbml1ggQXC8OHDG/a1s5Fl2x9++GF46aWXWnZr9OjRYZZZZml5rC87u2t7zjnnDPPNN19fmnAOAQIECBAgUCABgYgCFUtXCRAgQKC0AgIRpS2tgREgQIAAgfwLCETkv0Z6SIAAAQIE2hW4++67w6qrrtpw+b333htWWmmlhn3tbGTZ9qOPPhrGjRvXsltnnHFG2H///Vse68vOCy+8MOy6665dTt1jjz3COeec02W/HQQIECBAgECxBQQiil0/vSdAgACBcggIRJSjjkZBgAABAgQKKSAQUciy6TQBAgQIEOiTQJahhSzb7ikQsfbaa4dbbrmlT+NvddLGG28crr322i6HBCK6kNhBgAABAgRKISAQUYoyGgQBAgQIFFxAIKLgBdR9AgQIECBQZAGBiCJXT98JECBAgEDPAlmGFrJsu6dAxLBhw8ILL7wQFlxwwZ4H3+JofCESPxny0UcfdTkqENGFxA4CBAgQIFAKAYGIUpTRIAgQIECg4AICEQUvoO4TIECAAIEiCwhEFLl6+k6AAAECBHoWyDK0kGXbPQUi4ohPP/30cMABB/Q8+BZHJ02aFPbee+8WR0IQiGjJYicBAgQIECi8gEBE4UtoAAQIECBQAgGBiBIU0RAIECBAgEBRBQQiilo5/SZAgAABAr0LZBlayLLt3gIRa621Vrj11lt7B2g6Y/z48eGmm25q2vv3TYGIlix2EiBAgACBwgsIRBS+hAZAgAABAiUQEIgoQRENgQABAgQIFFVAIKKoldNvAgQIECDQu0CWoYUs2+4tEBE/m/H888+HhRZaqHeET8946aWXwtixY8P06dNbXiMQ0ZLFTgIECBAgUHgBgYjCl9AACBAgQKAEAgIRJSiiIRAgQIAAgaIKCEQUtXL6TYAAAQIEehfIMrSQZdutAhFjxowJL7zwQjro/n42I55/0EEHpdc3rwhENIvYJkCAAAEC5RAQiChHHY2CAAECBIotIBBR7PrpPQECBAgQKLSAQEShy6fzBAgQIECgR4EsQwtZtt0qEHHooYeGH/zgB+l411xzzXDbbbel272trL766uGuu+5KTpt11llD/HzGNddck14mEJFSWCFAgAABAqUSEIgoVTkNhgABAgQKKiAQUdDC6TYBAgQIECiDgEBEGapoDAQIECBAoLVAlqGFLNtuFYiIYYYYavjkk0+SwfbnsxnPPPNMWGyxxVKkrbfeOsw444zhkksuSfcJRKQUVggQIECAQKkEBCJKVU6DIUCAAIGCCghEFLRwuk2AAAECBMogIBBRhioaAwECBAgQaC2QZWghy7ZbBSKmTp0attxyy3DnnXemgz3ttNPCgQcemG53t3LSSSeFI488Mj182WWXhUsvvVQgIhWxQoAAAQIEyisgEFHe2hoZAQIECBRHQCCiOLXSUwIECBAgUDoBgYjSldSACBAgQIBAKpBlaCHLtrsLRMQZHQ466KB0fGussUa4/fbb0+3uVlZcccVw//33J4fnnnvu8Morr4Rdd91VIKI7MPsJECBAgECJBAQiSlRMQyFAgACBwgoIRBS2dDpOgAABAgSKLyAQUfwaGgEBAgQIEOhOIMvQQpZtdxeI+Pjjj8PCCy8cpk+fngw5fjbjueeeC2PGjOmOIDz22GNh2WWXTY/vvPPO4cILLwzbb7+9QESqYoUAAQIECJRXQCCivLU1MgIECBAojoBARHFqpacECBAgQKB0AgIRpSupAREgQIAAgVQgy9BClm13F4gYPXp0WGeddcItt9ySjvHUU09tmDUiPfDpynHHHReOPfbYdPc111wTNtpoI4GIVMQKAQIECBAot4BARLnra3QECBAgUAwBgYhi1EkvCRAgQIBAKQUEIkpZVoMiQIAAAQKJQJahhSzb7ikQMXHixLDvvvumFe7tsxlxdog4S0RcRo4cGaZOnRpmmmkmgYhU0AoBAgQIECi3gEBEuetrdAQIECBQDAGBiGLUSS8JECBAgEApBQQiSllWgyJAgAABAolAlqGFLNvuKRDxyiuvhIUWWijEz2fEpafPZjzwwANhhRVWSM6L/zFhwoQQAxVx8cmMhMF/ECBAgACB0gsIRJS+xAZIgAABAgUQEIgoQJF0kQABAgQIlFVAIKKslTUuAgQIECAQQpahhSzb7ikQEeu6/vrrh9/+9rdpibv7bMaRRx4ZTjrppPS8+KmNtddeO9kWiEhZrBAgQIAAgVILCESUurwGR4AAAQIFERCIKEihdJMAAQIECJRRQCCijFU1JgIECBAg8HeBLEMLWbbdWyDi3HPPDXvuuWda5tVXXz3ccccd6XZtZbHFFgvPPPNMsjlmzJjw3HPPJTNKxB0CEQmL/yBAgAABAqUXEIgofYkNkAABAgQKICAQUYAi6SIBAgQIECirgEBEWStrXAQIECBAoLwzRMQXGwsssED46KOPkjLHz2Y8++yzYezYsWnZ77rrrhCDErXl0EMPDaecckptUyAilbBCgAABAgTKLSAQUe76Gh0BAgQIFENAIKIYddJLAgQIECBQSgGBiFKW1aAIECBAgEAikOUsDlm23dsMEXFwG220UbjuuuvSSv/nf/5nOPjgg9Ptgw46KJx++unp9j333BNWXnnldNsMESmFFQIECBAgUGoBgYhSl9fgCBAgQKAgAgIRBSmUbhIgQIAAgTIKCESUsarGRIAAAQIE/i6QZWghy7b7Eoi44IILwm677ZaWerXVVgt33nlnsj19+vRktoiXXnop2V5yySXD448/np4bVwQiGjhsECBAgACB0goIRJS2tAZGgAABAgUSEIgoULF0lQABAgQIlE1AIKJsFTUeAgQIECDwD4EsQwtZtt2XQMSbb74ZRo0aFT744INkwPWfzbjpppvC+PHjU4hjjjkmHHfccel2XBGIaOCwQYAAAQIESisgEFHa0hoYAQIECBRIQCCiQMXSVQIECBAgUDYBgYiyVdR4CBAgQIDAPwSyDC1k2XZfAhFxlJtttln49a9/nQ649tmMCRMmhLPPPjvdP2XKlLDMMsuk23FFIKKBwwYBAgQIECitgEBEaUtrYAQIECBQIAGBiAIVS1cJECBAgEDZBAQiylZR4yFAgAABAv8QyDK0kGXbfQ1EXHzxxWGnnXZKB7zGGmuEW265JSy44ILh1VdfTfavsMIK4Y9//GN6Tm1FIKIm4ZcAAQIECJRbQCCi3PU1OgIECBAohoBARDHqpJcECBAgQKCUAgIRpSyrQREgQIAAgUQgy9BClm33NRDx9ttvJ5/NeO+995Lxxs9mXHjhhWHnnXdOn4CTTjopHHHEEel2bUUgoibhlwABAgQIlFtAIKLc9TU6AgQIECiGgEBEMeqklwQIECBAoJQCAhGlLKtBESBAgACBRCDL0EKWbfc1EBEHueWWW4bLL7+8ZcVjQOLpp58OiyyySJfjAhFdSOwgQIAAAQKlFBCIKGVZDYoAAQIECiYgEFGwgukuAQIECBAok4BARJmqaSwECBAgQKBRIMvQQpZt9ycQcckll4QYbmi1xE9o3H777a0OJdfEa2vLHnvsEc4555zapl8CBAgQIECgJAICESUppGEQIECAQKEFBCIKXT6dJ0CAAAECxRYQiCh2/fSeAAECBAj0JNAqtDDTTDOFOHNCf5eVV1453HnnnellWbbdn0DE3/72tzD//POHd999N+1bbeXMM88M++23X22z4dcMEQ0cNggQIECAQGkFBCJKW1oDI0CAAIECCQhEFKhYukqAAAECBMomIBBRtooaDwECBAgQ+IdAq9DCP472b22VVVYJsb3akmXb/QlExP5st9124Ze//GWta8nvjDPOGF566aUkLNFw4NMNgYhWKvYRIECAAIHyCQhElK+mRkSAAAECxRMQiChezfSYAAECBAiURkAgojSlNBACBAgQINBFIMvQQpZt9zcQMXny5LDVVls1jH+DDTYI119/fcO++g2BiHoN6wQIECBAoLwCAhHlra2RESBAgEBxBAQiilMrPSVAgAABAqUTEIgoXUkNiAABAgQIpAKdDC2st9564YYbbhiUtp9//vmwyCKLhE8++SS939SpU8Po0aPT7fqVadOmhVGjRoV33nkn3X3++eeHr3/96+l288pee+0VzjnnnHT3YYcdFk4++eR02woBAgQIECBQDgGBiHLU0SgIECBAoNgCAhHFrp/eEyBAgACBQgsIRBS6fDpPgAABAgQIECBAgAABAgQI9CAgENEDjkMECBAgQGCQBAQiBgnabQgQIECAAIGuAgIRXU3sIUCAAAECBAgQIECAAAECBMohIBBRjjoaBQECBAgUW0Agotj103sCBAgQIFBoAYGIQpdP5wkQIECAAAECBAgQIECAAIEeBAQiesBxiAABAgQIDJKAQMQgQbsNAQIECBAg0FVAIKKriT0ECBAgQIAAAQIECBAgQIBAOQQEIspRR6MgQIAAgWILCEQUu356T4AAAQIECi0gEFHo8uk8AQIECBAgQIAAAQIECBAg0IOAQEQPOA4RIECAAIFBEhCIGCRotyFAgAABAgS6CghEdDWxhwABAgQIECBAgAABAgQIECiHgEBEOepoFAQIECBQbAGBiGLXT+8JECBAgEChBQQiCl0+nSdAgAABAgQIECBAgAABAgR6EBCI6AHHIQIECBAgMEgCAhGDBO02BAgQIECAQFcBgYiuJvYQIECAAAECBAgQIECAAAEC5RAQiChHHY2CAAECBIotIBBR7PrpPQECBAgQKLSAQEShy6fzBAgQIECAAAECBAgQIECAQA8CAhE94DhEgAABAgQGSUAgYpCg3YYAAQIECBDoKiAQ0dXEHgIECBAgQIAAAQIECBAgQKAcAgIR5aijURAgQIBAsQUEIopdP70nQIAAAQKFFhCIKHT5dJ4AAQIECBAgQIAAAQIECBDoQUAgogcchwgQIECAwCAJCEQMErTbECBAgAABAl0FBCK6mthDgAABAgQIECBAgAABAgQIlENAIKIcdTQKAgQIECi2gEBEseun9wQIECBAoNACAhGFLp/OEyBAgAABAgQIECBAgAABAj0ICET0gOMQAQIECBAYJAGBiEGCdhsCBAgQIECgq4BARFcTewgQIECAAAECBAgQIECAAIFyCAhElKOORkGAAAECxRYQiCh2/fSeAAECBAgUWkAgotDl03kCBAgQIECAAAECBAgQIECgBwGBiB5wHCJAgAABAoMkIBAxSNBuQ4AAAQIECHQVEIjoamIPAQIECBAgQIAAAQIECBAgUA4BgYhy1NEoCBAgQKDYAgIRxa6f3hMgQIAAgUILCEQUunw6T4AAAQIECBAgQIAAAQIECPQgIBDRA45DBAgQIEBgkAQEIgYJ2m0IECBAgACBrgICEV1N7CFAgAABAgQIECBAgAABAgTKISAQUY46GgUBAgQIFFtAIKLY9dN7AgQIECBQaAGBiEKXT+cJECBAgAABAgQIECBAgACBHgQEInrAcYgAAQIECAySgEDEIEG7DQECBAgQINBVQCCiq4k9BAgQIECAAAECBAgQIECAQDkEBCLKUUejIECAAIFiCwhEFLt+ek+AAAECBAotIBBR6PLpPAECBAgQIECAAAECBAgQINCDgEBEDzgOESBAgACBQRIQiBgkaLchQIAAAQIEugoIRHQ1sYcAAQIECBAgQIAAAQIECBAoh4BARDnqaBQECBAgUGwBgYhi10/vCRAgQIBAoQUEIgpdPp0nQIAAAQIECBAgQIAAAQIEehAQiOgBxyECBAgQIDBIAgIRgwTtNgQIECBAgEBXgfpAxPDhw8NnPvOZrifZQ4AAAQIECBAgQIAAAQIECBAooEAMRHz88cdJzydOnBhGjBhRwFHoMgECBAgQKLaAQESx66f3BAgQIECg0AJvvfVWmDBhQqHHoPMECBAgQIAAAQIECBAgQIAAgd4EJk2aFOaaa67eTnOcAAECBAgQ6LCAQESHQTVHgAABAgQI9E/g4YcfDm+++Wb/LnI2AQIECBAgQIAAAQIECBAgQKAAAsOGDUtmhhg3blwBequLBAgQIECgfAICEeWrqRERIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIHKCwhEVP4RAECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBMonIBBRvpoaEQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQqLyAQETlHwEABAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECgfAICEeWrqRERIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIHKCwhEVP4RAECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBMonIBBRvpoaEQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQqLyAQETlHwEABAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECgfAICEeWrqRERIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIHKCwhEVP4RAECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBMonIBBRvpoaEQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQqLyAQETlHwEABAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECgfAICEeWrqRERIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIHKCwhEVP4RAECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBMonIBBRvpoaEQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQqLyAQETlHwEABAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECgfAICEeWrqRERIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIHKCwhEVP4RAECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBMonIBBRvpoaEQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQqLyAQETlHwEABAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECgfAICEeWrqRERIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIHKCwhEVP4RAECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBMonIBBRvpoaEQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQqLyAQETlHwEABAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECgfAICEeWrqRERIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIHKCwhEVP4RAECAAAECBIZWYPLkyeH1118f2k64OwECBAgQIECAAAECBAgQIEAgA4Fhw4aFkSNHhi222CKD1jVJgAABAgQI9CYgENGbkOMECBAgQIBAZgJ/+ctfwv77759Z+xomQIAAAQIECBAgQIAAAQIECORBYOLEiWHEiBF56Io+ECBAgACBSgkIRFSq3AZLgAABAgTyJVAfiBg3bpz/YSBf5dEbAgQIECBAgAABAgQIECBAYAAC//u//xumTJmStCAQMQBIlxIgQIAAgQEICEQMAM+lBAgQIECAwMAE6gMRJ554okDEwDhdTYAAAQIECBAgQIAAAQIECORIIAYijj766KRHAhE5KoyuECBAgEClBAQiKlVugyVAgAABAvkSEIjIVz30hgABAgQIECBAgAABAgQIEOicgEBE5yy1RIAAAQIE2hUQiGhXznUECBAgQIDAgAUEIgZMqAECBAgQIECAAAECBAgQIEAgpwICETktjG4RIECAQKUEBCIqVW6DJUCAAAEC+RIQiMhXPfSGAAECBAgQIECAAAECBAgQ6JyAQETnLLVEgAABAgTaFRCIaFfOdQQIECBAgMCABQQiBkyoAQIECBAgQIAAAQIECBAgQCCnAgIROS2MbhEgQIBApQQEIipVboMlQIAAAQL5EhCIyFc99IYAAQIECBAgQIAAAQIECBDonIBAROcstUSAAAECBNoVEIhoV851BAgQIECAwIAFBCIGTKgBAgQIECBAgAABAgQIECBAIKcCAhE5LYxuESBAgEClBAQiKlVugyVAgAABAvkSEIjIVz30hgABAgQIECBAgAABAgQIEOicgEBE5yy1RIAAAQIE2hUQiGhXznUECBAgQIDAgAUEIgZMqAECBAgQIECAAAECBAgQIEAgpwICETktjG4RIECAQKUEBCIqVW6DJUCAAAEC+RIQiMhXPfSGAAECBAgQIECAAAECBAgQ6JyAQETnLLVEgAABAgTaFRCIaFfOdQQIECBAgMCABQQiBkyoAQIECBAgQIAAAQIECBAgQCCnAgIROS2MbhEgQIBApQQEIipVboMlQIAAAQL5EhCIyFc99IYAAQIECBAgQIAAAQIECBDonIBAROcstUSAAAECBNoVEIhoV851BAgQIECAwIAFBCIGTKgBAgQIECBAgAABAgQIECBAIKcCAhE5LYxuESBAgEClBAQiKlVugyVAgAABAvkSEIjIVz30hgABAgQIECBAgAABAgQIEOicgEBE5yy1RIAAAQIE2hUQiGhXznUECBAgQIDAgAUEIgZMqAECBAgQIECAAAECBAgQIEAgpwICETktjG4RIECAQKUEBCIqVW6DJUCAAAEC+RIQiMhXPfSGAAECBAgQIECAAAECBAgQ6JyAQETnLLVEgAABAgTaFRCIaFfOdQQIECBAgMCABQQiBkyoAQIECBAgQIAAAQIECBAgQCCnAgIROS2MbhEgQIBApQQEIipVboMlQIAAAQL5EhCIyFc99IYAAQIECBAgQIAAAQIECBDonIBAROcstUSAAAECBNoVEIhoV851BAgQIECAwIAFBCIGTKgBAgQIECBAgAABAgQIECBAIKcCAhE5LYxuESBAgEClBAQiKlVugyVAgAABAvkSEIjIVz30hgABAgQIECBAgAABAgQIEOicgEBE5yy1RIAAAQIE2hUQiGhXznUECBAgQIDAgAUEIgZMqAECBAgQIECAAAECBAgQIEAgpwICETktjG4RIECAQKUEBCIqVW6DJUCAAAEC+RIQiMhXPfSGAAECBAgQIECAAAECBAgQ6JyAQETnLLVEgAABAgTaFRCIaFfOdQQIECBAgMCABQQiBkyoAQIECBAgQIAAAQIECBAgQCCnAgIROS2MbhEgQIBApQQEIipVboMlQIAAAQL5EhCIyFc99IYAAQIE+i/w9ttvh7feeiu9cPbZZw/zzTdfum2FAAECBAgQIECgugICEdWtvZETIECAQH4EBCLyUws9IUCAAAEClRMQiKhcyQ2YAAECmQjcdtttYdVVVw3Dhw/PpP2eGj355JPDd7/73fSU7bffPkycODHdtkKAAAECBAgQIFBdAYGI6tbeyAkQIEAgPwICEfmphZ4QIECAAIHKCQhEVK7kBkyAAIGOCjz//PPhqKOOCldffXW48sorw9prr93R9vvSmEBEX5ScQ4AAAQIECBCopoBARDXrbtQECBAgkC8BgYh81UNvCBAgQIBApQQEIipVboMlQIBARwXiLAwnnnhimDZtWtKuQERHeTVGgAABAgQIECDQAQGBiA4gaoIAAQIECAxQQCBigIAuJ0CAAAECBNoXEIho386VBAgQqLrAF7/4xfDEE0+kDAIRKYUVAgQIECBAgACBnAgIROSkELpBgAABApUWEIiodPkNngABAgQIDK2AQMTQ+rs7AQIEiiwgEFHk6uk7AQIECBAgQKAaAgIR1aizURIgQIBAvgUEIvJdH70jQIAAAQKlFhCIKHV5DY4AAQKZCghEZMqrcQIECBAgQIAAgQ4ICER0AFETBAgQIEBggAICEQMEdDkBAgQIECDQvoBARPt2riRAgEDVBQQiqv4EGD8BAgQIECBAIP8CAhH5r5EeEiBAgED5BQQiyl9jIyRAgAABArkVEIjIbWl0jAABArkX6EQg4sMPPwx//vOfw+OPPx6mTp0aFl100TBu3Liw0EIL9Xn8J598cvjud7+bnr/99tuHiRMnptudXon9fPjhh8PTTz8dxo4dGz73uc+FMWPGdOw2Dz30UHjmmWfCK6+8EuaZZ56w/PLLh3/+538OM8wwQ8fu0aqh999/Pzz33HPJuOLv3HPPHZZaaqmw9NJLh9lmm63VJf3al4Xbe++9Fx577LHw7LPPhpdffjnMO++8YZFFFgkrr7xymHHGGfvVv6E4OQuT+nFk8Sy99tpr4amnngrPP/98iC+YFlxwwbD44ouHxRZbLMw+++z1t+/XetYW/eqMkwkQIECgVAICEaUqp8EQIECAQEEFBCIKWjjdJkCAAAECZRAQiChDFY2BAAECgycQwwenn356csNp06aFTz75pOHmtRei8SX+1Vdf3XCsfuP2228Pxx57bHjggQfCRx99VH8oWR8xYkQSBIjnxLZ6WvoTiIj32muvvcJ1113X0OSXv/zl8NOf/rRhX/3GI488kvT3/vvvD/H/dzYvsb8rrbRSOP7448Oyyy7bfLhh+5RTTgmnnXZasm+WWWYJ119/ffJC+bzzzguTJk1KAiINF/zfRnSN7R922GHhS1/6UvPhAW3HMf3oRz8K//3f/x1iKKJ5GTZsWBL8iCGDgw8+OCy33HLNp3S73Um3+pu8+uqr4dxzzw0XXHBB8lK+/lhcHz16dNh3333DhAkTwjnnnBNOOOGE9JSf/OQnYf3110+348pRRx3VUP/99tsvHHnkkQ3ntNrYYIMNwpQpU9JDrdpOD3660kmTwXqW4r/z+O/5wgsvDDfeeGOYPn1687CSwM6WW26ZuMWARF+WTlr05X7OIUCAAIFqCghEVLPuRk2AAAEC+RIQiMhXPfSGAAECBAhUSkAgolLlNlgCBAgMWOA73/lOiC9he1tGjhyZzPrQfN4LL7wQjj766HDllVc2H2q5PdNMM4VDDz00HHLIIWHmmWdueU5fAxEff/xx2HPPPcPll1/e0E6cfSGGAeJL9OYlvgiOs03EF+qtwgLN58eAwzHHHBP23nvvEIMErZY4m0Xsc235r//6r3D22WeHm2++ubarx99vfvObIf5fJ5b4knuXXXZp+YK7VftxTPHeRxxxRKvD6b4s3GqN33fffWGHHXYIcaaC3pZ11103rL766g0ziETvDTfcsOHS+IzFcEVtOeigg5I61ra7+11rrbXCo48+mh5u1XbtYBYmg/Esvf322y1DRLVxNf/Gf7Pf+ta3kvBM87HadhYWtbb9EiBAgACBZgGBiGYR2wQIECBAYPAFBCIG39wdCRAgQIAAgU8FBCI8CgQIECDQH4GBBCLeeOONZHaDF198sT+3TM6Ns0RcddVVYc455+xybV8CEfEv2mNI4bLLLmu4fplllglXXHFFmH/++Rv2x434OYb4+Y1bbrmly7HedsQZCOLL8VafbWh+iR0/RxFn2+jPEgMla6+9dn8u6XLuPffcEzbffPNknF0O9rIjznCx8847tzwrK7d4szvuuCNss802/faq72ir0ELWgYisTLJ+luJMHJtssknyiYx6w76sxyBR/PfTvGRl0Xwf2wQIECBAoCYgEFGT8EuAAAECBIZOQCBi6OzdmQABAgQIVF5AIKLyjwAAAgQI9EsgTnFf+4v4+OmJ+iXO4rD00ksnu2adddaw2Wab1R8OO+20U5fPaMw111xJUCF+jmGRRRZJZpW46667Qvx8xIcffthw/T777NPw6YPawd4CETEMET+fcMkll9QuSX4/97nPhcmTJ4f55puvYX9tI86EEQMg9UucASK2teaaa4ZFF100PPPMMyEGC+InJ9588836U8MPfvCD8PWvf71hX9xofoldf8KoUaOSoEH87Ma7774b4qdF4ic1mj/TsemmmzZ84qG+jb6ujx8/PvlkSe38eeaZJ+y+++5h4403TmbLiDNqPP3008nMCXEGjfhX/bUl9jN+aiN6NC9ZucUZOuKMDE899VTDLeMnRA444ICwyiqrJDWIZvHTJS+//HLDebWNoQhEZGWS9bP0jW98o8uMKvHzLfHfwBe+8IUQZ1eJM3X8/Oc/DxdddFHDTCPDhw9PPq/R/AmZrCxq9fVLgAABAgSaBQQimkVsEyBAgACBwRcQiBh8c3ckQIAAAQIEPhUQiPAoECBAgEC7Al/84hfDE088kV7e06wF5557bpfPPHz+859PXrYvtthiaRu1lXvvvTcJE8RPbNSWONvCTTfdFMaNG1fblfz2FIiIYYj4sjy+sK1fVlxxxfCrX/0qjBgxon53uv7SSy+FVVddNQkl1HbGAEUMaiy55JK1XelvDEbEGRMefvjhdF8MWtx9991d7tHdS+zddtstHHvssV1mwYgBgK985SshzrBRW6LFgw8+GBZYYIHarn79xs8gLL744ukL7BhsuO6660KsSavlrLPO6vIJiTizRgwj1C9ZurV6kR5f2EfP+JmG+mXq1Knha1/7WkPgo3Z8sAMRWZpk+SzdcMMNYdttt62xJb9xRpWf/OQnYamllmrYHzeuueaasOOOOzYEZyZMmBBOPPHE9NwsLdKbWCFAgAABAk0CAhFNIDYJECBAgMAQCAhEDAG6WxIgQIAAAQJ/FxCI8CQQIECAQLsC/QlExJkj4l+S15b4iYr77rsvxL82726JYYj4V+hxZoDaEkMK8cXrsGHDartCd4GIOKPBgQceGC6++OL03LgS27j00ktDnJ2iu2XPPfds+LzGHHPMkczWsPDCC3d3SfjrX/+azFIQPzNQW+IsGvGldf3S6iX2CiusEOIL6BlmmKH+1HT95ptvDltssUW6HVdavdhvOKGHjd/85jdhu+22S8+InyS58cYb0+1WK6uttloyg0ft2JFHHhmOOOKI2mbym6VbDGvUB2TidgzIdLfEAEkcV6xL/dLKLctPZmRpkuWz9G//9m/Jv7WaXQzNxFlB4uwg3S3HHHNMiOGZ2hL/ncdZZWqfjsnSonZPvwQIECBAoFlAIKJZxDYBAgQIEBh8AYGIwTd3RwIECBAgQOBTAYEIjwIBAgQItCvQ10DEk08+mYQQ6u8TQwzxr/t7W44++ujwwx/+sOG0Bx54IIwdOzbd1yoQEa+JL7njX7PXL/FTF7/4xS9CDDh0t8T/0Tx+CqB+Oemkk8Iee+xRv6vl+o9//ONw+OGHp8fmnnvuEGePqF9avcSOszPE8EdPS/yr/Ndffz09Jb54jrMgtLPE8MNWW22VXjrzzDOHW2+9teVf/tdOuuqqq5JARJzRI34uJM6UUR8qydItzoax7rrr1rqS/Mbabr755g37mjfipzNOPfXUht2DGYjI0iQOKqtn6b333gtLLLFEmDZtWmrXPNtDeqBu5dlnnw0rrbRSOktEfD5i+CY+u1lb1HXDKgECBAgQaBCI/z8o/nfKuEycOLHL7F0NJ9sgQIAAAQIEMhEQiMiEVaMECBAgQIBAXwQEIvqi5BwCBAgQaCXQ10DERRddlMzUUGsjfuYh/qV5fAnf2xL/B+z4iYz6WSLipy7Gjx+fXtociIgzH8w555whhhPql/hC/Wc/+1mYbbbZ6nd3Wb/nnnuST1TUH5gyZUqPfxlfO/fll18On/3sZ2ubye/jjz8eRo4cme5rfokd+/rcc8+lx7tbiZ/NiH2rLfFF/y677FLb7NdvfOEdgw31rtElhjm23nrrMGbMmH61F0/O0i2ONYYbasuCCy6YfDKkuxk1aufF/54TP3USx1tbBjMQkaVJHE9Wz1Krz2U89thjIc740NsSgzPxUzQxBFE/m0TWFr31y3ECBAgQqK6AQER1a2/kBAgQIJAfAYGI/NRCTwgQIECAQOUEBCIqV3IDJkCAQMcE+hqI2GeffZJZGWo33nDDDZPPPdS2e/uNszrEQEJtiS+B46coaktzIKK2v/n3D3/4Q5eZH5rPiduXXHJJiH8NX79cdtll9Zs9rsdPDdQHDa699tqGGTKaX2Ivu+yyyec4emz0/w7GGR3qP2vx/e9/P+y+++69Xdbt8S233LLbT07El9nrr79++PKXvxzWWGONMOuss3bbTu1Alm6HHHJIiDNC1JZ11lknXH755bXNHn+/9KUvhYcffjg9ZzADEVmaxAFl9Sw1z3QSAz0x2DOQJWuLgfTNtQQIECBQbgGBiHLX1+gIECBAoBgCAhHFqJNeEiBAgACBUgoIRJSyrAZFgACBQRHoayAivrx+6KGH0j7FT0/ET1D0dYmfhYihgtoSP7URQxC1pa+BiNiPyZMnh2HDhtUubfnb/JK55Un92Bk/37HDDjukVzS3H2d+iJ/x6G2JbcRPa9SWgQYi4iwdcTaN1157rdZky98Yhoihgs022yxssskm3U4z3Tyulo31Y2e920477RSuvvrq9Opo0fwplfRg00rz8zOYgYgsTeIwm9vv1LMU/33W/xtdfvnlG8I4TcR92mzua58u6uGk+uejh9McIkCAAAECyWebfDLDg0CAAAECBIZWQCBiaP3dnQABAgQIVFpAIKLS5Td4AgQIDEigr4GIOMNAnG6/tnz7298O8S/++7p885vfDOeee256+nrrrRcuvfTSdLuvgYh4wSmnnBJ222239NpWKzFw0dfZB1pd37zvsMMOC0cddVS6u/nFcJxR4swzz0yPd7fS6UBEvE/8VMe2227b57/+n2WWWUIcT6xfc7AkS7eNN944/P73v09p4qwjJ5xwQrrd00rz8zOYgYgsTeKYs3qWYo3PP//8lPWrX/1quPjii9PtdlaytminT64hQIAAgWoImCGiGnU2SgIECBDIt4BARL7ro3cECBAgQKDUAgIRpS6vwREgQCBTgb4GImKA4Y9//GPalxgOiC9c+7oceuih4YILLkhPb345210gojYzRfwfwWvLHHPMkXyeYuGFF67t6vLbHDzockI/d+y3337hP/7jP9Krml9i77jjjuGMM85Ij3e30tyvgc4QUbvPO++8Ey666KIQPwsSZ43oy7LpppuGiRMnhjnnnDM9vbl/6YE2V+rdtt566/C73/0ubak/gYhof9ppp6XXdjIQ0fw5l+a2szSJA8rqWWr+NxdnBonPyECWrC0G0jfXEiBAgEC5BQQiyl1foyNAgACBYggIRBSjTnpJgAABAgRKKSAQUcqyGhQBAgQGRaCvgYjmzx3sueee4Xvf+16f+xhnMLjhhhvS82N7p59+errdKhCxzTbbJC/s/+d//ifssssu6blxZe211w5XXHFFlxkOaid961vfCpMmTapthtVWW61f/U0v/HRl/vnnD6NHj053Z/USO73BAFaef/75xPrmm28O8f/eeuutblv713/914ZZBLJ0a57lYauttmqYNaTbTv7fgRieqP8kSXNoIV7bHADYf//9w3HHHddTs8mxz3/+8+GFF15Iz2tuO0uTeNOsnqU4Y8m///u/p+NaccUVw29/+9t0u52VrC3a6ZNrCBAgQKAaAgIR1aizURIgQIBAvgUEIvJdH70jQIAAAQKlFhCIKHV5DY4AAQKZCvQ1EBFfrNZ/EiJ+/uBnP/tZn/u2+uqrhz/96U/p+fFzDfGzG7WlORARAw/xkxczzDBDckoMRFx11VW105PfnmZXOO+888IRRxyRnr/YYouFe++9N90e6EpWL7EH2q/m6z/++ONk3L/5zW+SQMGLL77YcEr0feSRR8KoUaOS/Vm6xYBKfKFeW1ZdddVw7bXX1jZ7/N1ss82SWUFqJzWHFuL+5kDE3nvvHb7zne/ULun2d9FFFw1vv/12ery57SxN4k2zepZ+/etfh5133jkdV6zxlClT0u2eVp599tnw1FNPhTgLy9ixY0P8zEpcsrboqU+OESBAgEC1BQQiql1/oydAgACBfAgIROSjDnpBgAABAgQqKSAQUcmyGzQBAgQ6ItDXQMRPf/rTcNBBB6X3HDNmTPIJjRlnnDHd193KG2+8ET772c+G999/Pz3lpJNOCnvssUe63RyI2H777ZPZIWonvPLKK8ksD/WzHcw+++zhtttuC/GFdvNy0003hS233DLdPWzYsPDnP/85zDPPPOm+nlauv/765HDthXD8TEf9ktVL7Pp79Lb+ySefJDMbPPHEE8nL6y222CKMHDmy28tiOCKGROo/XRJPjjNtfOlLX0quy9ItzhASZwqpLSNGjEjCGLPNNlttV8vf6dOnh+WWWy68/PLL6fHm0EI8cOSRR4ZzzjknPad5FpL0QN1KbDM+m/VLc9tZmsT7ZvUsPfroo2GttdZKhxbDL08++WSI7r0t8d9OHHdc4nVxdo/DDz882Zflv6vkhv6DAAECBAi0EBCIaIFiFwECBAgQGGQBgYhBBnc7AgQIECBA4B8CAhH/sLBGgAABAv0TaA5ExFkZ1llnnS6NxODB5ptv3rD/jDPOCDvuuGPDvlYb8bMF9Z/HiOfcfffdYYkllkhP7y0QEU+MM1LEzyDUL/GF75VXXtnl0xnxsxHLL798/anh4IMPDkcffXTDvlYbt99+e4gzEtQvEydODDGkUVuyeolda78vv821O/XUU7t8WqS5nffeey8stNBCIYYpasv5558f4qcz4pKl2zPPPBNWWmml2m2T3+OPPz7su+++DfuaNy677LIQP9FSvzSHFuKxY445Jpx11lnpaeuuu26YPHlyut1qJX6GI36Oo35pbjtLk3jfrJ6ladOmhRhcqq91DDbE/+tpic9InFGlPsD0q1/9KowfPz7T56OnPjlGgAABAgQEIjwDBAgQIEBg6AUEIoa+BnpAgAABAgQqKyAQUdnSGzgBAgQGLBDDDw899FDazs9//vOw0UYbpdu1lfiSNL6Ajy+Ha0t8sX7PPfek0+nX9tf/vvrqq2HFFVcM8eVsbYl/7X/LLbfUNpPfvgQi4olxFoSbb7654drm2SbiwfgSeIMNNgj33Xdfeu7w4cPDrbfeGpZccsl0X/NKnEUhBj/uvPPO9FCciSJ+VqJ+domsXmKnN+3Dym677ZbM7lA7dbXVVgvxMwm1z4zU9tf/xnoss8wy9buSz1bEz1fEJWu3HXbYIVx33XXp/f/pn/4pmWkkGrdaYj3iuOLnG+qX5tBCPBbDEDEUUVuiQ7zXyiuvXNvV8Buf6fhplt7aztoky2dp6623Dr/73e/SccfZIR544IEw11xzpfuaV2KwJgZVasvcc88d4iwkM888c+bPR+2efgkQIECAQLOAQESziG0CBAgQIDD4AgIRg2/ujgQIECBAgMCnAgIRHgUCBAgQaFcghh/+8Ic/pJfHv8T/3ve+l27Xr1x11VVdZiCIL6t//OMfhwUWWKD+1GT9T3/6U3L+448/3nAsvgDea6+9Gvb1NRARZxlYc801GwIW8WV6DDrEv2qvX2IYIoYi6v9CPs5KET+rEEMazUs8L85AEUMh9UsMHpxyyin1uzL7q/6Gm/Sy0aoePc2CEce36667hnhdbZlllllCrFN86V1bsnSLn2yI9fvwww9rtwsxjBE/yTL//POn++LKu+++G/bbb7+G0EfthFaBiFYzeyy77LLJZx7iy/z65c033wzf+MY3wo033li/O1lv1XaWJlkGImJtY+jjo48+SscZZ1U577zzunjHE+64444kdFRfn2222SZMmjQpvT5Li/QmVggQIECAQJOAQEQTiE0CBAgQIDAEAgIRQ4DulgQIECBAgMDfBQQiPAkECBAg0K5A81/sx3biC+oFF1wwzDjjjEl4YNiwYWnzrWZoGDlyZDjwwAPDKquskoQS4kvYu+66K/lMRnypXb/El7PxsxzNsxj0NRAR2/rhD3/Y5dMXa6yxRvKiv76v8dzYr4suuiiupstMM80UJkyYEOInFcaNGxfiTAHxJW/8y/j62TLiBfEv6eNL88UXXzy9Pq5k+RK74UY9bMR+x8+CvPbaaw1nfeELXwgHHHBAiGGAGDKIx+O4TjvttHD//fc3nBtDAdG+ecnKLd7n29/+doifIKlf4vN2yCGHJLOQzDbbbMnMI2eeeWYyM0f9ebX1VqGFeCwGdJoDONFo7733TmYGeeedd5LZSWIAI/73p1ZLd21nZZL1sxQ/kXHuuec2DHXUqFHh0EMPTT5hsvDCC4cYNPrlL38Z4udTpk+fnp4777zzJs//2LFj031xJSuLhpvYIECAAAECdQICEXUYVgkQIECAwBAJCEQMEbzbEiBAgAABAiH5H/Rr31Q/8cQTQ5wO2UKAAAECBPoiEGeD+P73v9/tqQ8++GAYM2ZMejxOnb/pppt2eQmfntDDSnzxes0117ScTaI/gYj4GYWvfOUryacW6m/XauaJ+NI7zoLR/FmE+uu6W4+zJ1x22WXJjAbN52T9Erv5ft1t//73vw//8i//Ej744IPuTul2/6KLLpp8LqN5ZoZ4QVZuse233normYWgOZwRj/V16S60EF/8xwBAf5b55puvIRzRXdtZmWT9LL3xxhth/fXXD08//XR/WJLQUnz+Y3CoecnKovk+tgkQIECAQE1AIKIB8TryAAALu0lEQVQm4ZcAAQIECAydgEDE0Nm7MwECBAgQqLxA/B+lBSIq/xgAIECAQFsCcfaAr371q90GBi699NKw3nrrNbQd///O4Ycf3vJTBg0n1m1svPHGycwO3YX2+hOIiM0++uijYfz48Q2fXogzC8RPZzTP5jBt2rRw/PHHJ9P+138+o657XVY/85nPJLMYbLjhhl2OxR1Zv8RuedNudl5yySXJX+z3JxSx9NJLh3hdDKl0t2ThVrvX+++/n8wUET+30tuy2WabhYcffrjhhX53oYUYlomfY5k8eXJvzYY4m0j8JMfo0aPD//t//y89v7u24wlZmAzGs/S3v/0tHHXUUV1mS0kH3bQS/53GkG2cQaa7JQuL7u5lPwECBAgQEIjwDBAgQIAAgaEXEIgY+hroAQECBAgQqKyAQERlS2/gBAgQ6IjACy+8EHbbbbfkMwXNDcagQvysQqvliiuuSD63EGeN+Oijj7qcEj+LEUMLu+66a9hkk026HK/f0RyI2HPPPUOcvaKnpflFcjx3u+22Cz/60Y9aXnbnnXeGY489Nnm5Hl/mtlpiEGKfffZJXqrPOeecrU5J9jV/tiNec8IJJ3R7fu1A8ydKzj777LDtttvWDrf9G2sYDWPIoadgxDLLLBN23nnnsPvuu4f46ZC+LJ10a75fDC6ccsopyWcu6j/VEM+Ln9GIfY3hm/iplRiCqS09hRZi6CXObHDWWWcl18SQRPOy4oorJgGBGPaJNYhhgdpy9dVXh9VXX7222fK3kyaD+Sxde+21SdAhflbkww8/7DK2+JmcaB494swZfVk6adGX+zmHAAECBKopIBBRzbobNQECBAjkS0AgIl/10BsCBAgQIFApAYGISpXbYAkQIJCZwMsvvxzii/V33nknjBw5MvlURgwI9LbEF/AxFPHII4+EF198McRPMMTPbMSX76NGjert8kE/Hl+8P/vss2HKlCnhySefDHPMMUeIn49YZJFFkhkThg8fPuh96tQN40vu+HmQxx57LDz33HNJUCX+tX+sybhx48Jiiy3W9q2ydPvrX/8a7rvvvvDQQw+FeeedN6yxxhpJPWqdXWuttfociKhdE39j8CW2GT/PEW0WWGCBsNxyy4Wlllqq/rS217M0abtTfbgw/puNoYg480b8dx9nComzhiyxxBJh1lln7UMLXU8pqkXXkdhDgAABAnkUEIjIY1X0iQABAgSqJiAQUbWKGy8BAgQIEMiRgEBEjoqhKwQIECBAgEDHBdoNRHS8IxokQIAAAQIEhkRAIGJI2N2UAAECBAg0CAhENHDYIECAAAECBAZTQCBiMLXdiwABAgQIEBhsAYGIwRZ3PwIECBAgkC8BgYh81UNvCBAgQKCaAgIR1ay7URMgQIAAgVwICETkogw6QYAAAQIECGQkIBCREaxmCRAgQIBAQQQEIgpSKN0kQIAAgVILCESUurwGR4AAAQIE8i0gEJHv+ugdAQIECBAgMDABgYiB+bmaAAECBAgUXUAgougV1H8CBAgQKIOAQEQZqmgMBAgQIECgoAICEQUtnG4TIECAAAECfRIQiOgTk5MIECBAgEBpBQQiSltaAyNAgACBAgkIRBSoWLpKgAABAgTKJiAQUbaKGg8BAgQIECBQLyAQUa9hnQABAgQIVE9AIKJ6NTdiAgQIEMifgEBE/mqiRwQIECBAoDICAhGVKbWBEiBAgACBSgqsu+664cEHH0zHfumll4b11lsv3bZCgAABAgQIlFtAIKLc9TU6AgQIECiGgEBEMeqklwQIECBAoJQCAhGlLKtBESBAgAABAp8KPPXUUyH+953assIKK4Thw4fXNv0SIECAAAECJRcQiCh5gQ2PAAECBAohIBBRiDLpJAECBAgQKKeAQEQ562pUBAgQIECAAAECBAgQIECAQAgCEZ4CAgQIECAw9AICEUNfAz0gQIAAAQKVFRCIqGzpDZwAAQIECBAgQIAAAQIECJReQCCi9CU2QAIECBAogIBARAGKpIsECBAgQKCsAgIRZa2scREgQIAAAQIECBAgQIAAAQICEZ4BAgQIECAw9AICEUNfAz0gQIAAAQKVFRCIqGzpDZwAAQIECBAgQIAAAQIECJReQCCi9CU2QAIECBAogIBARAGKpIsECBAgQKCsAgIRZa2scREgQIAAAQIECBAgQIAAAQICEZ4BAgQIECAw9AICEUNfAz0gQIAAAQKVFRCIqGzpDZwAAQIECBAgQIAAAQIECJReQCCi9CU2QAIECBAogIBARAGKpIsECBAgQKCsAgIRZa2scREgQIAAAQIECBAgQIAAAQICEZ4BAgQIECAw9AICEUNfAz0gQIAAAQKVFRCIqGzpDZwAAQIECBAgQIAAAQIECJReQCCi9CU2QAIECBAogIBARAGKpIsECBAgQKCsAgIRZa2scREgQIAAAQIECBAgQIAAAQICEZ4BAgQIECAw9AICEUNfAz0gQIAAAQKVFRCIqGzpDZwAAQIECBAgQIAAAQIECJReQCCi9CU2QAIECBAogIBARAGKpIsECBAgQKCsAgIRZa2scREgQIAAAQIECBAgQIAAAQICEZ4BAgQIECAw9AICEUNfAz0gQIAAAQKVFRCIqGzpDZwAAQIECBAgQIAAAQIECJReQCCi9CU2QAIECBAogIBARAGKpIsECBAgQKCsAgIRZa2scREgQIAAAQIECBAgQIAAAQICEZ4BAgQIECAw9AICEUNfAz0gQIAAAQKVFRCIqGzpDZwAAQIECBAgQIAAAQIECJReQCCi9CU2QAIECBAogIBARAGKpIsECBAgQKCsAvWBiHHjxoURI0aUdajGRYAAAQIECBAgQIAAAQIECFRMIAYipkyZkox64sSJ/nePitXfcAkQIEAgHwICEfmog14QIECAAIFKCtQHIioJYNAECBAgQIAAAQIECBAgQIBAJQQEIipRZoMkQIAAgRwKCETksCi6RIAAAQIEqiQwefLk8Prrr1dpyMZKgAABAgQIECBAgAABAgQIVERg2LBhYeTIkWGLLbaoyIgNkwABAgQI5EtAICJf9dAbAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAoAMCAhEdQNQEAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgkC8BgYh81UNvCBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgQ4ICER0AFETBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQL4EBCLyVQ+9IUCAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBDogIBDRAURNECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAvkSEIjIVz30hgABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEOiAgEBEBxA1QYAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECORLQCAiX/XQGwIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQKADAgIRHUDUBAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIJAvAYGIfNVDbwgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIEOCAhEdABREwQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgEC+BAQi8lUPvSFAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQ6ICAQ0QFETRAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQL5EhCIyFc99IYAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBDogIBARAcQNUGAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAjkS0AgIl/10BsCBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECgAwL/HwAA///LeEqTAABAAElEQVTs3QfcFMX9x/EfKIiCKIogNhCwUYyKovI3ir1ib7EgxS52bBhr1KixgYANuzGWiCWiRLGiBltiRRFsiIotArGgWP5+V2fZ22fvbu+4vdu95zOvl7kts7sz79nbe8L8dqbJz78kIyGAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAnUk0ISAiDpqTaqCAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAp4AARHcCAgggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCBQdwIERNRdk1IhBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEECAggnsAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBOpOgICIumtSKoQAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACBERwDyCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIBA3QkQEFF3TUqFEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQICACO4BBBBAAAEEEEAAAQQQQACB1AlMmjTJfvjhh9SViwIhgAAChQSaNWtm66+/fqEs7EMAAQQQQAABBBBAAAEEEKiiAAERVcTmUggggAACCCCAAAIIIIAAAsUFFAwxYsSI4hnJgQACCKRQ4JhjjrHevXunsGQUCQEEEEAAAQQQQAABBBBofAIERDS+NqfGCCCAAAIIIIAAAggggECqBZ566ikbPXq0V8b27dunuqwUDgEEEHACn3zyibc4ZMgQ69Onj9vMJwIIIIAAAggggAACCCCAQA0FCIioIT6XRgABBBBAAAEEEEAAAQQQaCjgAiIUDHHppZc2zMAWBBBAIIUCRx99tH322WdGQEQKG4ciIYAAAggggAACCCCAQKMVICCi0TY9FUcAAQQQQAABBBBAAAEE0ilAQEQ624VSIYBAYQECIgr7sBcBBBBAAAEEEEAAAQQQqIUAARG1UOeaCCCAAAIIIIAAAggggAACeQUIiMhLww4EEEixAAERKW4cioYAAggggAACCCCAAAKNVoCAiEbb9FQcAQQQQAABBBBAAAEEEEinAAER6WwXSoUAAoUFCIgo7MNeBBBAAAEEEEAAAQQQQKAWAgRE1EKdayKAAAIIIIAAAggggAACCOQVICAiLw07EEAgxQIERKS4cSgaAggggAACCCCAAAIINFoBAiIabdNTcQQQQAABBBBAAAEEEEAgnQIERKSzXSgVAggUFiAgorAPexFAAAEEEEAAAQQQQACBWggQEFELda6JAAIIIIAAAggggAACCCCQV4CAiLw07EAAgRQLEBCR4sahaAgggAACCCCAAAIIINBoBQiIaLRNT8URQAABBBBAAAEEEEAAgXQKEBCRznahVAggUFiAgIjCPuxFAAEEEEAAAQQQQAABBGohQEBELdS5JgIIIIAAAggggAACCCBQI4EvvvjCll566RpdPd5lCYiI50QuBBBIl0BWAiJmzZplH374oS2yyCLWtWvXdCFSGgQQQAABBBBAAAEEEECgwgIERFQYlNMhgAACCCCAAAIIIIAAAmkV+Oabb+z++++3PffcM61F9MpFQESqm4fCIYBAHoGsBERMnDjRrrjiCuvQoYNdfPHFeWrDZgQQQAABBBBAAAEEEECgPgQIiKiPdqQWCCCAAAIIIIAAAggggEBRgUceecQLiLj00kuL5q1lBgIiaqnPtRFAoFwBAiLKleM4BBBAAAEEEEAAAQQQQCA5AQIikrPlzAgggAACCCCAAAIIIIBAqgTOOeccmzx5sp166qnWvXv3VJUtWBgCIoIaLCOAQFYECIjISktRTgQQQAABBBBAAAEEEGhMAgRENKbWpq4IIIAAAggggAACCCDQaAVmzpxpxx13nFd/BUMoKCKtiYCItLYM5UIAgUICBEQU0mEfAggggAACCCCAAAIIIFAbAQIiauPOVRFAAAEEEEAAAQQQQACBqgrcfffdduedd3rXXHTRRW3MmDHWpEmTqpYh7sUIiIgrRT4EEEiTAAERaWoNyoIAAggggAACCCCAAAII/CpAQAR3AgIIIIAAAggggAACCCDQCAROPPFEmzFjhl/TIUOGWJ8+ffz1NC0QEJGm1qAsCCAQV4CAiLhS5EMAAQQQQAABBBBAAAEEqidAQET1rLkSAggggAACCCCAAAIIIFATgSlTpthZZ52Vc+11113Xn0IjZ0cKVgiISEEjUAQEEChZgICIksk4AAEEEEAAAQQQQAABBBBIXICAiMSJuQACCCCAAAIIIIAAAgggUFuBG264wR566KEGhbjiiitsiSWWaLC91hsIiKh1C3B9BBAoR4CAiHLUOAYBBBBAAAEEEEAAAQQQSFaAgIhkfTk7AggggAACCCCAAAIIIFBzgUMPPdTmzJnToBz9+/e3bbbZpsH2Wm8gIKLWLcD1EUCgHAECIspR4xgEEEAAAQQQQAABBBBAIFkBAiKS9eXsCCCAAAIIIIAAAggggEBNBZ5//nm79NJLI8uwyiqrNJhKIzJjlTcSEFFlcC6HAAIVESAgoiKMnAQBBBBAAAEEEEAAAQQQqKgAAREV5eRkCCCAAAIIIIAAAggggEC6BEaMGGGTJk3KW6jzzz/fVlpppbz7a7GDgIhaqHNNBBBYUAECIhZUkOMRQAABBBBAAAEEEEAAgcoLEBBReVPOiAACCCCAAAIIIIAAAgikQuDrr7+2gw46qGBZdt55Z9tzzz0L5qn2TgIiqi3O9RBAoBICBERUQpFzIIAAAggggAACCCCAAAKVFSAgorKenA0BBBBAAAEEEEAAAQQQSI3AhAkT7LrrritYnvbt2+edUqPggQnuJCAiQVxOjQACiQkQEJEYLSdGAAEEEEAAAQQQQAABBMoWICCibDoORAABBBBAAAEEEEAAAQTSLXDOOefY5MmTixZy2LBh1qNHj6L5qpWBgIhqSXMdBBCopAABEZXU5FwIIIAAAggggAACCCCAQGUECIiojCNnQQABBBBAAAEEEEAAAQRSJfDxxx/b8ccfH6tMm266adGpNWKdqEKZCIioECSnQQCBqgoQEFFVbi6GAAIIIIAAAggggAACCMQSICAiFhOZEEAAAQQQQAABBBBAAIFsCYwdO9b+/ve/xyp0ixYt7Nprr7UmTZrEyp90psYaEDF37lz75JNPEuNddNFFrV27dhU9/+zZs23WrFn+OVu2bGlt27b11xvTAhaNqbWj60pARLQLWxFAAAEEEEAAAQQQQACBWgoQEFFLfa6NAAIIIIAAAggggAACCCQkcMIJJ9iHH34Y++xDhgyxPn36xM6fZMbGGhDx8MMP21ZbbZUY7XbbbWfjxo2r6Pn/9Kc/2emnn+6fs3///nbjjTf6641pAYvG1NrRdSUgItqFrQgggAACCCCAAAIIIIBALQUIiKilPtdGAAEEEEAAAQQQQAABBBIQePPNN+3ss88u6cy9evWKPcVGSScuIzMBEWWgxTiEgIgYSAuQhYCIBcCrk0MJiKiThqQaCCCAAAIIIIAAAgggUFcCBETUVXNSGQQQQAABBBBAAAEEEEDA7IYbbrCHHnqoZIrRo0fbkksuWfJxlT6AgIhKi/56PgIiknF1ZyUgwkk03k8CIhpv21NzBBBAAAEEEEAAAQQQSK8AARHpbRtKhgACCCCAAAIIIIAAAgiUJTBgwAD7/vvvSz52//33t2233bbk4yp9QGMNiHj66adt4MCBRTmnTp2ak2ehhRayzp0752yLWunbt69dffXVUbvK3kYQwHw6LOZbNNYlAiIaa8tTbwQQQAABBBBAAAEEEEizAAERaW4dyoYAAggggAACCCCAAAIIlCjw3HPP2WWXXVbiUb9m79q1a8lTbZR1oSIHNdaAiCIs/u4zzzzTzjrrLH+9Q4cO9tFHH/nr1VwgCGC+9oQJE3JGZtlss81sm222mZ+BpboXICCi7puYCiKAAAIIIIAAAggggEAGBQiIyGCjUWQEEEAAAQQQQAABBBBAIJ/AG2+8YZ999lnk7ltvvdXmzJljW2yxhSn4ISr16tXLWrZsGbWratsIiChMTUBEYR/2IlArAQIiaiXPdRFAAAEEEEAAAQQQQACB/AIEROS3YQ8CCCCAAAIIIIAAAgggUFcCxxxzjH366ac2ZMgQ69OnT2rrRkBE4aYhIKKwD3sRqJUAARG1kue6CCCAAAIIIIAAAggggEB+AQIi8tuwBwEEEEAAAQQQQAABBBCoKwECIuqjOSsVEDFv3jybOnWqaVQRTbnRuXNnW3PNNW3FFVeMDVXtKTM+/PBDe/nll+3tt9+2jh072lprrWUrrbRS7PIWy/jSSy955545c6YtueSSts4669hqq61mTZs2LXZoKvcn6TV58mTP6ttvv7UePXpYt27dUmlQzUIREFFNba6FAAIIIIAAAggggAACCMQTICAinhO5EEAAAQQQQAABBBBAAIHMCxAQkfkm9CqwoAERTzzxhJ100kn24osv2g8//NAApU2bNqapUy644AIvIKBBhsCGUgIidK399tvP7r///sAZzLbaaisbO3ZszrbgyiuvvOKV94UXXrDPP/88uMtbVnl79+5tF110kdcx3yBDYMM555xj559/vrdlkUUWsUmTJnnTx4waNcqGDx9u06ZNC+T+dVFTyKy33np22mmn2WabbdZgv9sgL3m4tM8++9jVV1/tVu3QQw+1W265xV8vd+Gyyy6zAw88MO/hlfQaNmyYjRgxwr/Wk08+ad99950dcMABXjCNv+OXhQ022MC7ZzbeeOPg5ka1TEBEo2puKosAAggggAACCCCAAAIZESAgIiMNRTERQAABBBBAAAEEEEAAgQUVICBiQQXTcXy5ARHTp0+3oUOH2p133hmrIgsvvLCdeuqp3n/NmjWLPCZuQMSPP/5o++67r91+++0559HoC48++qgtt9xyOdu18vPPP9sll1ziXV+d8MWSAhwU7KBO6SZNmkRmP/3003OCFv7xj3+YAgweeeSRyPzhjWeccYbJPyoVsxg4cKDdcMMNUYeWtO2KK67wgivCByXhdfzxx3tt4K5188032xFHHGFz5sxxm3I+FXCie6axJgIiGmvLU28EEEAAAQQQQAABBBBIswABEWluHcqGAAIIIIAAAggggAACCFRQgICICmLW8FTlBET897//9aaX+OCDD0ouuaaNePzxx23xxRdvcGyxIAAd8NNPP9n+++9vt956a87x3bt39wIR2rdvn7NdK3PnzrXtt9/eC5ZosLPIhm233dYU6LDQQgs1yBkOiFhsscXsm2++aZCv0AYFcGy66aYNshSzSDIgIimvcEBEixYtvLZpUPlfNigI5d133/WmMona3xi2ERDRGFqZOiKAAAIIIIAAAggggEDWBAiIyFqLUV4EEEAAAQQQQAABBBBAoEwBAiLKhEvZYeUEROyyyy52zz335NSkdevW3mgK66+/vq288sr25ptv2sSJE03TR8ybNy8n77HHHpszUoDbWSwIQMEQAwYMMI0sEExrrbWWPfzww9a2bdvgZn9ZIw1oiopgUmf8cccdZ5tssol16dLF3n77bXv22Wft0ksvtS+//DKY1fKNohAOiAge1KFDB28qih49enhBEgoCGTduXINpOmQZNcVHMQtNFaLpLAolBRU0b97cNNrFgw8+aA888EBO9lVWWcWb5mOppZbK2Z6UVzggIueiv6yonG70jr59+9pjjz0WztKo1gmIaFTNTWURQAABBBBAAAEEEEAgIwIERGSkoSgmAggggAACCCCAAAIIILCgAgRELKhgOo4vNSBi5MiRduSRR+YUfu211/amzlBgQTgpyGDPPfc0TbHhkkZb+Pe//21rrrmm2+R9FgoCUDDE4MGDG0wTsd5669k///lPa9OmTc653MqMGTNs9dVXt6+//tpt8ka3+Nvf/uZt9zf+tvDOO+/Yrrvuai+//LK/S4EWb731VoNr5AuIOOyww+yCCy5oMArG1KlTbYMNNjCNsOGSLN5//31bfvnl3Sbvs5BFTsYYK2oDjULx7bff+rkVBDFp0iRTUEQwJemVLyBir732spNOOslrFwV5KHCkd+/etsUWWwSL1uiWCYhodE1OhRFAAAEEEEAAAQQQQCADAgREZKCRKCICCCCAAAIIIIAAAgggUAkBAiIqoVj7c5QaEKEpKT799FO/4Msuu6xNmzbNWrZs6W8LLygYYtVVV/Xf/tf+Pn362FNPPeVNjeDy5wsC+Pnnn+2ggw6ya6+91mX1PnUOjXyg0SnypX333Tdneo1WrVrZq6++ap06dcp3iP3vf//zyjtz5kw/z1FHHWXDhw/317UQFRDRq1cve+6556xp06Y5ed3KhAkTbMstt3Sr3qem5Nhhhx1ytuWzyMkUY0UBHhtuuGFOm2nUCI2osfHGGzc4Q5JeUQER3bp184JjNDoEKVeAgIhcD9YQQAABBBBAAAEEEEAAgTQIEBCRhlagDAgggAACCCCAAAIIIIBAFQQIiKgCchUuUUpAxJQpUxqMqqApMQ4//PCiJR06dKhdfPHFOfnee+8969ixo78tKgjghhtuMI24cNVVV/n5tKCpLjRthAIc8qUvvviiwTQal19+uQ0ZMiTfIf720aNH2xFHHOGvL7HEEjZr1ix/XQtRARHPPPOMF4CQkzG00q5dO/vss8/8rddff703FYi/4ZeFKIsbb7wxmKXoskaiUNCI2i2YdJ7+/fsHN3nLSXtFBURoKhG1JamhAAERDU3YggACCCCAAAIIIIAAAgjUWoCAiFq3ANdHAAEEEEAAAQQQQAABBKokQEBElaATvkwpARFjxozxRmpwRdI0D++++641a9bMbcr7qc72FVZYwebOnevn0VQXW221lb8eDgLYf//9vWknFJwQTBph4Z577rHFFlssuLnBsqaE0OgIwfTRRx9Zhw4dgpsil5UvPI2FRsZYZpll/PzhgIjFF1/c5syZ4+/Pt6AyqWwuKdjj4IMPdqveZ9hCAQylBER899133kgUEydOzDnvH//4Ry/YImfjbytJe4UDIpo0aeJNZbLoootGFafRbyMgotHfAgAggAACCCCAAAIIIIBACgUIiEhho1AkBBBAAAEEEEAAAQQQQCAJAQIiklCt/jlLCYg44IAD7KabbvILqWkeNN1D3NSzZ0977bXX/OyagkJTUbgUDgJw28Ofb775pq222mrhzQ3Wb7755gYjIYwfP75Bvnwbdtppp5xpPp5++mlvxAWXPxwQ0aNHD286Drc/3+fWW29tDz30kL975MiROaNRaEfYopSACE0xss8++9htt93mX0MLe++9tzd9iAIRolLSXuGACAWczJgxI6oobPtFgIAIbgMEEEAAAQQQQAABBBBAIH0CBESkr00oEQIIIIAAAggggAACCCCQiAABEYmwVv2kpQRErL322vbSSy/5ZdTUE5qCIm7acccdcwIoNNWGptxwKRwE4LaHPzfffHN7+OGHLV/HvssfDlhw28v91PQdCgpxKXz+7bff3pvGw+3P99mvX7+cfJUOiDj55JPtggsuyLm8RqV49NFHrUWLFjnbgyvh+gT3lbMc9goHRPTt29cee+yxck7dKI4hIKJRNDOVRAABBBBAAAEEEEAAgYwJEBCRsQajuAgggAACCCCAAAIIIIBAuQIERJQrl67jSgmI0AgIr7/+ul+Bc88914YNG+avF1s48sgjTZ3/Lm2zzTb24IMPutUGoyL4OyIWNI3GYYcdFrFn/iaNiHD77bfP37CAS+HpJsIBBIMGDbJrr7226FWSDIjQ9BuHHnpoThlWXnlle/bZZ3Om+8jJ8NtK0l7hgIjBgwebpmEhRQsQEBHtwlYEEEAAAQQQQAABBBBAoJYCBETUUp9rI4AAAggggAACCCCAAAJVFCAgoorYCV6qlICI3r172/PPP++XRiM6KEggblIAw5VXXuln15QU99xzj7+eb4QIjQihkSm++OILP2+rVq286Sk6derkbwsvhAMPwvtLXR86dKj95S9/8Q8LB0TE7eAPl6tSI0Q88MADplE4fvzxR7+MSy65pD3zzDO2xhpr+NvyLYTLlS9f3O1hr3BAhKZL0bQppGgBAiKiXdiKAAIIIIAAAggggAACCNRSgICIWupzbQQQQAABBBBAAAEEEECgigIERFQRO8FLlRIQscsuu+QEMGjEhxEjRsQu3XbbbZczIsSBBx5o11xzjX98VEDEvvvuazfeeKN33d13393Pq4VNN93UHnnkkbxTZ6hDOVi+jTbaKGc952QxVtq3b2/LLbecnzNNARH/+c9/bOONN7avvvrKL9/CCy/seW+xxRb+tkILSXsREFFIv+E+AiIamrAFAQQQQAABBBBAAAEEEKi1AAERtW4Bro8AAggggAACCCCAAAIIVEmAgIgqQSd8mVICIk488cScERI0GsG9994bu4Tdu3e3yZMn+/k13Yam3XApHBChgIcJEyZY06ZNvSy77babjR071mX3PqNGV3AZRo0aZUOGDHGr1qVLF5s2bZq/vqALaQmImD59um2wwQb28ccf51RJwSYKOombkvYiICJuS/yaj4CI0rzIjQACCCCAAAIIIIAAAghUQ4CAiGoocw0EEEAAAQQQQAABBBBAIAUCBESkoBEqUIRSAiLUwX7wwQf7V11ppZXsnXfesYUWWsjflm/hv//9ry2//PI2d+5cP8vll1+eE7AQDojo37+/NzqEO0Ad/t26dbNZs2a5TdayZUt75ZVXrHPnzv42t/Dwww/bVltt5Va9kSRUDk0jESeNGzfOy6ZpOTp27GiapiOY0hAQMXv2bNPIF6+99lqwaHbCCSfYhRdemLOt2ErSXgREFGuB3P0EROR6sIYAAggggAACCCCAAAIIpEGAgIg0tAJlQAABBBBAAAEEEEAAAQSqIEBARBWQq3CJUgIiHn/8cW+aimCxxowZY4MHDw5uilw++eST7YILLsjZ99Zbb9kqq6zibysWEKGM1113XYPr9e3b1x599NEGU2e8//77pmCGYDrllFPsvPPOC26KXH7iiSdM5w0mTd2hIA2Xah0QMW/ePNt22229aUNcmfSpqU3uuuuuBh7BPFHLSXsREBGlnn8bARH5bdiDAAIIIIAAAggggAACCNRKgICIWslzXQQQQAABBBBAAAEEEECgygIERFQZPKHLlRIQ8e2339oaa6xh6jh3acUVV7SpU6faIoss4jY1+Jw5c6Y3XcU333zj7/vd735nL730kr+uhTgBEcq3xRZbNAgCCI82oXw///yzN5XEc889p1UvNW/e3F5++WVbffXV3aYGnz/++KMX+DFx4kR/n0aimDFjRs7oErUOiBgwYEDOCBoq7HrrrWcKXFlsscX8ssddSNqLgIi4LfFrPgIiSvMiNwIIIIAAAggggAACCCBQDQECIqqhzDUQQAABBBBAAAEEEEAAgRQIEBCRgkaoQBFKCYjQ5caOHWu77bZbzpU1ZcNtt93mTYmRs+OXlcmTJ9vuu+9ub7zxRs6u4cOH21FHHZWzLW5AhKbp6NmzpwUDLBSwoECHLl265Jzz+eeft/XXX98LjnA7Vl11VfvrX/9q6667rtvkfyooYNCgQXbDDTf427Rw2GGH2ejRo3O21TIgItxuKphG23j66adtmWWWySlnKStJehEQUUpLmKUpIOK7776zzz//PLICL774ovf913134oknRuZRwFTbtm0j97ERAQQQQAABBBBAAAEEEMiSAAERWWotyooAAggggAACCCCAAAIILIAAARELgJeiQ8Md6x06dLCPPvqoYAm33HJLmzBhQk4edYZqWgwFHygoQQEQTz31lDdNxtdff52Td9NNN/WOb9q0ac72uAEROuiSSy4xdbAH08Ybb+yNjtCkSZPgZjvooINMU3sE08ILL2zHHnusN9rEmmuuaXPnzjWNJPHnP/+5wcgVrVu3NnX6du3aNXgKq1VAxLhx42yHHXbIKYtWBg4caO3bt/cCRRQs8tNPPzXIE7Vh1KhR1qJFC39XUl4ERPjEsRbSFBChAuv78sknn8QqezjTwQcf3GAKmnAe1hFAAAEEEEAAAQQQQACBLAgQEJGFVqKMCCCAAAIIIIAAAggggEAFBAiIqABiCk5RTkDEm2++6XVultM52qlTJy9QYvnll29Q+1ICIjStxYYbbmga0SCYokae0Jvtffr08ab2COaNs6xAgfHjx9smm2zSIHutAiJGjBjhjR7QoEBlbpg9e7Yp6MOlpLwIiHDC8T7TFhBx++2327333huv8IFcCy20kBeQVGhanUB2FhFAAAEEEEAAAQQQQACBVAsQEJHq5qFwCCCAAAIIIIAAAggggEDlBAiIqJxlLc9UTkCEyqtO8yOOOMLuuOOO2MXfcccdvako2rRpE3lMKQEROsGrr75qvXr1snnz5vnnW2yxxbypM8KjOWjEhGHDhpmCCTQtRpy01FJL2Y033hg5GoOOr9eACNUtCS8CIiQbP6UtIGL69OneKDDxa/BrTk2pc/jhh5d6GPkRQAABBBBAAAEEEEAAgVQKEBCRymahUAgggAACCCCAAAIIIIBA5QUIiKi8aS3OGJ56Is6UGcFyKiBCgQwaNeKHH34I7vKWNS3GVlttZYcccojtvPPODfYHN4QDIo488kgvgCGYJ7wcDkrQ/v33399uuummcFZvfeLEiXbSSSd5QRPq9I9KCoQ47rjj7KijjrLFF188Kou37eKLL7ahQ4f6+3WMthVL/fr1s/vvv9/PdvPNN9t+++3nr2uhkIWCNAYMGJCTv9wVjYDx5Zdf5kyZETxXJb3OPvtsO+OMM/zTy1cjepCiBdIWEKFSqv2mTp0aXeA8W0844QRbe+218+xlMwIIIIAAAggggAACCCCQLQECIrLVXpQWAQQQQAABBBBAAAEEEChbgICIsunq8sDvv//eC4p45ZVX7IMPPrD27dtbx44drVu3bqYgi7Sln376yd5991177bXXbMqUKdaqVSvr3Lmz95+m9WjevHnailzT8uBVff40BkQ8+OCDpgCeuEnBRSNHjoybnXwIIIAAAggggAACCCCAQOoFCIhIfRNRQAQQQAABBBBAAAEEEECgMgIERFTGkbMggAACUQJpDIiYNWtWSdNfbLfddg1GP4mqK9sQQAABBBBAAAEEEEAAgawIEBCRlZainAgggAACCCCAAAIIIIDAAgoQELGAgByOAAIIFBBIY0CEiqtpYV588cUCJZ+/S1O/dOnSZf4GlhBAAAEEEEAAAQQQQACBjAsQEJHxBqT4CCCAAAIIIIAAAggggEBcAQIi4kqRDwEEEChdIK0BEc8880ysaTA09cx5551XesU5AgEEEEAAAQQQQAABBBBIsQABESluHIqGAAIIIIAAAggggAACCFRSgICISmpyLgQQQCBXIK0BET///LMNHjzY5s6dm1vg0Nof/vAH69evX2grqwgggAACCCCAAAIIIIBAtgUIiMh2+1F6BBBAAAEEEEAAAQQQQCC2AAERsanIiAACCJQskNaACFXkmmuusccee6xgnYYPH27LLLNMwTzsRAABBBBAAAEEEEAAAQSyJkBARNZajPIigAACCCCAAAIIIIAAAmUKEBBRJhyHIYAAAjEE0hwQ8dprrxWcDmPNNde0k08+OUYtyYIAAggggAACCCCAAAIIZEuAgIhstRelRQABBBBAAAEEEEAAAQTKFiAgomw6DkQAAQSKCqQ5IEKFP/bYY+2TTz6JrMchhxxim2yySeQ+NiKAAAIIIIAAAggggAACWRYgICLLrUfZEUAAAQQQQAABBBBAAIESBAiIKAGLrAgggECJAmkPiLjjjjvsnnvuaVCrhRde2MaMGWPNmzdvsI8NCCCAAAIIIIAAAggggEDWBQiIyHoLUn4EEEAAAQQQQAABBBBAIKYAARExociGAAIIlCGQ9oCI6dOnR06LsdFGG9nhhx9eRo05BAEEEEAAAQQQQAABBBBIvwABEelvI0qIAAIIIIAAAggggAACCFREgICIijByEgQQQCBSIO0BESr0GWecYVOnTs0p/wknnGBrr712zjZWEEAAAQQQQAABBBBAAIF6ESAgol5aknoggAACCCCAAAIIIIAAAkUECIgoAsRuBBBAYAEEshAQMX78eLvpppv8Wi611FI2cuRIf50FBBBAAAEEEEAAAQQQQKDeBAiIqLcWpT4IIIAAAggggAACCCCAQB4BAiLywLAZAQQQqIBAFgIiZs+ebYcddphf2+2339723Xdff50FBBBAAAEEEEAAAQQQQKDeBAiIqLcWpT4IIIAAAggggAACCCCAQB6BESNG2Jdffmm77rqr9ezZM0+u2m9+6qmnbPTo0da+fXu79NJLa18gSoAAAgjEEMhCQISqceKJJ9qMGTO8Gp1zzjnWuXPnGLUjCwIIIIAAAggggAACCCCQTQECIrLZbpQaAQQQQAABBBBAAAEEEKhbAQIi6rZpqRgCdS2QlYCIf/3rX3b55ZfbyiuvbOeee25dtwmVQwABBBBAAAEEEEAAAQQIiOAeQAABBBBAAAEEEEAAAQQQSJUAARGpag4KgwACMQWyEhCh6gwePNh23nln69evX8zakQ0BBBBAAAEEEEAAAQQQyKYAARHZbDdKjQACCCCAAAIIIIAAAgjUrQABEXXbtFQMgboWyFJAxJgxY7yAiLZt29Z1m1A5BBBAAAEEEEAAAQQQQICACO4BBBBAAAEEEEAAAQQQQACBVAkQEJGq5qAwCCAQUyBLARHvv/++dezYMWbNyIYAAggggAACCCCAAAIIZFeAgIjsth0lRwABBBBAAAEEEEAAAQTqUoCAiLpsViqFQN0LZCkgou4bgwoigAACCCCAAAIIIIAAAr8JEBDBrYAAAggggAACCCCAAAIIIJAqAQIiUtUcFAYBBGIKEBARE4psCCCAAAIIIIAAAggggEAVBQiIqCI2l0IAAQQQQAABBBBAAAEEECguQEBEcSNyIIBA+gQIiEhfm1AiBBBAAAEEEEAAAQQQQICACO4BBBBAAAEEEEAAAQQQQACBVAkQEJGq5qAwCCAQU4CAiJhQZEMAAQQQQAABBBBAAAEEqihAQEQVsbkUAggggAACCCCAAAIIIIBAcQEXEKGcQ4YMKX4AORBAAIEUCIwcOdIrhZ5bffr0SUGJKAICCCCAAAIIIIAAAggggAABEdwDCCCAAAIIIIAAAggggAACqRJ49tlnbfjw4akqE4VBAAEE4goce+yxtt5668XNTj4EEEAAAQQQQAABBBBAAIEEBQiISBCXUyOAAAIIIIAAAggggAACCJQu8NFHH3kBEd99913pB3MEAgggUEOBFi1a2DHHHGPLLrtsDUvBpRFAAAEEEEAAAQQQQAABBJwAARFOgk8EEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQqBsBAiLqpimpCAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAgg4AQIinASfCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIFA3AgRE1E1TUhEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQcAIERDgJPhFAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEECgbgQIiKibpqQiCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIOAECIhwEnwigAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAQN0IEBBRN01JRRBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEDACRAQ4ST4RAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAIG6ESAgom6akooggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAgBMgIMJJ8IkAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACdSNAQETdNCUVQQABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEnQECEk+ATAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBOpGgICIumlKKoIAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACToCACCfBJwIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAgjUjQABEXXTlFQEAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBJwAARFOgk8EEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQqBsBAiLqpimpCAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAgg4AQIinASfCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIFA3AgRE1E1TUhEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQcAIERDgJPhFAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEECgbgQIiKibpqQiCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIOAECIhwEnwigAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAQN0IEBBRN01JRRBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEDACRAQ4ST4RAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAIG6ESAgom6akooggAACCCCQTYEffvjB5syZk83CU2oEEEAAAQQQQAABBBBAAAEEEECggECTJk2sdevWttBCCxXIxS4EEEAAAQQQSEqAgIikZDkvAggggAACCBQVmDdvnh199NE2a9asonnJgAACCCCAAAIIIIAAAggggAACCGRNoGnTprbkkkva8OHDCYrIWuNRXgQQQACBuhAgIKIumpFKIIAAAgggkE2BL774wo488shsFp5SI4AAAggggAACCCCAAAIIIIAAAjEENErEqFGjvMCIGNnJggACCCCAAAIVFCAgooKYnAoBBBBAAAEEShMIBkQMHTqUfxgojY/cCCCAAAIIIIAAAggggAACCCCQYoH//ve/dskll3glHD16NP/ukeK2omgIIIAAAvUrQEBE/bYtNUMAAQQQQCD1AsGAiHPPPZd/GEh9i1FABBBAAAEEEEAAAQQQQAABBBCIK6CAiNNOO83LTkBEXDXyIYAAAgggUFkBAiIq68nZEEAAAQQQQKAEAQIiSsAiKwIIIIAAAggggAACCCCAAAIIZEqAgIhMNReFRQABBBCoUwECIuq0YakWAggggAACWRAgICILrUQZEUAAAQQQQAABBBBAAAEEEECgHAECIspR4xgEEEAAAQQqK0BARGU9ORsCCCCAAAIIlCBAQEQJWGRFAAEEEEAAAQQQQAABBBBAAIFMCRAQkanmorAIIIAAAnUqQEBEnTYs1UIAAQQQQCALAgREZKGVKCMCCCCAAAIIIIAAAggggAACCJQjQEBEOWocgwACCCCAQGUFCIiorCdnQwABBBBAAIESBAiIKAGLrAgggAACCCCAAAIIIIAAAgggkCkBAiIy1VwUFgEEEECgTgUIiKjThqVaCCCAAAIIZEGAgIgstBJlRAABBBBAAAEEEEAAAQQQQACBcgQIiChHjWMQQAABBBCorAABEZX15GwIIIAAAgggUIIAARElYJEVAQQQQAABBBBAAAEEEEAAAQQyJUBARKaai8IigAACCNSpAAERddqwVAsBBBBAAIEsCBAQkYVWoowIIIAAAggggAACCCCAAAIIIFCOAAER5ahxDAIIIIAAApUVICCisp6cDQEEEEAAAQRKECAgogQssiKAAAIIIIAAAggggAACCCCAQKYECIjIVHNRWAQQQACBOhUgIKJOG5ZqIYAAAgggkAUBAiKy0EqUEQEEEEAAAQQQQAABBBBAAAEEyhEgIKIcNY5BAAEEEECgsgIERFTWk7MhgAACCCCAQAkCBESUgEVWBBBAAAEEEEAAAQQQQAABBBDIlAABEZlqLgqLAAIIIFCnAgRE1GnDUi0EEEAAAQSyIEBARBZaiTIigAACCCCAAAIIIIAAAggggEA5AgRElKPGMQgggAACCFRWgICIynpyNgQQQAABBBAoQYCAiBKwyIoAAggggAACCCCAAAIIIIAAApkSICAiU81FYRFAAAEE6lSAgIg6bViqhQACCCCAQBYECIjIQitRRgQQQAABBBBAAAEEEEAAAQQQKEeAgIhy1DgGAQQQQACBygoQEFFZT86GAAIIIIAAAiUIEBBRAhZZEUAAAQQQQAABBBBAAAEEEEAgUwIERGSquSgsAggggECdChAQUacNS7UQQAABBBDIggABEVloJcqIAAIIIIAAAggggAACCCCAAALlCBAQUY4axyCAAAIIIFBZAQIiKuvJ2RBAAAEEEECgBAECIkrAIisCCCCAAAIIIIAAAggggAACCGRKgICITDUXhUUAAQQQqFMBAiLqtGGpFgIIIIAAAlkQICAiC61EGRFAAAEEEEAAAQQQQAABBBBAoBwBAiLKUeMYBBBAAAEEKitAQERlPTkbAggggAACCJQgQEBECVhkRQABBBBAAAEEEEAAAQQQQACBTAkQEJGp5qKwCCCAAAJ1KkBARJ02LNVCAAEEEEAgCwIERGShlSgjAggggAACCCCAAAIIIIAAAgiUI0BARDlqHIMAAggggEBlBQiIqKwnZ0MAAQQQQACBEgQIiCgBi6wIIIAAAggggAACCCCAAAIIIJApAQIiMtVcFBYBBBBAoE4FCIio04alWggggAACCGRBgICILLQSZUQAAQQQQAABBBBAAAEEEEAAgXIECIgoR41jEEAAAQQQqKwAARGV9eRsCCCAAAIIIFCCAAERJWCRFQEEEChR4JJLLrFp06Z5Rw0ePNh69eqVc4YPPvggZ33FFVfMWS9l5csvv7SvvvrKP6R58+bWvn17f70aC3PmzLHZs2f7l1psscVs6aWX9tcb08LMmTNt3rx5fpWXWWYZa9Gihb/OQvUFaJPqmPMcqI4zV0FgQQVGjhxpkydP9k6z2Wab2e67776gp+R4BFIrQEBEapuGgiGAAAIINCIBAiIaUWNTVQQQQAABBNImQEBE2lqE8iCQHoGnnnrKevfubepYr7dUjbo9/PDDttdee3l0TZs2tSeffNK6deuWQ6lO8h9//NHfdtVVV9kee+zhr5eycPzxx9v111/vH9KzZ0974okn/PVqLPzlL3+xP//5z/6l9t57bxs9erS/3pgWNtpoI7+jSfX++9//bupwItVOgDapjn3angPVeN5XR5arIFBZgdtuu80OP/xw76SLLrqoPf3009apU6fKXoSzIZASAQIiUtIQFAMBBBBAoFELEBDRqJufyiOAAAIIIFBbAQIiauvP1RFIo4BGLRg2bJiNGzfO7r33Xvv973+fxmKWVaZq1U0jNWy44Yb24YcfeuXs37+/XXbZZQ3KHA6IuPDCC+3AAw9skC/OBgIi4ihVLw+d79Wzjnsl2iSu1ILlS0tARLWe9wumxdEI1E7g559/ts0339xeeuklrxCbbLKJ3X333bUrEFdGIEEBAiISxOXUCCCAAAIIxBQgICImFNkQQAABBBBAoPICBERU3pQzIpBlAb3Nf+6559q3337rVaOeAiKqWbehQ4fadddd5xm2atXKXnjhBWvXrl2DW4OAiAYkdbOBzvf0NSVtUp02SUNARDWf99VR5SoIJCPwzDPP2A477OCffMSIEbbffvv56ywgUC8CBETUS0tSDwQQQACBLAsQEJHl1qPsCCCAAAIIZFyAgIiMNyDFR6DCAuuvv75NnTrVP2s9BURUq27qXOjXr5/pzUsljbahAImoREBElEp9bKPzPX3tSJtUp03SEBBRred9dUS5CgLJCigA4oEHHvAu0rp1a5s0aZItu+yyyV6UsyNQZQECIqoMzuUQQAABBBCIECAgIgKFTQgggAACCCBQHQECIqrjzFUQyIpAPXciVaNuP/74ozdVxrRp07wmb9Omjb388sumUSKiUr0FRDzxxBP26KOP+lXdeOONveG4/Q2NaIHO9/Q1Nm1SnTZJw3OgGs/76mhyFQSSF3j11Vetb9++fiDnzjvv7I9ylfzVuQIC1REgIKI6zlwFAQQQQACBQgIERBTSYR8CCCCAAAIIJCpAQESivJwcgcwJ1HMnUjXqphE1Bg4c6Lf7H//4RzvuuOP89fBCvQVEhOvXmNfpfE9f69Mm6WuTpEpUjed9UmXnvAjUQmD//fe3cePGeZdu2rSpPfvss9alS5daFIVrIpCIAAERibByUgQQQAABBEoSICCiJC4yI4AAAggggEAlBQiIqKQm50Ig+wL13IlUjbptscUW9u9//9u7EdShMHnyZGvXrl3eG4OAiLw0md9B53v6mpA2SV+bJFWiajzvkyo750WgFgKPPPKI7bHHHv6lBw0aZBdddJG/zgICWRcgICLrLUj5EUAAAQTqQYCAiHpoReqAAAIIIIBARgUIiMhow1HssgWmTJli7777rs2dO9fWWGMNW2211Qqe6+eff7YPP/zQpk6dau+88443nHDLli2tc+fO9rvf/c5atGhR8Pis7axEJ9LHH39sr732mue84oorWs+ePW2FFVaoOUUl6laoEv/6179s++2397P83//9n/3jH//w16MW0hgQoaGz33vvPfvkk09siSWW8O7zrl27mgI8kkyfffaZvf322/bBBx+Y/tF6ueWW875nK6+8si222GJlXXrevHne9/att94y3ZedOnWy7t272/LLL1/W+dxBen64Z4mcll56ae/cvXr1siZNmnjZFrTzPQ3fI9rEtfivn2loE1ei//3vf/b888979/U333zjPWP1XdF3deGFF3bZSv5M6jtTakEW9DmU9PPe1ee7776z6dOne793+mzdurWtuuqq3t8Wiy66qMtW9mcS95yeX2+++aa9//77NnPmTFtqqaWsY8eOpufXQgstVHZZq3VgEibBsi/ovRc8l1tO4lmqc1fSQt99/U08a9Ysr9j6+1YW+n0jIVAPAgRE1EMrUgcEEEAAgawLEBCR9Rak/AgggAACCGRYgICIDDceRc8r8Kc//cmuuuoqf7+GAP7+++/t8MMP9zpc/R2/LKy77rp25plnWp8+ffzNs2fP9t6KmzhxohcI8e233/r7ggvNmjWzddZZx4YMGZLTEa48+m4pYCKY7rjjjpzrBPdpWdf7wx/+kLNZZQ+fJ5hBHe6HHXaYv6lfv352xRVX+OtxFv7yl7/Y8OHDvayqq4JAgsl1Rqscbjjl4H4tv/76657jSy+95NU9vH/JJZf0rNQ2CkSJSlH11z/EP/7449amTZuoQ7xt6tTZaqut7Ouvv/bzqONb59PbjQtaN/+kRRb23Xdfe/DBB/1cch08eLC/HrVQi4AImVx22WVecRZZZBF76KGHvMCDMWPGeN8bBf6Ek+4B3etDhw61jTfeOLzbX5e16u3S7rvv7l/LbQt+6l7TPXXjjTfaY489Zj/99FNwt7esQIxdd93VTj75ZK+cDTJEbHj66ae9+/Hll1+2H374oUEO3Y+6n/XdL/T9Ch/46aefekbXXXed6TkRTgoAOv3002233XazcgIiKvE9Cpep1HXaJFeslm0S/K6qVDfffLP3m3XaaafZnXfeaVG/TQo+O+KII6x///5WSod8Jb8zcZ4DwbpV6jlUid+y3NbPv6bfOv3W3nfffaagiHBSYJSeBwoyOPbYY61Hjx7hLHnXk7rn9Py65ppr7Prrr/eCzsIFWHbZZb17R39TXH311XbOOef4WW644QbTCEjBNGzYMLvpppv8TfpbSM/pYmnLLbe0N954w88WdW5/528LlTRJ4t4Ll1frST1LK2kRLreeHX/729/8zWrPE0880V9nAYEsCxAQkeXWo+wIIIAAAvUiQEBEvbQk9UAAAQQQQCCDAgREZLDRKHJRgT/+8Y82evRoP9+VV15pJ5xwgumN2qh06qmn2vHHH+/tUsesOgH03Sgl7bXXXjZy5Mictyt///vfe4EC7jzqFFFHVr501lln+Z33Lo+2HXnkkW61wWf4H69VD9WnlHTeeefFGha5bdu2pjftg0n/4C9rmUV1CgXzalkdX+owPvTQQ/036YN5Bg4caPfee29wk+2999457Rnc+eOPP9p2223nvSkd3K4233PPPW1B6hY8X7Fljaigji95KMWZLkP5ahEQ8ec//zknaEGdH/J64oknVKSi6aSTTjL9F5XUIanzu1So7ebMmWOHHHKI/fOf/3TZC37qrfdTTjnF61zMl3HGjBnedyx8D+XLr3PqO3PccceZApwKpRdffNH0Pdc/qBdLKqfKoClTXPr73/9um222mVvN+az09yjn5CWs0CbzsdLQJuHvqjpy9Tuj502xtPrqq9vYsWNNndyFUhLfmTjPgXDdKvEcqtbzXkFcBxxwQGQAV5S1giP0zCzWsZzkPaepnBRwqZEKiqW+ffvahhtumPMsV/tsvfXWOYfq2angCpeOOeYY7/fdref7DAeLRZ3bHZuESRL3niuv+0ziWZqEhSuv+1SApH63XdLfffodW5BRZ9y5+ESg1gIERNS6Bbg+AggggAACZgREcBcggAACCCCAQM0ECIioGT0XTlAgHBChTvh8nfXqqNCbnnqTM9yJU2oR9eal3gx3Kdw5s9Zaa9mjjz7qdjf4VCfEK6+8krN98803994Eztn424replen1+eff+7v1hzQa6+9tr8eZyFcznzHhAMiNOy2/uH8ySefzHdI3u1601SdIOHhuTVUs6aa0DDQwaTRNcJvp2r/BRdc4P0XzKtOn1GjRnmbyq1b8HxxljW6QrCzK850GTpvGgIi9BZ51Jvmheqtzn4F/IRT+DuULyBCbyprehFNkVFqUgBOsMPGHf/ll196o1doiptSk0aJ0GgrrVq1ijzUjV6iqQnKTfkCIpL4HpVTRtpkvlpa2iTccTu/hPGWVlppJbv//vvzTlmU1HcmznMgXLdKPIeq8bx/4YUXbMcdd/Sm3YrXCvNzaWQejdwRlZK855555hnbY489Sn7OB8sZFbSQdEBEUiZJ3HtBqySepUlZBMutZY2mpilfFNDhkoIkNJoaCYGsCxAQkfUWpPwIIIAAAvUgQEBEPbQidUAAAQQQQCCjAgREZLThKHZBgXBARDhzMEBCbypqyGt1zKoTW/8Y7JLyach/BTksv/zy3ugG6mzVnMojRoywjz76yGX1Ptdcc01vege3UW9kBjvxFXwxZcoUU2BBOCmoQXM36w3AYNJUBe+++27k2+vqmNFUES4tt9xy9tprr7nV2J8aftm9ya439oNJb86rXEqaT1pTcrikt5XVARVMMtOoFbLs9Mu0Fe+9956pnBpa3M1L7fJffPHFphEhwkkjFWiKhKCF/NWps/jii/vZn3vuOa9TXaNEuNS1a1evDdw0H+XWzZ0v7qfeFlaHukvnn3++HXzwwW4172caAiKChWvfvr3XYadpTdT5r2H01RkSHjFlhx12yBkq3Z0jTkeo8moqkbvvvtsd5n2qzXTvrLfeeqZ21JvMwoS8xwAAMZFJREFUt956qzdNQHAqjebNm3vTa4SnXtl///0bTOmi+0WjkWj0jo4dO3ojnEyaNMkUwKL50oNJU+oEh4h3+9QRpbelNTVLMOn7PmDAAFOgk0afefbZZ737XJ3MUSlfQERS36OoMhTaRpvM10lLm4Q7bueX0Lz7TiMf6d7UtDC6rzWiUDjISCPo3HLLLcFD/eWkvjNxngOF6lbuc6gaz/tNN93UNBWPS0sssYQdeOCBtu2223qjcej3SL/ZGjlBf1sEf8dULwVg6ncynJK65xQMqr9zwveFpj466qijvI5u/TbrWa8prWbOnBkumrdei4CIpEySuPeCaEk8S5OyCJbbLevvQE3J45L+ptbfgiQEsi5AQETWW5DyI4AAAgjUgwABEfXQitQBAQQQQACBjAoQEJHRhqPYBQXyBUTssssudvTRR1vPnj29qSzU0auO0k022cT7x17NY+2SRi64/fbb8w5xr07SQYMG2fjx490h3qfeJO/evbu3rI4QLQc7GMKjSLiD77rrLjvooIPcas7ngw8+aOuvv37ONq2ce+65pqACl9Qpc+GFF7rVsj51nalTp/rH5hsJQMEgvXv39jrNXWa5qqN5lVVWcZv8TwVG6M3YYMDG0ksv7U11seSSS/r53EJUG6rz+ZJLLvGyqANaIxRMnz7dHeJ1MqlNVY6oFLduUccW2qbOetU52BH+8MMPe/dWoeO0L00BEbqfzzzzzAajJKgjTYE3wfrp+6HRTDp06JBTxTgdoRMmTPCmMwkeqJFO9P3Tm6nhpPt/v/32y+lY1Bz3uv9d0vcqPI2HAhbUKbnyyiu7bP6npr9QMI6mC3BJdXr88cf976/bru9Y8FrarkApBUUpSCiY1BG677772ptvvhnc7C1HBUQk/T1qUIg8G2iT+TBpaROVKF/HrQKSrrrqKtOoCsGkjm3d1+HpbxR8pN+5YEryOxPnOZCvbpV4DqmeSTzv9dZ8586d/akyFNigKX/0rIlKmt5EU0QF0z333OONZBPcluQ9F9WRrg57+YenQdDITPvss09OwIcrZ7UDIpI0SfLeS+JZmqSFa9/gZ/jZoOAZ3bckBLIuQEBE1luQ8iOAAAII1IMAARH10IrUAQEEEEAAgYwKEBCR0Yaj2AUFojrTNcqBOjuj3sxU4EK3bt3sk08+8c8b7nD1dwQW9I/UPXr0CGwxu+2223JGbdBbdcFAi+B0DsEDjzzySPvrX/8a3OQvn3zyyTnTMbgd+kfqYIBBVKeXyxv3M24nkkY/UAevSy1btvTeMNUQ7fnSV1995b2NquGkXdKbiOqcCCe91arRNfTGr0saYUPDv+uNaLWPAlaCSdNn5AsqUb64dQueM86y3hbWW8MuqZwK1JBJsZSWgAiNcqCOnKZNm0YWWR2sCigKpqgOsjgdoQoYUJCDS/pO6q1pvT2dL6lTUZ2LLrVr1867N9yUK/p+a0QJl7RfI7S4kULc9uCngiE0GkVwOh0F+ahsakMlPRs0EkXwni029Y1+VzUFR3h6jaiAiKS/R8H6FlqmTebrpKVNVKKojlsFDz311FN5v6vqtNf9N3v2bL9SW2+9tTdFkb/hl4WkvjO6RpznQFTdKvUcUhmSeN4r0G2vvfbS6b0k58cee8ytRn5usMEG3sg0bmfU73mS95yCNYKBX1rX30L5kgLfVC/9XgdT1PM+ySkzkjRJ8t5L4lmapEWwjd2yni+aFsYl/Ua/90tQa9Tfzy4PnwhkQYCAiCy0EmVEAAEEEKh3AQIi6r2FqR8CCCCAAAIpFiAgIsWNQ9HKFogKiNB0BprGISqpA0Bvpertbk2JoSGvNT2DOp6KpXCnkt5I32mnnfzDwh0o6vR94403/P1uQYEVCrBQatasWc5w/ip3cDoG5QkHY2jY7rfeeityag3lj5vidCLpHxQ1pUEwFQtGcHmvvfZa0zDvLrVu3dr7h3a3HvzUNB6bb755Toe1vIcOHdog8KHQsPDunHHq5vKW8qlAFgW0uKSgEHXwx0lpCYjQW84KDiiUNHqDpnZxSQEKeps4mIp1hGpklS5duuTMZR8n+EjTVayzzjr+KBGaCkPfLZVp2rRp3mgl4XLoLehi6bTTTrNRo0blZFOAy4orruhtCwe7aOONN96YM3VMzsG/rUSdNxwQUa3vUVT5gttok818jrS0iStQVMdt+DfG5Q1+6nms/1zSSCbvvPOOP6JJkt8ZXbPYc0B5oupWqeeQzp/E817BD5pCyyX9VmtUqKiRbVwe/Xbrt1kj1XTq1MkbTSg49VOS95xG8enbt68rivepAM1gZ3fOzt9WNHXGpZdemrOrmgERSZqoUknde0k8S5O2yGnk31airhkc/SzqGLYhkAUB3dv6+0xp9OjRFjVCXRbqQRkRQAABBBDIsgABEVluPcqOAAIIIIBAxgUIiMh4A1L8SIFwQITe9lagQ3h4+6iD582b540UscIKK0Ttztmmt8c1bYM67l3SMOZ77LGHWzX9A7mCB4Jvi+vtO41I4ZI6S/QWqUuHH364N1qEe8NXb+UpWCNYfnVqBOd01jV17QVNcTqRXnjhhZxRMHRNBXkUesPflUvThwTrru2qf9u2bV2WnM8rrrjCTj311Jxt4ZXll1/ennzySWvTpk14V856nLrlHBBzRVMnaKoJlxTEEZx/222P+kxDQESrVq1yph6JKqe2adoMtb1L6jA74IAD3Kr3WawjNGo4cU0voREdiiV1LOofr9X5GLzXbr75Zm8qHHe8pvFQQIo6K4sl/eO4prUJjhKh6WvciB/h6TI6duzojTzhRpDId36NNqM3sfU8cSkcEFHN75ErQ9QnbTI/ICItbeLaKdxxq3v/X//6lz+CicsX/tRvhwKPNJ2PS3om6dmklOR3Rucv9hxQnnDdKvkc0vmTeN7r91yBDcHnhaYtUZCfptGJ83eDyhZMSd5zekYruMGl5ZZbzpvqKN9IQC6f/r+Bpp5SfV2qZkBEkiaqT1L3XhLP0qQtXPuGPzUNmO4DlyoxApk7F58I1EqAgIhayXNdBBBAAAEE5gsQEDHfgiUEEEAAAQQQqLIAARFVBudyVREIB0SogzQ49UI5hVDHkoadfvvtt23KlClep9TTTz9t+se1YLryyittzz33DG7yOo2DIzycffbZNmTIED+Pjhk2bJi/fscdd9jVV1/tTWHgNob/MVpTb+htWpfivPXp8hb6jNOJpKkq9FZ/MAWnzwhuj1rWkNLBDqXx48c3eMPfHaegk1133dU0ZUNU0pQJsg0GlETl07Y4dct3bKHt4ekc1DGm9ouT0hAQoSkhdC8XS3ozOjg8/IUXXmgHHnhgzmHFOkLDI4QoEEYBMQuSFECkqWpcipoewO2L+tQILMFRW9RZpqlclPQ9vfXWW/3DSjl3r169vEAmd3A4IKKa3yNXhqhP2mR+QERa2sS1U7jjVs/Oyy+/3O0u+KlAn48//tjPE/y+Jvmd0QWLPQeUJ1y3Sj6HdP6knvf6Pco35YQCVjTV02abbWZ9+vTJCWJUmaJSkvdceMquTTbZxPS3RJwUnpKrmgERSZqo7knde0k8S5O2yHcvaPoojSTj0pgxY7y/xdw6nwhkUYCAiCy2GmVGAAEEEKg3AQIi6q1FqQ8CCCCAAAIZEiAgIkONRVFjC4QDIjbaaCO77777Yh+v4Ad1/OrNPAVS6B+FNUJDsBM/38miAiLUkXDEEUf4h2gI67Fjx/rre++9tz300EPeut5q17U02kPwzc5jjz3WH+ZVb2127tzZf3tTI0iojC1btvTPWe5CnE6kcGdCuddyx2nKAgV45Evq1FMbamqTcDrllFNypuAI7w+ux6lbMH/cZbWt2tilgQMHmkYWiJOWXXZZ+/777/2savPgveLviLGgaTs0fYdL+ea3D7efRn4IBhS448Of4SCcYAery1usIzQ8lH++MrrzxflUJ9+rr77qZz3ooINypgvwd+RZ0LQfCspxSVNtqB5KCm7SW78uldK2GpZeo8G4FA6ICLeDy1fuZ7HvUb7z0ibzAyLS0iaurcLlOemkk0z/xUnbbLONPffcc35WTTXkAu+S/M7ogsWeA8oTrlsln0M6f1LPe40+s9dee9lnn32my+RNGtFJQQX9+vWz7bffPu/Q7GGHvCeMuSP4HNh///1t3Lhx/pF6hoenCPJ3hhbCz8VqBkQkaaJqhs9fqXsviWdpuKyhZip5NXh/FDpYo8n85z//8bOcf/75dvDBB/vrLCCQRQECIrLYapQZAQQQQKDeBAiIqLcWpT4IIIAAAghkSICAiAw1FkWNLRAOiNhvv/1M0xoUSwp4UIey5pXVfOvlpKiACH3PVl99dfvxxx+9UwanwFBnuIIb3JQaG264odeB8e9//9t709SVQW+bP/zww96qgicUROFS3H/Md/kLfcbpRFKHcdy3TAtdy+0LdtS5beHP8NQFbv/7779vwfnY3faozzh1izqu2DZ1jrm2Ud6jjz7azjjjjGKHefv1RvHnn3/u5y2lw9M/6LeFQYMG2T333ONvzhcIFO5gifvWeSUCItTW1113nV/G7bbbzm655RZ/vZwFvYmtaTdc0vc/OJ2M257vU+bXXHONvzs45Yne9FYHqEsnnniinXzyyW614Oehhx5qGu3FpXBARC2+R64swU/aZH5ARFraxLVP+Luq0SH0fY2TFLxz7733+lkHDBhgl1xyibee5HdGFygnIKKSzyGVIannvc49ffp0L1gq7ug2+s3X90zPpfB0O0nec9tuu609++yzKrKXNDLIOeec41YLfoafi9UMiEjSRJUOf68qde8l8SxN2iLfTbDLLrvkjMylaWEUgEpCIMsCBERkufUoOwIIIIBAvQgQEFEvLUk9EEAAAQQQyKAAAREZbDSKXFQgHBCh4e/1D+CFkoIV1GEUfJuyUP7ll1/eGxZbb9bPmzfPzxoVEKGdekNUc7+75DpHNVWB3iB1yXWIa5SKrl272qxZs7xdmhpCQRrq/D/++OPt+uuvd4fY8OHDTW+CViLF6UQKd4wv6HU1LYGmEcmXFCyiN5o1XUk49e/f3y677LLw5sj1OHWLPLDIxuAIH8parD7B06233no59Sqlwyp4Hi3vscce9sgjj/ibd9hhB7vpppv8dbcQ7gyKGzAUbvdyRogI37v6Xtx8882uaGV9ht9k1Vvw6piKm8JlCgZphAMi1KGp50ucpA4kDaHukvvOu/Wwp9te7mcp913wGuH60ybzpyIKOpWzXG6buGuFv6t61umZFyepk/fBBx/0swZHTknyO6MLlhMQUcnnkMqQ1PNe51b63//+5z279L0OBk39ujf6f/VMVsBlq1at/AxJPgc0fdOjjz7qX6uU3xf9Jgd/WysZEBGepih87iRNhBH+XlXq3kviWZq0hX9zhBZ22mknmzhxor+VgAifgoUMCxAQkeHGo+gIIIAAAnUjQEBE3TQlFUEAAQQQQCB7AgREZK/NKHFxgXICIsJzbburLLzwwrbaaqtZt27dvFEetLz22mtbhw4dvCzhN23zBUSMHDnSTj/9dHdacx0T5557bs70Cg888IBtsMEGXr4DDjjA/vGPf/jHuE6Dnj172ocffuhtb9q0qb3xxhu2zDLL+PkWZCFOJ5LeEtSUHi6pvBpOudzUrl0709QR+VK+tnH5NaqH3oQtluLUrdg5ovYfddRROaMcxH3bVOfacsst7cUXX/RPqw6sq6++2l8vZWGdddax9957zz8kXzmS6gzShYt1hOoN9+DoGfouBYM4/MKXsBAeFl7DepdyP4anxdD5FGSkFB79Q9/JSy+9NFbpwuUKB0RU+3uUr9C0yfwRItLSJq6twt/VUqbU0dRMr7zyijuVnXrqqV4wnTaE781Kfmd0/mLPAeUJ161SndI6t1JSz/tfz577vx988IE3tc4TTzzhvVU/e/bs3AyBtZ133jlnlJwk77nwKA+77bZbzmg4gWI1WNTfKMGplNzfH8GM4QAATdt01llnBbNELq+55po2Y8YMf1/43Ema6KJJ3XtJPEuTtvAbIbTw+9//3psyzm2+6KKLTKNQkRDIsgABEVluPcqOAAIIIFAvAgRE1EtLUg8EEEAAAQQyKEBARAYbjSIXFSg1IGLq1Kle50nwxBriWm9IqkO0devWwV05y2ussYZ98skn/rYrrrjCO8bf8NvCtGnTrHfv3v5mBVZoxIitt97ann/+eW97y5YtvVEgmjVr5q1raoHgm+6HHXaY6W1BzUvukjp9gm8Bu+3lfsbpRBozZoxp6gCXVl555ZxOfbe9Ep/jx483zWVeKLVt29aeeuopU2BFoRSnboWOz7cvHNSyzTbb2K233pove8728OgSnTp1Mk2XUmr6+uuvrWPHjqaRRVzKN3VHUp1Bum6xjtD7778/5w339u3bewE9rsyFPjU9ikYJWWmllWzFFVc0fUeVFGChjiiXFByjIJm4SdPUTJkyxc8eHAVCHXzBc2nECAU2xEmbbrqpvfzyy37WcEBENb9HfiEiFmiT+QERaWkT10zh72qc0Y7csRphSJ0/LmnaKAUdKCX5ndH5iz0HlCdctywHRKg+Lmm0KQW5aRolBRS44EW3X0GMr7/+uunZp5TkPafARXWou6S/QfSbGidp5CqNYOVSOGhB28MBEZom6LzzznOH5P3U79ycOXP8/eFzJ2miiyZ17yXxLE3awm+E0IKCgGfOnOlv1UhOGj2IhECWBQiIyHLrUXYEEEAAgXoRICCiXlqSeiCAAAIIIJBBAQIiMthoFLmoQKkBETfccIM3t3fwxBqZQcM6F0oaMrtLly72ww8/+NlGjRrlBS34GwIL4Q55ze2ta7jjNVrA7bff7h+hzl9NqeCS/oFa8zqrA94lBW1oWPZKpXAZNQe93hQMpscff9x23XVXf5PmRNd0HksssYS/rdDCQw895O12HdsKBIlKn332mW200UamT5eWW245u+CCC7xO9Z9//tlttq222irnbVZ/R2AhTt0C2WMvhjss1Gb//Oc/Yx0f7jjUQa+++qppSpZS0tixY+3AAw/MOeSWW24xTf8QTkl1Buk64foo4ENDxLs0efJkr03dujoHFSy05JJLuk15P3XP6d5T0nF6+1nDeGtakGOOOcbbrv9ZYYUV7D//+Y9pmpli6csvv/RGf/nuu+/8rLq/NL2AkkaD0Fv5LmlkGAU5aOSYQknf6VVWWcWCb4qHAyKq9T0qVE7to03mB0SkpU1cm4W/q5o66O6773a7835qxILf/e53OfvVEe6C8pL8zuiixZ4DyhOuWxYCIvSbo5ENFESp32f9HisgL19ScISCB4NTXCnvPffc4wc2JnnPTZgwwTQCjkt6zioYY9FFF3WbIj8VWNejR4+cDvFw0IIOPPnkk3NGNAqOrhN54l82qpNdf8sEU/jcSZrouknde0k8S5O2CLZDcFmjdn3//ff+Jv3dtu666/rrLCCQRQECIrLYapQZAQQQQKDeBAiIqLcWpT4IIIAAAghkSICAiAw1FkWNLVBqQIQ6ktWh7JI6MhWsUCypY2rw4ME52YJv4ebs+GXlzDPPNO13SXM0K+DApajh0NWppc4tJQUeaBSA4LQIL7zwgnXu3NmdYoE/w0EDqqM64YIpqrPt2GOPtdNOOy2YLXJZb5zqzdNgUoe5Os7DKTx6gvbfeeedtvnmm5uud+ONN+YcUmxI5zh1yzlhzBUFz2gqBZdKGTFDo4NolJBg0qgkGmkkblInneam14gjLuleUaBBmzZt3Cb/M6nOIF2gWEfot99+6wUsBINZFNig/wqluXPnmlyDgQt33XWXaRQGjQ6y44475hxe6HsYzKjh3d30GG672kSBTkpvvvmmaVqcYIpzbr1NqxE6gikcEFGt71GwDFHLtMn8gIi0tIlrp/B3Vd9rTcugzupCKfzmvqZU0tRKCiRSSvI7o/MXew4oT7huSQdERP2WqRylpPBviAKmgs/+qHPp2aUAt+AzT6M/aeoMpSTvOf2toKmUginq74zgfi3rWaVpVIIpHLSgfZoGTNOBudS3b9+cv6Xc9uCnRs3QdBzBFD53kia6blL3XhLP0qQtgu3gljV6R6dfRvEIJgUCamQmEgJZFiAgIsutR9kRQAABBOpFgICIemlJ6oEAAggggEAGBQiIyGCjUeSiAqUGRGgaiuAb/XoLzo1ikO9iX331lfem+/Tp03OyqCMoHCThMijIQsP550sTJ0607t275+wOD9kf3Ln66qvbM888E9y0wMsKftAIBS5p6gdNARFM6tjRaBbBqR2aN29uKr+CSfIlvS2rjutgx/1iiy3mvbEaHl1Cb9SqUy+Y9t13X39qBI3OoY7q4HDkeutVnYUaKj4qxalb1HHFtslhiy228LOp01JvD8cZ9UAjCajzXfUJpssuuyxnaongvuCyjr/wwgtNwSDBpE4wvR0clZLqDNK14nSE7r777vboo4/6RZOTOlsWX3xxf1t4ITxSg6ax0Vvaml5GHY7qqFTHkUvqgFSwkJtWw20Pfn766ae29tprmzqxXFJH85NPPulWvc9wJ6hGNlHQhJvaJifzLyt6q1ajhATLozzhgIhqfI/CZcu3Tpv8KpOmNlGJwt9VbVNAWTgYTNtd0jOxV69eOW93q9Ne3yGXkv7OxHkOhOtW6YCIJJ73gwYN8kZ3cI4bbLCBaZoEF2jitgc/9ZzRb3UwBUfrSPqeC/99o+AYjaCj396opN9p1Uu/YcEUDlrQPgVDKCjCJTnobyndf1FJ951GnCp27qRNkrz3Kv0sTdoiqp30G+gCdrRfIyJpZBT9nUdCIMsCBERkufUoOwIIIIBAvQgQEFEvLUk9EEAAAQQQyKAAAREZbDSKXFSg1IAITUFx8cUX++fVP/6OGzcuZ7oKf+cvC+rgUNBDcH5tt1+jQBx11FFuNedTw1CrY+Tzzz/P2a4VdVLobXR1pgeT3oJ3w/cHt2tZAQOnnnpqePMCrSv44bnnnvPPobdEzz//fH/dLSgIQEER+sd6l9Sxf/XVV3udzG6b+1Q+BXcowCKY1MEU7szXyAbqzAp2VGuqDAV/qCPcpUceecT22GMPt+p9qoNbnU1RndVx65ZzwhgrCkpYddVVbdasWX5uvQWraTziJN17wWlQ3DEKAJGZzh2V5KHh2DVMdzhFTXXi8iTZGRSnI3TKlClep5jcXNLUKJp6pF27dm6T/6l6amj6efPm+dvU7ldddZW/Hh6lQzvUqXfttdeaprkIJ5VBncRvvfVWzi7ZHHLIITnbwtMLaKdGptC5w0EvCmzR2896foRTOCBC+5P8HoWvX2idNpmvk5Y2UYnC31VXSo2Qo2d/uCNeHc16bgTvaz0L1cG52mqrucO9zyS/M3GeA+G6VTogIonnfZRZodGR9Ls3YMAA03EuKUhL37fgb1mS95x+TzU1V/D5qalT9FwLP2+/+eYbbwouTekRTlEBEVEjPq2xxhqmaR7Cv8H6fdTfTY899lj41BZ17iRNkrz3kniWJmnRoDF+2aAgy+Dfffp9vu+++6Kysg2BTAkQEJGp5qKwCCCAAAJ1KkBARJ02LNVCAAEEEEAgCwIERGShlShjqQKlBkSo81JzXweT5gVXh5Pm33bzbeuNb3V0q+P2s88+C2b3l4cMGWJnn322vx5eyDfigzp81cEaTrqOgiiCgQcujwICFABQyRR+m1TnVueJAhIWWmghL+DBBW1oSgBNDRBMCiY57LDDTENna7QLvRGqf8zX28nBkSd0jEYEUOdIcMoPddqoI0tvsAaTmyojuE3LUZ7HHXec6R4Ip1LqFj622LqCVhS84tIxxxyT8+as2x71qTorAEQBMVFpzTXX9Ix0T+qZrWHQ33//fdM/7EalfPeSy5tkZ1CcjlCVQ1NkXHPNNa5I3mf79u29IB+NbqFRGFTPO+64wzS8vIKJXFpqqaW8+yY8fLfqrRFCgklmuk816oum3FBn1aRJk7xpMtT5F0x6c1nD6oc7mfXd23777b3jgvl1PnWG6vuhYzRqhEb20MgVUSkqIEL5kvgeRV2/2DbaZL5QWtok/F2dX8Jfg3LU2b7WWmt5v0caeUfBZbNnzw5mKxg4l9R3Js5zIFy3SgdEJPG81++ZprEK//5rRBgFQioYQEEG2q/fOz0PXnrppZz2UFCAfMIpyXsu/DeRrq3fdP1WagQc/Y2jEXUuv/xyb8SmcNm0HhW0oO0KPAsG4GibjA499FBvxCgFiSkgRwEY+v2KSvnOnZRJ0vdeEs/SpCyi2mO33XbLCVyJM81K1HnYhkDaBAiISFuLUB4EEEAAgcYoQEBEY2x16owAAggggEBKBAiISElDUIyKCoT/8V9vfOsfwPMldbaqY0hTPkSlpZde2r7++muvcz9qf3CbAhQUqJAvRQVfKO/w4cMbBGW4c6ij9vXXX3er3qc6M1577bWcbZVY0VuBejswX3rllVdshRVW8Hbr+aHghfDw1/mODW7XW7LqINabq8EUHq1D+4JTZQTzalmdfxtuuKHNnDnT36XADb2Rq46aYCqlbsHj4iyHR/JQmaJGCch3LrXv3nvvnTMFSL68hbZvvfXWXsBOy5Yt82ZLsjMoTkeoCvbll19604y8++67ecsZtUOBB7pvFHATTgpE2GGHHRp0VobzRa0rAOPBBx+MHE1C+XWPa7qXjz/+OOrwWNvyBUQk8T2KVaBQJtpkPkha2iT8XZ1fwnhLmlZI912LFi0iD0jqOxPnORCuW6UDIpJ63mvqq5122ilnSpJI3IiNnTp18kYwCo/MoKxJ3nP6ndTfOOHgjIgi5t2UL2hBgW0KACgl6W8q1delfOdOyiTpey+JZ2lSFq4N3KemTFGwn6aFc0nBMsHAVbedTwSyJkBARNZajPIigAACCNSjAAER9diq1AkBBBBAAIGMCOgf2PSGtZI6IsPDf2ekGhQTgRyBUgMidLA6OrfYYovYHZ7q0Nd19Gb/xhtvnHN9TTnRtWvXnG1uRdNA6B+Wv/vuO7fJ+9SICB07dszZ5lbC9dH2Aw88sGDggju21E+92brddtvlDXIIj9Sg+ujtQU1fEDWKRdT127RpY6NHjzZ13geTOprUoa1/kHcpaqoMt899as5yvQ0cTLJUgEurVq38zaXWzT8wxoI6nFZZZRVz00Do/lAner552qNOqeexRprQcOOlJgUK6I1cjU4SHuEgfK4kO4PidIS68ijIaNiwYQ1GGXH7w5/6fdLvVLitg/lkeMIJJ1jUkO/BfMHlbbfd1kaNGlX09+/DDz/0pmjJN5JH8JwK4tE9oNEtXMoXEKH9lfweueuV80mbzFdLQ5uEv6sKcNCUS+E38ueXev6SRjcaMWKENW/efP7GiKUkvjNxngPhulU6ICLJ5/3tt9/ujezy/fffR4hGb9KUJTpOwVf5UpL3nP7m0N8SUSNRhcvTr18/L+AyGLCWL2hBv9cKOh07dmz4NA3WNbqURtFadtllc6b7yndunSAJk6TvPZU7iWdpEhYqazBpRC/9LeyS/q7R32YkBOpBgICIemhF6oAAAgggkHUBAiKy3oKUHwEEEEAAgQwLEBCR4caj6HkFwvMfFxshwp1I/4CtDqSRI0d6/wjvtgc/NTqCAgY0JYL+UV9JU1p8+umnfraLL77YBg4c6K+HF8JDeeutUf0jdL40YcIEb+qO4H4N7a9gjCTSjBkzbNCgQd4Q2uHzq6NLQ36Hk4ZrP/PMM71OFP2jfVRSIMThhx/udZ4EAxVc3i233NJefPFFt+p9asqE4D/O5+wMrCgYQHmD6ZRTTvE6x4Pbyqlb8PhCy3prODjKiIYfV8d4KUmjlSggQh1n999/f9770J1zmWWWMXUk9u/fP29AjcvrPtX5f9ppp7lVr03OOeccfz3fQvi+vfLKKxvcl+GO0IMPPjhnLvKoc48fP94LdFAnb3Cee5dXI36ofgqe0JvFcZICIlQWvQHvglSCxyloZNNNNzVNOaDpMOImTbNx/fXXewE9UaNFKIBHI31ouh29Na3pdVx66KGHvKk73HrUZyW+R1HnLXUbbTJfrJZtEtVxq6CgoUOHmgLB5syZM7+gvy1pdBp1OivQp5RUye9MnOdAks8hV+8kn/c6t+qpZ3WhwAj9faDnl4IYNaVUnJTkPafABU2toudtcCoilUvPL5VVQWUamWry5Ml+cQsFLSgYUgFf+ttJxwSDGt0JNHqWnuGbb7656bdDyy5pNCXdt4VSJU2qce+5uiTxLK2khSun+zzxxBNzfrc0FYz+tiMhUA8CBETUQytSBwQQQACBrAsQEJH1FqT8CCCAAAIIZFiAgIgMNx5FT0xAgRHTpk2z9957z6ZPn26afqBbt27ef61bt07sumk7saahUKeP5gBv27atN1WGghoKJXWwvP/++/bGG294hrJTwIdGbNCbscXeVi507kruK6duxa6vAAZ1Jrmkudk1DUO5SYElug8/+ugjbyoNvV2rDnkFBXTo0MHrvJJts2bNyr1Eqo5Tp6I66TQVjNpH94vequ7SpUveIf+LVUDnVFCEpiTRCA8aql5BTeqkbN++fbHD8+7XeVXOd955x/uOqKy9e/f2p5PJe2DMHWn5HtEm8xusFm0SFRChoD0llUedz3p7W2//a5j7VVdd1fu+zC916UtJfWdKL0nljkjiee9KpyAujQakkWP094ICsDSajZ413bt399rF5S31M8l7TlMiKBDz1VdftaWWWso0+khwlKqNNtoodkBEsF763dI5NT2HbPRb1aNHD+/eDOYrdzlJk3LLFOe4JJ6llbaYO3eurbHGGt5UZKqTAng0XUahUU3i1J08CKRFgICItLQE5UAAAQQQaMwCBEQ05tan7ggggAACCNRYgICIGjcAl0cAgboR0Fuy6kRSMIhL6qzUkNMkBBBAoFSBQgERpZ6L/AiUIlBuQEQp1yBvugQ0JZpGVHNpn3328Ub9cOt8IpB1AQIist6ClB8BBBBAoB4ECIioh1akDggggAACCGRUgICIjDYcxUYAgVQK3HXXXXbQQQf5ZdPQ9Weffba/zgICCCAQV4CAiLhS5Ku0AAERlRZN//mC035pWqlJkyZZ165d019wSohATAECImJCkQ0BBBBAAIEEBQiISBCXUyOAAAIIIIBAYQECIgr7sBcBBBAoRUBzp2+wwQbeEOo6TtNbaOhwTR1CQgABBEoRICCiFC3yVlKAgIhKaqb/XJry5f/+7/9MI10p7brrrjZmzJj0F5wSIlCCAAERJWCRFQEEEEAAgYQECIhICJbTIoAAAggggEBxAQIiihuRAwEEEChF4I477rBDDz3UP+Skk04y/UdCAAEEShEgIKIULfJWUoCAiEpqpv9cmh5j/PjxXkEXXnhhe/LJJ2311VdPf8EpIQIlCBAQUQIWWRFAAAEEEEhIgICIhGA5LQIIIIAAAggUFyAgorgRORBAAIFSBfbYYw975JFHvMM0OsSLL75o7dq1K/U05EcAgUYsQEBEI278GledgIgaN0AVL//MM8/YDjvs4F9x6NChNmzYMH+dBQTqRYCAiHppSeqBAAIIIJBlAQIistx6lB0BBBBAAIGMCxAQkfEGpPgIIJBKgY8++sj69Oljc+bM8co3aNAgu+iii1JZVgqFAALpFCAgIp3t0hhKRUBEY2jlX+u45ZZbekGbWuvevbsXzNm8efPGA0BNG40AARGNpqmpKAIIIIBAigUIiEhx41A0BBBAAAEE6l2AgIh6b2HqhwACtRL429/+ZkcccYR3eQ1Brbcwu3btWqvicF0EEMiYwIUXXmjnn3++X+oBAwbYJZdc4q+zgEBSAn379rVXXnnFP/2dd95pm2++ub/OQn0I3HvvvTZw4ECvMs2aNbMJEyZYz54966Ny1AKBkAABESEQVhFAAAEEEKiBAAERNUDnkggggAACCCDwqwABEdwJCCCAQHICp5xyik2bNs27gDodtttuu+QuxpkRQKCuBNR5454fqljHjh2tffv2dVVHKpNOgbffftv0/xFcWmuttYxRA5xG/XzefPPNdt9993kV0kgRBx98cP1UjpogEBIgICIEwioCCCCAAAI1ECAgogboXBIBBBBAAAEEfhUgIII7AQEEEEAAAQQQQAABBBBAAAEE6lWAgIh6bVnqhQACCCCQJQECIrLUWpQVAQQQQACBOhMgIKLOGpTqIIAAAggggAACCCCAAAIIIICAL0BAhE/BAgIIIIAAAjUTICCiZvRcGAEEEEDg/9u5QxyFgiCKop8QLGGnOFaBQrBPgkNNQHTGlurX9Y/DdKg6hZrcDAECggi/AQIECBAgQIAAAQIECBAgQKCrgCCi62XtRYAAAQIrCQgiVrqWWQkQIECAQDMBQUSzg1qHAAECBAgQIECAAAECBAgQGAKCiEHhAwECBAgQmCYgiJhG74sJECBAgAABQYTfAAECBAgQIECAAAECBAgQINBVQBDR9bL2IkCAAIGVBAQRK13LrAQIECBAoJmAIKLZQa1DgAABAgQIECBAgAABAgQIDAFBxKDwgQABAgQITBMQREyj98UECBAgQICAIMJvgAABAgQIECBAgAABAgQIEOgqIIjoell7ESBAgMBKAoKIla5lVgIECBAg0ExAENHsoNYhQIAAAQIECBAgQIAAAQIEhoAgYlD4QIAAAQIEpgkIIqbR+2ICBAgQIEBAEOE3QIAAAQIECBAgQIAAAQIECHQVEER0vay9CBAgQGAlAUHEStcyKwECBAgQaCYgiGh2UOsQIECAAAECBAgQIECAAAECQ0AQMSh8IECAAAEC0wQEEdPofTEBAgQIECAgiPAbIECAAAECBAgQIECAAAECBLoKCCK6XtZeBAgQILCSgCBipWuZlQABAgQINBMQRDQ7qHUIECBAgAABAgQIECBAgACBISCIGBQ+ECBAgACBaQKCiGn0vpgAAQIECBAQRPgNECBAgAABAgQIECBAgAABAl0FBBFdL2svAgQIEFhJQBCx0rXMSoAAAQIEmgn8DyJut9t2uVyabWgdAgQIECBAgAABAgQIECBAYK8C3yDifr//1n8+n/7usdcfgr0JECBAYKqAIGIqvy8nQIAAAQL7FvgfROxbwvYECBAgQIAAAQIECBAgQIBAZwFBROfr2o0AAQIEkgUEEcnXMRsBAgQIEGgu8Pl8tuv1ur1er+abWo8AAQIECBAgQIAAAQIECBDYo8DhcPj9Z4jH47Edj8c9EtiZAAECBAhMFRBETOX35QQIECBAgMA3ini/3yAIECBAgAABAgQIECBAgAABAi0FzufzdjqdWu5mKQIECBAgkC4giEi/kPkIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBsoAgokzmAQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIJAuIIhIv5D5CBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgbKAIKJM5gEBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECCQLiCISL+Q+QgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIGygCCiTOYBAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgkC4giEi/kPkIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBsoAgokzmAQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIJAuIIhIv5D5CBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgbKAIKJM5gEBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECCQLiCISL+Q+QgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIGygCCiTOYBAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgkC4giEi/kPkIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBsoAgokzmAQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIJAuIIhIv5D5CBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgbKAIKJM5gEBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECCQLiCISL+Q+QgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIGygCCiTOYBAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgkC4giEi/kPkIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBsoAgokzmAQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIJAuIIhIv5D5CBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgbKAIKJM5gEBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECCQLiCISL+Q+QgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIGygCCiTOYBAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgkC4giEi/kPkIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBsoAgokzmAQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIJAuIIhIv5D5CBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgbKAIKJM5gEBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECCQLiCISL+Q+QgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIGygCCiTOYBAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgkC7wB15ZytRX0mgWAAAAAElFTkSuQmCC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decoding\n",
    "\n",
    "> Given a sequence of integers in the range [0, vocab_size], what is the text?\n",
    "\n",
    "Now we'll begin writing the decoder, which will take a sequence of tokens and convert them back into bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x01'\n",
      "b'\\x02'\n",
      "b'\\x01\\x02'\n"
     ]
    }
   ],
   "source": [
    "# byte/list addition: byte([p0]) + byte([p1]) = byte([p0, p1])\n",
    "b1 = bytes([1])\n",
    "b2 = bytes([2])\n",
    "b3 = b1 + b2\n",
    "print(b1)\n",
    "print(b2)\n",
    "print(b3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a vocab mapping from and integer index to its respective byte based on the original 0 to 255 encodings\n",
    "\n",
    "Next, we add the additional byte pair encoding stored in the  `merge` dictionary. This ensures that we get the new tokens, like (101, 32) -> 256. We add these new token indexes in the order they were created by the BPE algorithm above so that we don't overwrite earlier token mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map each index to its respective byte (convert list [idx] to a byte)\n",
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    # byte/list addition: byte([p0]) + byte([p1]) = byte([p0, p1])\n",
    "    vocab[idx] = vocab[p0] + vocab[p1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write the decoder, which concatenates the bytes into a byte string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ids: list[int]) -> str:\n",
    "    \"\"\"given ids (list of integers), return Python string\"\"\"\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "    text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, we must set `errors=\"replace\"` to handle cases like decoding 128, whose binary representation does not conform with [UTF-8 standards](https://en.wikipedia.org/wiki/UTF-8#Encoding). In these cases, we simply replace the output with the error character ï¿½. By [default](https://docs.python.org/3/library/codecs.html), `errors=\"strict\"`, meaning that the code will throw an error when run.\n",
    "\n",
    "For example, the binary representation of the byte 128 is 10000000 (2^7). However, the UTF-8 schema does not allow for the sequence 10xxxxx, where we start with 1 an 0, followed by the 6 remaining values. But the byte 101, which in binary is 01100101, does follow the format 0xxxxxx, and can be properly decoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï¿½\n",
      "e\n"
     ]
    }
   ],
   "source": [
    "print(decode([128])) # binary: 10000000\n",
    "print(decode([101])) # binary: 01100101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoding\n",
    "\n",
    "> The other way around: Given a string, what are the tokens?\n",
    "\n",
    "And the encoder, which turns a string into a sequence of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100, 33]\n"
     ]
    }
   ],
   "source": [
    "def encode(text: str) -> list[int]:\n",
    "    \"\"\"given a string, return a list of integers (the tokens)\"\"\"\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "\n",
    "    # check if at least two tokens to cover cases like encode(\"h\") where there are no pairs to merge!\n",
    "    while len(tokens) >= 2:\n",
    "        # get mapping of pairs to counts (int, int) -> count (int)\n",
    "        stats = get_stats(ids)             \n",
    "\n",
    "        # we want to work our way up the dictionary from first merged to last merged\n",
    "        # so we need to get the byte pair merger with the lowest index \n",
    "        # merges has items (child, child) -> parent (int)\n",
    "        # the key says to get the pair (child, child) with the smallest parent idx (int) -- added earliest\n",
    "        # if the pair is not found within the merges dict, it outputs inf, which is definitely not the min in the generated list\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\"))) \n",
    "        \n",
    "        # if all pairs are not in merges, then they all return inf above, which is the min\n",
    "        # then pair will become the first element in stats, which is still not in merges\n",
    "        # at this point, we have merged all possible pairs into idxs, meaning that we can stop\n",
    "        if pair not in merges:\n",
    "            break \n",
    "\n",
    "        # otherwise, get the token representing the pair, merge it (replace in tokens), and continue\n",
    "        idx = merges[pair]\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "print(encode(\"hello world!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have encoder and decoder, we better be able to cancel them out to get the original input back! \n",
    "\n",
    "Note that not all token sequences are valid byte strings, so we may encounter a decoder error where we get ï¿½ (but should work fine in a lot of cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world!\n"
     ]
    }
   ],
   "source": [
    "print(decode(encode(\"hello world!\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that the training text works too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "text2 = decode(encode(text))\n",
    "print(text2 == text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And some validation data that the tokenizer was not trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "valtext = \"Many common characters, including numerals, punctuation, and other symbols, are unified within the standard and are not treated as specific to any given writing system. Unicode encodes thousands of emoji, with the continued development thereof conducted by the Consortium as a part of the standard.[4] Moreover, the widespread adoption of Unicode was in large part responsible for the initial popularization of emoji outside of Japan. Unicode is ultimately capable of encoding more than 1.1 million characters.\"\n",
    "valtext2 = decode(encode(valtext))\n",
    "print(valtext2 == valtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4) Regex\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the problems with this sort of encoding is that it creates repetitious tokens like \"dog\", \"dog.\", \"dog?\", \"dog!\". It feels wrong that there should be tokens representing every word + punctuation combination, and this was determined to perform suboptimally in the [GPT-2 paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (2.2. Input Representation) (it wastes vocabulary slots and does not perform well empirically).\n",
    "\n",
    "To address cases like these, they used regex patterns to enforce rules about which parts of the text will never be merged. \n",
    "\n",
    "Here's a Unicode regex [reference](https://www.regular-expressions.info/unicode.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2 regex pattern in action\n",
    "\n",
    "We can copy the GPT-2 regex pattern from its [GitHub repo](https://github.com/openai/gpt-2/blob/master/src/encoder.py#L53).\n",
    "\n",
    "Some takeaways:\n",
    "\n",
    "- lots of ORs \"|\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it out with a string. `re.findall()` will try to match patterns from left to right within the string, and store all of the matches in a list.\n",
    "\n",
    "So for \"Hello world\", it will start at the left of `gpt2pat`, moving right, and find that it never matches `'s|'t|'re|'ve|'m|'ll|'d|`. Then it will find its first match at `| ?\\p{L}+|`, an optional space \" ?\" followed by \"\\p{L}+\" one or more times. According to the regex reference above, \n",
    "\n",
    "> \\p{L} or \\p{Letter}: any kind of letter from any language\n",
    "\n",
    "So \" ?\\p{L}+\" means an optional space followed by one or more letters from any language. \n",
    "\n",
    "Then for the string \"Hello world\", \"Hello\" will match the pattern (optional space (none) followed by letters). The match ends after \"Hello\", since the whitespace \" \" is not a letter. \n",
    "\n",
    "From there on, the pattern will attempt to match all `'s|'t|'re|'ve|'m|'ll|'d|` again, will fail, and then match `| ?\\p{L}+|` with \" world\" (optional space (one) followed by letters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ' world', ' how', ' are', ' you']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(gpt2pat, \"Hello world how are you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is that we now *first* split the text into a list of texts. *Then* we tokenize these individual pieces independently.\n",
    "\n",
    "This means that we can only ever find merges using BPE between the elements of this list. After we've found all of our merges, we join the results via concatenation. So in the list\n",
    "\n",
    "```py\n",
    "['Hello', ' world', ' how', ' are', ' you']\n",
    "```\n",
    "\n",
    "we will never merge \" are\" and \" you\", meaning we will never merge \"e\" with the whitespace \" \". Wow! Recall that in the text above, the most frequent char pairing, and thus the first to be merged, was exactly that: \"e\" followed by a space \" \"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen a simple example, let's brave our way through the whole pattern\n",
    "\n",
    "```py\n",
    "            gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "```\n",
    "\n",
    "The patterns that are attempted to be matched in the order: \n",
    "1) `'s|'t|'re|'ve|'m|'ll|'d`  -- checks for the use of an apostrophe before these characters (common contractions, possessive case, etc) \n",
    "    - fails for Unicode apostrophe \"how`s\" instead of \"how's\"\n",
    "    - does not use `re.IGNORECASE`, so will fail for uppercase variants like \"how'S\" or \"that'LL\"\n",
    "2) `| ?\\p{L}+|` -- checks for an optional space followed by one or more characters from any language\n",
    "3) `| ?\\p{N}+|` -- checks for an optional space followed by one or more numeric character in any script\n",
    "4) `| ?[^\\s\\p{L}\\p{N}]+|` -- checks for optional space followed by something that is not (^) a space (\\s), a letter (\\p{L}) or a number (\\p{N})\n",
    "    - checks for punctuation like  \"what?!?!?!\" -> \" what\" + \"?!?!?!\"\n",
    "5) `\\s+(?!\\S)` -- uses negative lookahead assertion to match up to, but not the last, whitespace character (one or more spaces followed by a non-space)\n",
    "    - separates \"    y\" where we have 4 whitespaces into \"   \" (3 spaces) and \" y\" (1 spaces)\n",
    "6) `\\s+` -- checks for one or more whitespace character\n",
    "    - the last fallback pattern, useful when there are lots of spaces at the end, not followed by anything like in pattern (5)\n",
    "\n",
    "We can manipulate our string to show each case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', \"'ve\", ' world', '123', ' how', '`', 's', ' are', '         ', ' ya', \"'\", 'LL', ' WHAT', '?!?!?!', '        ']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(gpt2pat, \"Hello've world123 how`s are          ya'LL WHAT?!?!?!        \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now a real-world example. We see that there is a split at each \"category\" change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'for', ' i', ' in', ' range', '(', '1', ',', ' 101', '):', '\\n   ', ' if', ' i', ' %', ' 3', ' ==', ' 0', ' and', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'FizzBuzz', '\")', '\\n   ', ' elif', ' i', ' %', ' 3', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'Fizz', '\")', '\\n   ', ' elif', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'Buzz', '\")', '\\n   ', ' else', ':', '\\n       ', ' print', '(', 'i', ')', '\\n']\n"
     ]
    }
   ],
   "source": [
    "example = \"\"\"\n",
    "for i in range(1, 101):\n",
    "    if i % 3 == 0 and i % 5 == 0:\n",
    "        print(\"FizzBuzz\")\n",
    "    elif i % 3 == 0:\n",
    "        print(\"Fizz\")\n",
    "    elif i % 5 == 0:\n",
    "        print(\"Buzz\")\n",
    "    else:\n",
    "        print(i)\n",
    "\"\"\"\n",
    "print(re.findall(gpt2pat, example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you'd think that OpenAI would've then trained the tokenizer by splitting training text into chunks based on the above regex pattern and then using BPE. However, this is not the case. Why? Well, above in the code example, we see a chunk \"\\n       \" consisting of a newline char and whitespace (the indent before a line of code). But using the GPT-2 tokenizer, we find that the spaces are actually never merged. Instead, tokenizing a chunk like this reveals a series of individual space tokens. So \"  \" would be tokenized into [220, 220], two whitespace tokens. So somewhere in the tokenizer training file there is an additional rule enforcing that spaces are never merged. However, this tokenizer training file has not been made publically available, so we can't see exactly what they did. We can, however, use the open source inference tokenizer, provided to us by the `tiktoken` package (or in the GPT-2 [GitHub repo](https://github.com/openai/gpt-2/blob/master/src/encoder.py)), which applies the trained merges (learned through BPE in the training code) to a new piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see below that the GPT-2 tokenizer treats the two whitespace characters as two separate tokens, denoted by the two 220 tokens. The GPT-4 tokenizer merges the spaces into a single token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[220, 220, 23748, 995, 10185]\n",
      "[256, 24748, 1917, 12340]\n"
     ]
    }
   ],
   "source": [
    "# GPT-2 does not merge spaces \n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(enc.encode(\"   hello world!!!\"))\n",
    "\n",
    "# GPT-4 does merge spaces\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc.encode(\"   hello world!!!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPT-4 tokenizer uses a different regex pattern. The regex patterns for all of the tokenizers can be found in the [tiktoken repo](https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py). The GPT-4 tokenizer, cl100k_base, has the regex pattern\n",
    "\n",
    "```py\n",
    "            gpt4pat = re.compile(r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\")\n",
    "```\n",
    "\n",
    "Key points:\n",
    "\n",
    "1) `?i:[sdmt]|ll|ve|re)` -- the i denotes a case insensitive match, fixing the apostrophe matching problem from the GPT-2 tokenizer\n",
    "2) a lot more whitespace handling\n",
    "3) `\\p{N}{1,3}` -- they only match 1 to 3 numbers -- never merge numbers that are more than three digits\n",
    "    - this means that numbers 1, 2, or 3 digits long are subject to be merged into single tokens. so something like \"12345\" may be tokenized into [\"123\", \"45\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5) Walking through the GPT-2 tokenizer\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have enough knowledge to start dissecting the GPT-2 tokenizer [encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py). Note that while the file and class are called encoders, it is really a tokenizer!\n",
    "\n",
    "We see that in `get_encoder()`, it instantiates a new `Encoder` object (tokenizer) using encoder.json and vocab.bpe. We can download these files to get an idea of what they store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to download these files:\n",
    "# !wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
    "# !wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that [encoder.json](encoder.json) is equivalent to our `vocab` dictionary above, meaning our mappings from ints to bytes\n",
    "\n",
    "```py\n",
    "    {\n",
    "      \"0\": \"b'\\x00'\",\n",
    "      \"1\": \"b'\\x01'\",\n",
    "      \"2\": \"b'\\x02'\",\n",
    "      \"3\": \"b'\\x03'\", \n",
    "    }\n",
    "```\n",
    "\n",
    "except that it actually maps the inverse: byte to int.\n",
    "\n",
    "[vocab.bpe](vocab.bpe) is equivalent to our `merges` dictionary above, meaning our mappings from pairs of tokens to\n",
    "\n",
    "```py\n",
    "    {\n",
    "      (101, 32): 256,\n",
    "      (105, 110): 257,\n",
    "      (115, 32): 258,\n",
    "      (116, 104): 259\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "with open(\"encoder.json\", \"r\") as f:\n",
    "    encoder = json.load(f) # <--- ~equivalent to our \"vocab\"\n",
    "\n",
    "\n",
    "with open(\"vocab.bpe\", \"r\", encoding=\"utf-8\") as f:\n",
    "    bpe_data = f.read()\n",
    "bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split(\"\\n\")[1:-1]]\n",
    "# ^---- ~equivalent to our \"merges\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to an `encoder` and `decoder`, the GPT-2 `Encoder` class also has a `byte_encoder` and `byte_decoder`. This just splits up the process a bit more: byte encode (Unicode code points to byte stream) -> encode (byte stream to tokens) -> decode (tokens to byte stream) -> byte decode (byte stream to code points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6) Special tokens\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the tokens from raw bytes and BPE merges, we can also use special tokens to delimit different parts of the data and create special structures in the token strings.\n",
    "\n",
    "The length of the GPT-2 `encoder` dictionary (bytes to int ids) is 50257. Where does this number come from?\n",
    "\n",
    "1) 256 raw byte tokens (1 byte == 8 bits --> 256 possible values with indexes ranging from 0 to 255)\n",
    "2) 50,000 tokens from BPE merges\n",
    "3) 1 special token \"<|endoftext|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('!', 0),\n",
       " ('\"', 1),\n",
       " ('#', 2),\n",
       " ('$', 3),\n",
       " ('%', 4),\n",
       " ('&', 5),\n",
       " (\"'\", 6),\n",
       " ('(', 7),\n",
       " (')', 8),\n",
       " ('*', 9)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k, v) for k, v in encoder.items()][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this special \"<|endoftext|>\" token is at the very end of the list. \n",
    "\n",
    "It's used to delimit documents in the training set. When we tokenize the training set, we'll get a string of tokens from 0 to 50256. The \"<|endoftext|>\" token is inserted\n",
    "between documents to indicate that what follows is going to be unrelated / not informative to what comes next. \n",
    "\n",
    "By including these tokens, we hope to teach the language model through data to \"wipe its memory\" of what came before the \"<|endoftext|>\" token when encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder[\"<|endoftext|>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use [https://tiktokenizer.vercel.app](https://tiktokenizer.vercel.app), we see that as we type \"<|endoftext|>\", it breaks it down into multiple tokens until the full string is typed, at which point it becomes a single token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 91, 8862, 728, 428, 91]\n"
     ]
    }
   ],
   "source": [
    "# GPT-4 \n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc.encode(\"<|endoftext|\")) # not the full <|endoftext|> token -- so broken up into tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so \n",
    "- Why does this work? \n",
    "- How does the string of tokens we see above change to a single token upon typing the final character? \n",
    "- Why is \"<|endoftext|>\" called a special token?\n",
    "\n",
    "Well, these so-called special tokens do not go through the BPE merge algorithm. Instead, the code has special instructions to locate and handle special tokens. \n",
    "\n",
    "For example, in the [tiktoken repo](https://github.com/openai/tiktoken/blob/main/src/lib.rs#L210) we see that special tokens are found outside of the BPE algorithm\n",
    "\n",
    "\n",
    "```rs\n",
    "        loop {\n",
    "            // Find the next allowed special token, if any\n",
    "            next_special = special_regex.find_from_pos(text, start_find).unwrap();\n",
    "            match next_special {\n",
    "                Some(m) => {\n",
    "                    if allowed_special.contains(&text[m.start()..m.end()]) {\n",
    "                        break;\n",
    "                    }\n",
    "                    start_find = m.start() + 1;\n",
    "                }\n",
    "                None => break,\n",
    "            }\n",
    "        }\n",
    "```\n",
    "\n",
    "The rest of the code then replaces the stream of tokens with the single special token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special tokens are used everywhere, especially in instruction/chat finetuning. This is because we don't want to just delimit documents, we also want to delimit conversations between an assistant and a user. Some other examples include \"<|im_start|>\", \"<|im_sep|>\", \"<|im_end|>\", etc.\n",
    "\n",
    "```text\n",
    "    <|im_start|>system<|im_sep|>You are a helpful assistant<|im_end|>\n",
    "    <|im_start|>user<|im_sep|><|im_end|>\n",
    "    <|im_start|>assistant<|im_sep|>\n",
    "```\n",
    "\n",
    "These special tokens are arbitrary and can be added to tokenizers, as long as they are handled.\n",
    "\n",
    "GPT-4 also has special `FIM_PREFIX`, `FIM_MIDDLE`, `FIM_SUFFIX` tokens (fill in the middle) that supposedly [help](https://arxiv.org/pdf/2207.14255) train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you add special tokens, however, you have to do some model surgery to the transformer and its parameters.\n",
    "\n",
    "By adding a special token, we're sort of extending the vocabulary. As such, we need to create a new row in the embedding matrix and include another neuron in \n",
    "the final projection layer, both to stand in for the special token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7) GPT-4 tokenizer exercise\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we'll follow along this [exercise](https://github.com/karpathy/minbpe/blob/master/exercise.md) to implement the GPT-4 tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "\n",
    "Write the `BasicTokenizer` class, with the following three core functions:\n",
    "\n",
    "- `def train(self, text, vocab_size, verbose=False)`\n",
    "- `def encode(self, text)`\n",
    "- `def decode(self, ids)`\n",
    "\n",
    "Train your tokenizer on whatever text you like and visualize the merged tokens. Do they look reasonable? One default test you may wish to use is the text file `tests/taylorswift.txt`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids: list[int], counts: dict = None) -> dict[tuple[int, int], int]:\n",
    "    \"\"\"\n",
    "    Given a list of integers, return a dictionary of counts of consecutive pairs\n",
    "    Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n",
    "    Optionally allows to update an existing dictionary of counts\n",
    "    \"\"\"\n",
    "    counts = {} if counts is None else counts\n",
    "    for pair in zip(ids, ids[1:]): \n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "def merge(ids: list[int], pair: tuple[int, int], idx: int) -> list[int]:\n",
    "    \"\"\"\n",
    "    In the list of integers (ids), replace all consecutive occurrences\n",
    "    of pair with the new integer token idx\n",
    "    Example: ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]\n",
    "    \"\"\"\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # if not at the very last position AND the pair matches, replace it\n",
    "        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        # otherwise, only proceed one step to check next pair\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTokenizer:\n",
    "    def __init__(self, ):\n",
    "        # default: vocab size of 256 (all bytes), no merges, no patterns\n",
    "        self.merges: dict[tuple[int, int], int] = {}  # (int, int) -> int\n",
    "        self.pattern: str = \"\" \n",
    "        self.special_tokens: dict[str, int] = {}  # str -> int, e.g. {'<|endoftext|>': 100257}\n",
    "        self.vocab: dict[int, bytes] = self._build_vocab()  # int -> bytes\n",
    "\n",
    "    def train(self, text: str, vocab_size: int, verbose: bool = False):\n",
    "        assert vocab_size > 256\n",
    "        num_merges = vocab_size - 256\n",
    "\n",
    "        for i in range(num_merges):\n",
    "            stats = get_stats()\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        ...\n",
    "    \n",
    "    def decode(self, ids: list[int]) -> str:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "Convert you `BasicTokenizer` into a `RegexTokenizer`, which takes a regex pattern and splits the text exactly as GPT-4 would. Process the parts separately as before, then concatenate the results. Retrain your tokenizer and compare the results before and after. You should see that you will now have no tokens that go across categories (numbers, letters, punctuation, more than one whitespace). Use the GPT-4 pattern:\n",
    "\n",
    "```\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "\n",
    "You're now ready to load the merges from the GPT-4 tokenizer and show that your tokenizer produces the identical results for both `encode` and `decode`, matching [tiktoken](https://github.com/openai/tiktoken).\n",
    "\n",
    "```\n",
    "# match this\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\") # this is the GPT-4 tokenizer\n",
    "ids = enc.encode(\"hello world!!!? (ì•ˆë…•í•˜ì„¸ìš”!) lol123 ðŸ˜‰\")\n",
    "text = enc.decode(ids) # get the same text back\n",
    "```\n",
    "\n",
    "Unfortunately, you will run into two issues:\n",
    "\n",
    "1. It is not trivial to recover the raw merges from the GPT-4 tokenizer. You can easily recover what we call `vocab` here, and what they call and store under `enc._mergeable_ranks`. Feel free to copy paste the `recover_merges` function in `minbpe/gpt4.py`, which takes these ranks and returns the raw merges. If you wish to know how this function works, read [this](https://github.com/openai/tiktoken/issues/60) and [this](https://github.com/karpathy/minbpe/issues/11#issuecomment-1950805306). Basically, under some conditions it is enough to only store the parent nodes (and their rank) and get rid of the precise details of which children merged up to any parent.\n",
    "2. Second, the GPT-4 tokenizer for some reason permutes its raw bytes. It stores this permutation in the first 256 elements of the mergeable ranks, so you can recover this byte shuffle relatively simply as `byte_shuffle = {i: enc._mergeable_ranks[bytes([i])] for i in range(256)}`. In both your encode and decode, you'll have to shuffle bytes around accordingly. If you're stuck, reference the minbpe/gpt4.py` file for hints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "\n",
    "(Optional, irritating, not obviously useful) Add the ability to handle special tokens. You'll then be able to match the output of tiktoken even when special tokens are present, e.g.:\n",
    "\n",
    "```\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\") # this is the GPT-4 tokenizer\n",
    "ids = enc.encode(\"<|endoftext|>hello world\", allowed_special=\"all\")\n",
    "```\n",
    "\n",
    "Without `allowed_special` tiktoken will error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the tokenizer out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\") # GPT-4 tokenizer\n",
    "print(enc.encode(\"ì•ˆë…•í•˜ì„¸ìš” ðŸ‘‹ (hello in Korean!)\"))\n",
    "print(enc.decode(enc.encode(\"ì•ˆë…•í•˜ì„¸ìš” ðŸ‘‹ (hello in Korean!)\")) == \"ì•ˆë…•í•˜ì„¸ìš” ðŸ‘‹ (hello in Korean!)\")\n",
    "# match the above for your own tokenizer, and also implement a train() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "\n",
    "If you've made it this far, you're now a pro at LLM Tokenization! Sadly, you're not exactly done yet because a lot of LLMs outside of OpenAI (e.g. Llama, Mistral) use [sentencepiece](https://github.com/google/sentencepiece) instead. Primary difference being that sentencepiece runs BPE directly on Unicode code points instead of on UTF-8 encoded bytes. Feel free to explore sentencepiece on your own (good luck, it's not too pretty), and stretch goal if you really experience and suffer from the burden of time, re-write your BPE to be on Unicode code points and match the Llama 2 tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8) Sentencepiece\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commonly used because (unlike tiktoken) it can efficiently both train and inference BPE tokenizers. It is used in both Llama and Mistral series.\n",
    "\n",
    "[sentencepiece on Github link](https://github.com/google/sentencepiece).\n",
    "\n",
    "**The big difference**: sentencepiece runs BPE on the Unicode code points directly! It then has an option `character_coverage` for what to do with very very rare codepoints that appear very few times, and it either maps them onto an UNK token, or if `byte_fallback` is turned on, it encodes them with utf-8 and then encodes the raw bytes instead.\n",
    "\n",
    "TLDR:\n",
    "\n",
    "- tiktoken encodes to utf-8 and then BPEs bytes\n",
    "- sentencepiece BPEs the code points and optionally falls back to utf-8 bytes for rare code points (rarity is determined by character_coverage hyperparameter), which then get translated to byte tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a toy.txt file with some random text\n",
    "with open(\"toy.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "  f.write(\"SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) [Sennrich et al.]) and unigram language model [Kudo.]) with the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "z2h",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
